From dfd805f4c32463502d24832b586b9e4191b79800 Mon Sep 17 00:00:00 2001
From: Maru Newby <marun@redhat.com>
Date: Thu, 29 Oct 2020 13:56:00 +0100
Subject: [PATCH] UPSTREAM: <carry>: Add OpenShift tooling, images, configs and
 docs

UPSTREAM: <carry>: Copy hack scripts and tools from openshift/origin

UPSTREAM: <carry>: Fix shellcheck failures for copied openshift-hack bash

UPSTREAM: <carry>: Enable build, test and verify

UPSTREAM: <carry>: Copy README content from origin

UPSTREAM: <carry>: Copy watch-termination command from openshift/origin

UPSTREAM: <carry>: Switch image and rpm build to golang 1.14

UPSTREAM: <carry>: Copy test annotation from origin

UPSTREAM: <carry>: Build openshift-compatible kube e2e binary

UPSTREAM: <carry>: Updating openshift-hack/images/hyperkube/Dockerfile.rhel baseimages to mach ocp-build-data config

UPSTREAM: <carry>: Update test annotation rules

UPSTREAM: <carry>: Enable k8s-e2e-serial

UPSTREAM: <carry>: Update test annotation rules

UPSTREAM: <carry>: Build with golang 1.15

UPSTREAM: <carry>: (squash) Stop installing recent bash and protoc from source

UPSTREAM: <carry>: Add rebase instructions

UPSTREAM: <carry>: (squash) Update README.openshift to reflect transition

UPSTREAM: <carry>: (squash) Stop annotating origin tests with [Suite:openshift]

The detection logic was error-prone (different results based on the
repo existing in GOPATH vs not) and whether a test comes from origin
can be inferred from the absence of the `[Suite:k8s]` tag.

UPSTREAM: <carry>: (squash) Update hyperkube version

UPSTREAM: <carry>: (squash) Update OpenShift docs

UPSTREAM: <carry>: watch-termination: fix deletion race and write non-graceful message also to termination.log

UPSTREAM: <carry>: watch-termination: avoid false positives of NonGracefulTermination events

UPSTREAM: <carry>: (squash) remove servicecatalog e2e that was dropped upstream

UPSTREAM: <carry>: (squash) Fix annotation rules

UPSTREAM: <carry>: (squash) Fix image refs

UPSTREAM: <carry>: Updating openshift-enterprise-hyperkube builder & base images to be consistent with ART
Reconciling with https://github.com/openshift/ocp-build-data/tree/b0ab44b419faae6b18e639e780a1fa50a1df8521/images/openshift-enterprise-hyperkube.yml

UPSTREAM: <carry>: (squash) Retry upstream flakes

UPSTREAM: <carry>: (squash) Update test exclussions for 1.20.0

UPSTREAM: <carry>: (squash) Add detail to rebase doc

- Add new section 'Maintaining this document'
- Move checklist above the instructions to emphasize their importance
- Add new section 'Reacting to new commits'
- Mention that generated changes in carries should be dropped

UPSTREAM: <carry>: Enable CSI snapshot e2e tests

All images were uploaded to our quay.io mirror and the tests should
succeed.

UPSTREAM: <carry>: Stop skipping multi-az test (skipped upstream)

UPSTREAM: <carry>: bump tag version & update rebase doc

UPSTREAM: <carry>: update rebase doc & image

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: Add Dockerfile to build pause image

Ensuring the target directory exists before writing a file to it.

UPSTREAM: <carry>: disable part of hack/verify-typecheck-providerless.sh due to our carry patches

UPSTREAM: <carry>: Updating openshift-enterprise-pod images to be consistent with ART
Reconciling with https://github.com/openshift/ocp-build-data/tree/691e628254f318ce56efda5edc7448ec743c37b8/images/openshift-enterprise-pod.yml

UPSTREAM: <carry>: Updating openshift-enterprise-hyperkube images to be consistent with ART
Reconciling with https://github.com/openshift/ocp-build-data/tree/691e628254f318ce56efda5edc7448ec743c37b8/images/openshift-enterprise-hyperkube.yml

UPSTREAM: <carry>: Add process overlap detection event to watch-termination

NOTE: Squash this to watch-termination commit on rebase.

UPSTREAM: <carry>: openshift-hack/images/os/Dockerfile: Add io.openshift.build.versions, etc.

For example, consider the current 4.10 RHCOS:

  $ oc image info -o json registry.ci.openshift.org/ocp/4.10:machine-os-content
  io.k8s.description: The Universal Base Image is designed and engineered to be the base layer for all of your containerized applications, middleware and utilities. This base image is freely redistributable, but Red Hat only supports Red Hat technologies through subscriptions for Red Hat products. This image is maintained by Red Hat and updated regularly.
  io.k8s.display-name: Red Hat Universal Base Image 8
  io.openshift.build.version-display-names: machine-os=Red Hat Enterprise Linux CoreOS
  io.openshift.build.versions: machine-os=49.84.202109102026-0
  io.openshift.expose-services:
  io.openshift.tags: base rhel8

A bunch of those seem to be inherited from the UBI base image, so we
can leave them alone.  But the io.openshift.build.* entries are
RHCOS-specific, and are consumed by 'oc adm release new ...' [1,2] and
friends to answer questions like "which RHCOS is in this release?":

  $ oc adm release info -o json quay.io/openshift-release-dev/ocp-release:4.8.12-x86_64
  {
    "kubernetes": {
      "Version": "1.21.1",
      "DisplayName": ""
    },
    "machine-os": {
      "Version": "48.84.202109100857-0",
      "DisplayName": "Red Hat Enterprise Linux CoreOS"
    }
  }

Setting this label will avoid failures when consumers like
driver-toolkit's version consumer [3]:

  name: 0.0.1-snapshot-machine-os

bump into ci-tools-built machine-os-content images that lack the
io.openshift.build.versions declaration of machine-os version [4]:

  error: unable to create a release: unknown version reference "machine-os"

I've gone with generic testing values, so hopefully this is not
something that local maintainers need to remember to bump for each
OpenShift z stream.

[1]: https://github.com/openshift/oc/blob/f94afb52dc8a3185b3b9eacaf92ec34d80f8708d/pkg/cli/admin/release/image_mapper.go#L328-L334
[2]: https://github.com/openshift/oc/blob/f94afb52dc8a3185b3b9eacaf92ec34d80f8708d/pkg/cli/admin/release/annotations.go#L19-L28
[3]: https://github.com/openshift/driver-toolkit/commit/464accad880f2eaaf1f150464e58e6ebc58245fe#diff-4caed9b2b966a8fa7a016ae28976634a2d3d1b635c4e820d5c038b2305d6af53R18
[4]: https://prow.ci.openshift.org/view/gs/origin-ci-test/pr-logs/pull/openshift_kubernetes/959/pull-ci-openshift-kubernetes-master-images/1438398678602616832#1:build-log.txt%3A97

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: squash with the rest of tooling

UPSTREAM: <carry>: Updating openshift-enterprise-pod images to be consistent with ART
Reconciling with https://github.com/openshift/ocp-build-data/tree/5b89f5b601508a0bcc0399fd3f34b7aa2e86e90e/images/openshift-enterprise-pod.yml

UPSTREAM: <carry>: Updating openshift-enterprise-hyperkube images to be consistent with ART
Reconciling with https://github.com/openshift/ocp-build-data/tree/5b89f5b601508a0bcc0399fd3f34b7aa2e86e90e/images/openshift-enterprise-hyperkube.yml

UPSTREAM: <carry>: rebase script

UPSTREAM: <carry>: Fix networking-related test exclusions

Tests that fail on openshift-sdn specifically should be tagged as
such, so that they don't also get skipped when running under
ovn-kubernetes or third-party network plugins.

UPSTREAM: <carry>: Skip "subPath should be able to unmount" NFS test

Due to a kernel bug https://bugzilla.redhat.com/show_bug.cgi?id=1854379
in Linux 5.7+ this test fails - the bind-mounted NFS share cannot be
cleanly unmounted, gets "Stale file handle" error instead on umount.
As a result this test is permafailing on Fedora CoreOS nodes.

UPSTREAM: <carry>: Skip GlusterFS tests

GlusterFS is not supported in 4.x, we've been running its tests just
because we could. Now it does not work on IPv6 systems.

E [MSGID: 101075] [common-utils.c:312:gf_resolve_ip6] 0-resolver: getaddrinfo failed (Address family for hostname not supported)

UPSTREAM: <carry>: Skip GlusterFS tests

The previous commit left two GlusterFS test still running:

[sig-storage] Volumes GlusterFS should be mountable [Skipped:ibmcloud] [Suite:openshift/conformance/parallel] [Suite:k8s]
[sig-storage] Dynamic Provisioning GlusterDynamicProvisioner should create and delete persistent volumes

Skip it, we don't support Gluster and it does not work on ipv6

UPSTREAM: <carry>: 1.22 alpha & other tests disablement

UPSTREAM: <carry>: 1.21 alpha & other tests disablement

UPSTREAM: <carry>: Enable GenerciEphemeralVolume tests

UPSTREAM: <carry>: Re-enable [Feature:NetworkPolicy] tests which were wrongly disabled in rebase

UPSTREAM: <carry>: Reenable NetworkPolicy test

UPSTREAM: <carry>: Conformance tests (sysctls) should be run

We have to run this test for conformance, and the tests pass. Reenable
this block which has been disabled for 2 releases (but appears to work fine).

UPSTREAM: <carry>: Don't force-disable IPv6, dual-stack, and SCTP tests

Instead, openshift-tests will enable or disable them depending on
cluster configuration.

UPSTREAM: <carry>: update Multi-AZ Cluster Volumes test name

This test was renamed upstream in
https://github.com/kubernetes/kubernetes/commit/006dc7477f15e42ae70adc02421a5bacd068ba05

UPSTREAM: <carry>: re-enable networking tests after rebase

During a bump to k8 ver. 1.22.0, networking
tests were disabled to accomplish the bump.
This disabled netpol and older network tests.
Netpol tests will be enabled in a following
PR and therefore only partially fixes BZ.

This commit partially fixes bug 1986307.
https://bugzilla.redhat.com/show_bug.cgi?id=1986307

UPSTREAM: <drop>: update test annotate rules

UPSTREAM: <carry>: Add DOWNSTREAM_OWNERS

UPSTREAM: <carry>: clarify downstream approver rules

UPSTREAM: <carry>: copy extensions into resulting image

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: Fix conformance and serial tests by stopping node cordoning

Master nodes already have `master` taint which
cannot be tolerated by normal workloads. If we manually
cordon the master nodes again, some of the control plane
components cannot get rescheduled unless they have
toleration to the `node.kubernetes.io/unschedulable`
taint. Even if we have the toleration in the pod
spec, because of the backwards compability issues
scheduler will ignore nodes which have `unschedulable`
field set. IOW:

- Cordoning master nodes is redundant as masters already
  have taints
- Cordoning master nodes can cause issues which are hard
  to debug as control-plane components may be evicted/preempted
  during e2e run(highly unlikely but a possibility).

So, let's stop cordoning master nodes.

UPSTREAM: <carry>: enable internal traffic policy tests

Fixes:
https://bugzilla.redhat.com/show_bug.cgi?id=1986307

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: enable e2e test after 1.23 rebase in sdn

Enable "[sig-network] Conntrack should be able to preserve UDP traffic when initial unready endpoints get ready" after 1.23 rebase in openshift/sdn

UPSTREAM: <carry>: Unskip OCP SDN related tests

Unskip networkPolicy tests concerning IpBlock and
egress rules since both features have now been
implemented.

UPSTREAM: <carry>: enable should drop INVALID conntrack entries test

UPSTREAM: <carry>: update e2es

UPSTREAM: revert: <carry>: Unskip OCP SDN related tests

These newly-enabled tests are breaking some CI, possibly due to race
conditions in the tests. Re-disable them for now.

This reverts commit aba8d2093ce5153bf7b5d98979a019247cf71073.

UPSTREAM: <carry>: update hyperkube and image version

UPSTREAM: <drop>: disable e2e tests

- disable 'ProxyTerminatingEndpoints' feature e2e tests

- disable [sig-network] [Feature:Topology Hints] should distribute endpoints evenly
see https://bugzilla.redhat.com/show_bug.cgi?id=2079958 for more context

UPSTREAM: <carry>: Add kubensenter to the openshift RPM

This carry-patch adds the kubensenter script to the openshift-hyperkube
RPM, by importing it via the new hack/update-kubensenter.sh script.

UPSTREAM: <carry>: Skip session affinity timeout tests

in 4.12 and higher the default CNI is OVNKubernetes and
these two tests do not pass. Skip them. They are also
skipping in the origin test suites for ovnk.

UPSTREAM: <carry>: Update kubensenter to use exec instead of direct call

Because kubelet relies on systemd's Type=notify mechanism, we don't need
or want kubensenter to keep itself in the process tree. exec is best.

UPSTREAM: <carry>: update to ginkgo v2 - squash to tooling

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: allow annotating with a specific suite

If a test specifies a suite, don't append another one to it. We want the
ability to add tests to a particular suite without automatically being
added to parallel conformance.

UPSTREAM: <carry>: Ensure balanced brackets in annotated test names

We recently started marking tests with apigroups, and in one case we
missed the closing bracket on the annotation resulting in the test being
erroneously skipped.

This adds a check in the annotation generation, and errors when brackets
are unbalanced.

```
Example:
$ ./hack/verify-generated.sh
FAILURE after 12.870s: hack/verify-generated.sh:13: executing '/home/stbenjam/go/src/github.com/openshift/origin/hack/update-generated.sh' expecting success: the command returned the wrong error code
Standard output from the command:
Nov  4 14:11:25.026: INFO: Enabling in-tree volume drivers
Nov  4 14:11:25.026: INFO: Warning: deprecated ENABLE_STORAGE_GCE_PD_DRIVER used. This will be removed in a future release. Use --enabled-volume-drivers=gcepd instead
Nov  4 14:11:25.026: INFO: Enabled gcepd and windows-gcepd in-tree volume drivers

Standard error from the command:
failed: unbalanced brackets in test name:
[Top Level] [sig-scheduling][Early] The openshift-console console pods [apigroup:console.openshift.io should be scheduled on different nodes
                                                                       ^
```

UPSTREAM: <carry>: add CSI migration feature gates for vSphere and Azure File

This commit is the next natural step for commits 2d9a8f90b24
and d37e84c5426. It introduces custom feature gates to enable
the CSI migration in vSphere and Azure File plugins.

See openshift/enhancements#549 for details.

Stop <carrying> the patch when CSI migration becomes GA (i.e.
features.CSIMigrationAzureFile / features.CSIMigrationVSphere
are GA).

UPSTREAM: <carry>: Skip in-tree topology tests win Azure Disk migrated to CSI

Skip test that depend on in-tree Azure Disk volume plugin that (wrongly)
uses failure domains for value of "topology.kubernetes.io/zone" label in
Azure regions that don't have availability zones.

Our e2e tests blindly use that label and expect that a volume provisioned
in such a "zone" can be used only by nodes in that "zone" (= topology
domain). This is false, Azure Disk CSI driver can use such a volume in any
zone and therefore the test may randomly fail.

See https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=2066865

UPSTREAM: <carry>: Stop ignoring generated openapi definitions

openshift/origin needs to be able to vendor these definitions so they
need to be committed.

Signed-off-by: astoycos <astoycos@redhat.com>
Signed-off-by: Jamo Luhrsen <jluhrsen@gmail.com>
Signed-off-by: Jim Ramsay <jramsay@redhat.com>
Signed-off-by: Martin Kennelly <mkennell@redhat.com>
Signed-off-by: Mohamed Mahmoud <mmahmoud@redhat.com>
Signed-off-by: Riccardo Ravaioli <rravaiol@redhat.com>
OpenShift-Rebase-Source: 514f1814b28
OpenShift-Rebase-Source: 87e220b8298
OpenShift-Rebase-Source: b25e156ce35
OpenShift-Rebase-Source: 22563879f46
OpenShift-Rebase-Source: e4d66c11577
OpenShift-Rebase-Source: 5af594b9f81

UPSTREAM: <carry>: disable tests for features in alpha

UPSTREAM: <carry>: disable tests dependent on StackDriver

UPSTREAM: <carry>: add default sysctls for kubelet in rpm

UPSTREAM: <carry>: add new approvers

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: update hyperkube image version

UPSTREAM: <carry>: update hyperkube image version

Updated builder as well.

UPSTREAM: <carry>: add missing generated file

UPSTREAM: <carry>: Add OpenShift tooling, images, configs and docs

UPSTREAM: <carry>: Add OpenShift tooling, images, configs and docs

Add CSI mock volume tests. In upstream these tests were moved
to a different package, so we stopped generating their names
in OpenShift. This patch fixes that.

UPSTREAM: <carry>: Add OpenShift tooling, images, configs and docs

Disable CSI mock tests for SELinux and RecoverVolumeExpansionFailure, which
are alpha features and require additional work to get enabled.

UPSTREAM: <carry>: Add OpenShift tooling, images, configs and docs

UPSTREAM: <carry>: update rebase doc

UPSTREAM: <carry>: disable failing dnsPolicy test

UPSTREAM: <carry>: disable failing dnsPolicy test

UPSTREAM: <carry>: Create minimal wrapper needed to run k8s e2e tests

UPSTREAM: <carry>: Change annotation mechanics to allow injecting testMaps and filter out tests

UPSTREAM: <carry>: Move k8s-specific rules to our fork

UPSTREAM: <carry>: Create minimal wrapper needed to run k8s e2e tests

UPSTREAM: <carry>: Create minimal wrapper needed to run k8s e2e tests

UPSTREAM: <carry>: Add OpenShift tooling, images, configs and docs

Update the list of tests that should be skipped.

UPSTREAM: <carry>: Force using host go always and use host libriaries

UPSTREAM: <carry>: ignore vendor when generating code

UPSTREAM: <carry>: ignore vendor when installing ncpu from hack/tools

UPSTREAM: <carry>: move test rules from origin

These were brought back in o/o PRs as follows:
- netpol - https://github.com/openshift/origin/pull/26775/
- schedulerpreemption - https://github.com/openshift/origin/pull/27874/

UPSTREAM: <carry>: UserNamespacesSupport feature was rename to UserNamespacesStatelessPodsSupport

See commit 531d38e323c54378acd8ea664f5752d31e8ee27a.

UPSTREAM: <carry>: allow apiserver-library-go to depend on k8s.io/kubernetes

UPSTREAM: <carry>: Add OpenShift tooling, images, configs and docs

Remove commitchecker.

UPSTREAM: <carry>: Force using host go always and use host libriaries

UPSTREAM: <carry>: Add OpenShift tooling, images, configs and docs

Update builder images.
---
 .ci-operator.yaml                             |   4 +
 .gitignore                                    |   7 +
 DOWNSTREAM_OWNERS                             |  29 +
 README.openshift.md                           |  73 ++
 REBASE.openshift.md                           | 532 +++++++++++++++
 build/pause/Dockerfile.Rhel                   |  13 +
 build/run.sh                                  |   6 +
 cmd/watch-termination/main.go                 | 366 ++++++++++
 hack/lib/golang.sh                            |  46 +-
 hack/make-rules/test.sh                       |   1 +
 hack/make-rules/update.sh                     |   2 +
 hack/make-rules/verify.sh                     |  35 +
 hack/update-codegen.sh                        |   2 +-
 hack/update-kubensenter.sh                    |   1 +
 hack/update-test-annotations.sh               |   1 +
 hack/update-vendor.sh                         |   5 +-
 hack/verify-govet-levee.sh                    |   6 +-
 hack/verify-kubensenter.sh                    |   1 +
 hack/verify-pkg-names.sh                      |   2 +-
 hack/verify-test-annotations.sh               |   1 +
 hack/verify-typecheck-providerless.sh         |  16 +-
 hack/verify-vendor.sh                         |   6 +-
 openshift-hack/build-go.sh                    |  21 +
 openshift-hack/build-rpms.sh                  | 130 ++++
 openshift-hack/cmd/k8s-tests/k8s-tests.go     |  98 +++
 openshift-hack/cmd/k8s-tests/provider.go      | 147 ++++
 openshift-hack/cmd/k8s-tests/runtest.go       | 143 ++++
 openshift-hack/cmd/k8s-tests/types.go         |  69 ++
 openshift-hack/conformance-k8s.sh             |  96 +++
 .../create-or-update-rebase-branch.sh         |  67 ++
 openshift-hack/e2e/annotate/annotate.go       | 290 ++++++++
 openshift-hack/e2e/annotate/annotate_test.go  |  55 ++
 openshift-hack/e2e/annotate/cmd/main.go       |   9 +
 .../generated/zz_generated.annotations.go     |   1 +
 openshift-hack/e2e/annotate/rules.go          | 383 +++++++++++
 openshift-hack/e2e/annotate/rules_test.go     |  88 +++
 openshift-hack/e2e/include.go                 |  23 +
 openshift-hack/e2e/kube_e2e_test.go           | 102 +++
 openshift-hack/e2e/namespace.go               | 160 +++++
 openshift-hack/images/OWNERS                  |  11 +
 .../images/hyperkube/Dockerfile.rhel          |  16 +
 openshift-hack/images/hyperkube/OWNERS        |   5 +
 openshift-hack/images/hyperkube/hyperkube     |  57 ++
 openshift-hack/images/hyperkube/kubensenter   | 117 ++++
 openshift-hack/images/os/Dockerfile           |  27 +
 openshift-hack/images/os/install.sh           |  40 ++
 openshift-hack/images/tests/Dockerfile.rhel   |  21 +
 openshift-hack/images/tests/OWNERS            |   5 +
 openshift-hack/kubensenter.env                |  16 +
 openshift-hack/lib/build/binaries.sh          | 457 +++++++++++++
 openshift-hack/lib/build/rpm.sh               |  95 +++
 openshift-hack/lib/build/version.sh           |  88 +++
 openshift-hack/lib/cmd.sh                     | 645 ++++++++++++++++++
 openshift-hack/lib/constants.sh               | 324 +++++++++
 openshift-hack/lib/deps.sh                    |  28 +
 openshift-hack/lib/init.sh                    |  68 ++
 openshift-hack/lib/log/output.sh              | 104 +++
 openshift-hack/lib/log/stacktrace.sh          |  91 +++
 openshift-hack/lib/test/junit.sh              | 202 ++++++
 openshift-hack/lib/util/ensure.sh             | 116 ++++
 openshift-hack/lib/util/environment.sh        | 296 ++++++++
 openshift-hack/lib/util/find.sh               |  73 ++
 openshift-hack/lib/util/misc.sh               | 224 ++++++
 openshift-hack/lib/util/text.sh               | 164 +++++
 openshift-hack/lib/util/trap.sh               |  99 +++
 openshift-hack/rebase.sh                      | 175 +++++
 openshift-hack/sysctls/50-kubelet.conf        |   6 +
 openshift-hack/test-go.sh                     |  16 +
 openshift-hack/test-integration.sh            |  20 +
 openshift-hack/test-kubernetes-e2e.sh         |  88 +++
 openshift-hack/update-kubensenter.sh          | 139 ++++
 openshift-hack/update-test-annotations.sh     |  13 +
 openshift-hack/verify-kubensenter.sh          |  12 +
 openshift-hack/verify-test-annotations.sh     |  12 +
 openshift-hack/verify.sh                      |  26 +
 openshift.spec                                | 137 ++++
 pkg/kubelet/DOWNSTREAM_OWNERS                 |  17 +
 test/typecheck/main.go                        |   3 +
 78 files changed, 7036 insertions(+), 54 deletions(-)
 create mode 100644 .ci-operator.yaml
 create mode 100644 DOWNSTREAM_OWNERS
 create mode 100644 README.openshift.md
 create mode 100644 REBASE.openshift.md
 create mode 100644 build/pause/Dockerfile.Rhel
 create mode 100644 cmd/watch-termination/main.go
 create mode 120000 hack/update-kubensenter.sh
 create mode 120000 hack/update-test-annotations.sh
 create mode 120000 hack/verify-kubensenter.sh
 create mode 120000 hack/verify-test-annotations.sh
 create mode 100755 openshift-hack/build-go.sh
 create mode 100755 openshift-hack/build-rpms.sh
 create mode 100644 openshift-hack/cmd/k8s-tests/k8s-tests.go
 create mode 100644 openshift-hack/cmd/k8s-tests/provider.go
 create mode 100644 openshift-hack/cmd/k8s-tests/runtest.go
 create mode 100644 openshift-hack/cmd/k8s-tests/types.go
 create mode 100755 openshift-hack/conformance-k8s.sh
 create mode 100755 openshift-hack/create-or-update-rebase-branch.sh
 create mode 100644 openshift-hack/e2e/annotate/annotate.go
 create mode 100644 openshift-hack/e2e/annotate/annotate_test.go
 create mode 100644 openshift-hack/e2e/annotate/cmd/main.go
 create mode 100644 openshift-hack/e2e/annotate/generated/zz_generated.annotations.go
 create mode 100644 openshift-hack/e2e/annotate/rules.go
 create mode 100644 openshift-hack/e2e/annotate/rules_test.go
 create mode 100644 openshift-hack/e2e/include.go
 create mode 100644 openshift-hack/e2e/kube_e2e_test.go
 create mode 100644 openshift-hack/e2e/namespace.go
 create mode 100644 openshift-hack/images/OWNERS
 create mode 100644 openshift-hack/images/hyperkube/Dockerfile.rhel
 create mode 100644 openshift-hack/images/hyperkube/OWNERS
 create mode 100755 openshift-hack/images/hyperkube/hyperkube
 create mode 100644 openshift-hack/images/hyperkube/kubensenter
 create mode 100644 openshift-hack/images/os/Dockerfile
 create mode 100755 openshift-hack/images/os/install.sh
 create mode 100644 openshift-hack/images/tests/Dockerfile.rhel
 create mode 100644 openshift-hack/images/tests/OWNERS
 create mode 100644 openshift-hack/kubensenter.env
 create mode 100644 openshift-hack/lib/build/binaries.sh
 create mode 100644 openshift-hack/lib/build/rpm.sh
 create mode 100644 openshift-hack/lib/build/version.sh
 create mode 100644 openshift-hack/lib/cmd.sh
 create mode 100755 openshift-hack/lib/constants.sh
 create mode 100644 openshift-hack/lib/deps.sh
 create mode 100755 openshift-hack/lib/init.sh
 create mode 100644 openshift-hack/lib/log/output.sh
 create mode 100644 openshift-hack/lib/log/stacktrace.sh
 create mode 100644 openshift-hack/lib/test/junit.sh
 create mode 100644 openshift-hack/lib/util/ensure.sh
 create mode 100644 openshift-hack/lib/util/environment.sh
 create mode 100644 openshift-hack/lib/util/find.sh
 create mode 100644 openshift-hack/lib/util/misc.sh
 create mode 100644 openshift-hack/lib/util/text.sh
 create mode 100644 openshift-hack/lib/util/trap.sh
 create mode 100755 openshift-hack/rebase.sh
 create mode 100644 openshift-hack/sysctls/50-kubelet.conf
 create mode 100755 openshift-hack/test-go.sh
 create mode 100755 openshift-hack/test-integration.sh
 create mode 100755 openshift-hack/test-kubernetes-e2e.sh
 create mode 100755 openshift-hack/update-kubensenter.sh
 create mode 100755 openshift-hack/update-test-annotations.sh
 create mode 100755 openshift-hack/verify-kubensenter.sh
 create mode 100755 openshift-hack/verify-test-annotations.sh
 create mode 100755 openshift-hack/verify.sh
 create mode 100644 openshift.spec
 create mode 100644 pkg/kubelet/DOWNSTREAM_OWNERS

diff --git a/.ci-operator.yaml b/.ci-operator.yaml
new file mode 100644
index 00000000000..844f0d26ad6
--- /dev/null
+++ b/.ci-operator.yaml
@@ -0,0 +1,4 @@
+build_root_image:
+  name: release
+  namespace: openshift
+  tag: rhel-8-release-golang-1.20-openshift-4.14
diff --git a/.gitignore b/.gitignore
index 32317a564c5..67c30909d55 100644
--- a/.gitignore
+++ b/.gitignore
@@ -128,3 +128,10 @@ zz_generated_*_test.go
 
 # generated by verify-vendor.sh
 vendordiff.patch
+
+# Ignore openshift source archives produced as part of rpm build
+openshift*.tar.gz
+
+# Ensure that openapi definitions are not ignored to ensure that
+# openshift/origin can vendor them.
+!pkg/generated/openapi/zz_generated.openapi.go
diff --git a/DOWNSTREAM_OWNERS b/DOWNSTREAM_OWNERS
new file mode 100644
index 00000000000..80598d361f9
--- /dev/null
+++ b/DOWNSTREAM_OWNERS
@@ -0,0 +1,29 @@
+# See the OWNERS docs at https://go.k8s.io/owners
+
+filters:
+  ".*":
+    # Downstream reviewers, don't have to match those in OWNERS
+    reviewers:
+    - deads2k
+    - sttts
+    - soltysh
+    - mfojtik
+
+    # Approvers are limited to the team that manages rebases and pays the price for carries that are introduced
+    approvers:
+    - deads2k
+    - sttts
+    - soltysh
+    - mfojtik
+    - tkashem
+
+  "^\\.go.(mod|sum)$":
+    labels:
+    - "vendor-update"
+  "^vendor/.*":
+    labels:
+    - "vendor-update"
+  "^staging/.*":
+    labels:
+    - "vendor-update"
+component: kube-apiserver
diff --git a/README.openshift.md b/README.openshift.md
new file mode 100644
index 00000000000..b04871fc09c
--- /dev/null
+++ b/README.openshift.md
@@ -0,0 +1,73 @@
+# OpenShift's fork of k8s.io/kubernetes
+
+This respository contains core Kubernetes components with OpenShift-specific patches.
+
+## Cherry-picking an upstream commit into openshift/kubernetes: Why, how, and when.
+
+`openshift/kubernetes` carries patches on top of each rebase in one of two ways:
+
+1. *periodic rebases* against an upstream Kubernetes tag.  Eventually,
+any code you have in upstream Kubernetes will land in Openshift via
+this mechanism.
+
+2. Cherry-picked patches for important *bug fixes*.  We really try to
+limit feature back-porting entirely. Unless there are exceptional circumstances, your backport should at least be merged in kubernetes master branch. With every carry patch (not included in upstream) you are introducing a maintenance burden for the team managing rebases.
+
+### For Openshift newcomers: Pick my Kubernetes fix into Openshift vs. wait for the next rebase?
+
+Assuming you read the bullets above... If your patch is really far behind, for
+example, if there have been 5 commits modifying the directory you care about,
+cherry picking will be increasingly difficult and you should consider waiting
+for the next rebase, which will likely include the commit you care about or at
+least decrease the amount of cherry picks you need to do to merge.
+
+To really know the answer, you need to know *how many commits behind you are in
+a particular directory*, often.
+
+To do this, just use git log, like so (using pkg/scheduler/ as an example).
+
+```
+MYDIR=pkg/scheduler/algorithm git log --oneline --
+  ${MYDIR} | grep UPSTREAM | cut -d' ' -f 4-10 | head -1
+```
+
+The commit message printed above will tell you:
+
+- what the LAST commit in Kubernetes was (which effected
+"/pkg/scheduler/algorithm")
+- directory, which will give you an intuition about how "hot" the code you are
+cherry picking is.  If it has changed a lot, recently, then that means you
+probably will want to wait for a rebase to land.
+
+### Cherry-picking an upstream change
+
+Since `openshift/kubernetes` closely resembles `k8s.io/kubernetes`,
+cherry-picking largely involves proposing upstream commits in a PR to our
+downstream fork. Other than the usual potential for merge conflicts, the
+commit messages for all commits proposed to `openshift/kubernetes` must
+reflect the following:
+
+- `UPSTREAM: <UPSTREAM PR ID>:` The prefix for upstream commits to ensure
+  correct handling during a future rebase. The person performing the rebase
+  will know to omit a commit with this prefix if the referenced PR is already
+  present in the new base history.
+- `UPSTREAM: <drop>:` The prefix for downstream commits of code that is
+  generated (i.e. via `make update`) or that should not be retained by the
+  next rebase.
+- `UPSTREAM: <carry>:` The prefix for downstream commits that maintain
+  downstream-specific behavior (i.e. to ensure an upstream change is
+  compatible with OpenShift). Commits with this are usually retained across
+  rebases.
+
+## Updating openshift/kubernetes to a new upstream release
+
+Instructions for rebasing `openshift/kubernetes` are maintained in a [separate
+document](REBASE.openshift.md).
+
+## RPM Packaging
+
+A specfile is included in this repo which can be used to produce RPMs
+including the openshift binary. While the specfile will be kept up to
+date with build requirements the version is not updated. Building the
+rpm with the `openshift-hack/build-rpms.sh` helper script will ensure
+that the version is set correctly.
diff --git a/REBASE.openshift.md b/REBASE.openshift.md
new file mode 100644
index 00000000000..2edea6fe8e8
--- /dev/null
+++ b/REBASE.openshift.md
@@ -0,0 +1,532 @@
+# Maintaining openshift/kubernetes
+
+OpenShift is based on upstream Kubernetes. With every release of Kubernetes that is
+intended to be shipped as OCP, it is necessary to incorporate the upstream changes
+while ensuring that our downstream customizations are maintained.
+
+## Rebasing for releases < 4.6
+
+The instructions in this document apply to OpenShift releases 4.6 and
+above. For previous releases, please see the [rebase
+enhancement](https://github.com/openshift/enhancements/blob/master/enhancements/rebase.md).
+
+## Maintaining this document
+
+An openshift/kubernetes rebase is a complex process involving many manual and
+potentially error-prone steps. If, while performing a rebase, you find areas where
+the documented procedure is unclear or missing detail, please update this document
+and include the change in the rebase PR. This will ensure that the instructions are
+as comprehensive and accurate as possible for the person performing the next
+rebase.
+
+## Rebase Checklists
+
+The checklists provided below highlight the key responsibilities of
+someone performing an openshift/kubernetes rebase.
+
+In preparation for submitting a PR to the [openshift fork of
+kubernetes](https://github.com/openshift/kubernetes), the following
+should be true:
+
+- [ ] The new rebase branch has been created from the upstream tag
+- [ ] The new rebase branch includes relevant carries from target branch
+- [ ] Dependencies have been updated
+- [ ] Hyperkube dockerfile version has been updated
+- [ ] `make update` has been invoked and the results committed
+- [ ] `make` executes without error
+- [ ] `make verify` executes without error
+- [ ] `make test` executes without error
+- [ ] The upstream tag is pushed to `openshift/kubernetes` to ensure that
+      build artifacts are versioned correctly
+      - Upstream tooling uses the value of the most recent tag (e.g. `v1.25.0`)
+        in the branch history as the version of the binaries it builds.
+      - Pushing the tag is easy as
+```
+git push git@github.com:openshift/kubernetes.git refs/tags/v1.25.0
+```
+
+Details to include in the description of the PR:
+
+- [ ] A link to the rebase spreadsheet for the benefit for reviewers
+
+After the rebase PR has merged to `openshift/kubernetes`, vendor the changes
+into `openshift/origin` to ensure that the openshift-tests binary reflects
+the upstream test changes introduced by the rebase:
+
+- [ ] Find the SHA of the merge commit after your PR lands in `openshift/kubernetes`
+- [ ] Run `hack/update-kube-vendor.sh <o/k SHA>` in a clone of the `origin`
+      repo and commit the results
+- [ ] Run `make update` and commit the results
+- [ ] Submit as a PR to `origin`
+
+As a final step, send an email to the aos-devel mailing list announcing the
+rebase. Make sure to include:
+
+- [ ] The new version of upstream Kubernetes that OpenShift is now based on
+- [ ] Link(s) to upstream changelog(s) detailing what has changed since the last rebase landed
+- [ ] A reminder to component maintainers to bump their dependencies
+- [ ] Relevant details of the challenges involved in landing the rebase that
+      could benefit from a wider audience.
+
+## Getting started
+
+Before incorporating upstream changes you may want to:
+
+- Read this document
+- Get familiar with tig (text-mode interface for git)
+- Find the best tool for resolving merge conflicts
+- Use diff3 conflict resolution strategy
+   (https://blog.nilbus.com/take-the-pain-out-of-git-conflict-resolution-use-diff3/)
+- Teach Git to remember how youâ€™ve resolved a conflict so that the next time it can
+  resolve it automatically (https://git-scm.com/book/en/v2/Git-Tools-Rerere)
+
+## Send email announcing you're starting work
+
+To better spread the information send the following email:
+
+```
+Title: k8s <version> bump is starting...
+
+I'm starting the process of updating our fork to bring in
+the latest available version of kubernetes. This means that
+every PR landing in openshift/kubernetes should go through
+extra scrutiny and only 2 exceptions allow merging PRs in the
+upcoming time:
+1. High priority backports which require landing master first
+to start the backport process.
+2. Critical PRs unblocking the org.
+In both cases make sure to reach out to me for final approval.
+
+There is no ETA yet, but feel free to reach out to me with
+any questions.
+```
+
+## Preparing the local repo clone
+
+Clone from a personal fork of kubernetes via a pushable (ssh) url:
+
+```
+git clone git@github.com:<user id>/kubernetes
+```
+
+Add a remote for upstream and fetch its branches:
+
+```
+git remote add --fetch upstream https://github.com/kubernetes/kubernetes
+```
+
+Add a remote for the openshift fork and fetch its branches:
+
+```
+git remote add --fetch openshift https://github.com/openshift/kubernetes
+```
+
+## Creating a new local branch for the new rebase
+
+- Branch the target `k8s.io/kubernetes` release tag (e.g. `v1.25.0`) to a new
+  local branch
+
+```
+git checkout -b rebase-1.25.0 v1.25.0
+```
+
+- Merge `openshift(master)` branch into the `rebase-1.25.0` branch with merge
+  strategy `ours`. It discards all changes from the other branch (`openshift/master`)
+  and create a merge commit. This leaves the content of your branch unchanged,
+  and when you next merge with the other branch, Git will only consider changes made
+  from this point forward.  (Do not confuse this with `ours` conflict resolution
+  strategy for `recursive` merge strategy, `-X` option.)
+
+```
+git merge -s ours openshift/master
+```
+
+## Creating a spreadsheet of carry commits from the previous release
+
+Given the upstream tag (e.g. `v1.24.2`) of the most recent rebase and the name
+of the branch that is targeted for rebase (e.g. `openshift/master`), generate a tsv file
+containing the set of carry commits that need to be considered for picking:
+
+```
+echo 'Comment Sha\tAction\tClean\tSummary\tCommit link\tPR link' > ~/Documents/v1.24.2.tsv
+```
+```
+git log $( git merge-base openshift/master v1.24.2 )..openshift/master --ancestry-path --reverse --no-merges --pretty='tformat:%x09%h%x09%x09%x09%s%x09https://github.com/openshift/kubernetes/commit/%h?w=1' | grep -E $'\t''UPSTREAM: .*'$'\t' | sed -E 's~UPSTREAM: ([0-9]+)(:.*)~UPSTREAM: \1\2\thttps://github.com/kubernetes/kubernetes/pull/\1~' >> ~/Documents/v1.24.2.tsv
+```
+
+This tsv file can be imported into a google sheets spreadsheet to track the
+progress of picking commits to the new rebase branch. The spreadsheet can also
+be a way of communicating with rebase reviewers. For an example of this
+communication, please see the [the spreadsheet used for the 1.24
+rebase](https://docs.google.com/spreadsheets/d/10KYptJkDB1z8_RYCQVBYDjdTlRfyoXILMa0Fg8tnNlY/edit).
+
+## Picking commits from the previous rebase branch to the new branch
+
+Go through the spreadsheet and for every commit set one of the appropriate actions:
+ - `p`, to pick the commit
+ - `s`, to squash it (add a comment with the sha of the target)
+ - `d`, to drop the commit (if it is not obvious, comment why)
+
+Set up conditional formatting in the google sheet to color these lines appropriately.
+
+Commits carried on rebase branches have commit messages prefixed as follows:
+
+- `UPSTREAM: <carry>:`
+  - A persistent carry that should probably be picked for the subsequent rebase branch.
+  - In general, these commits are used to modify behavior for consistency or
+    compatibility with openshift.
+- `UPSTREAM: <drop>:`
+  - A carry that should probably not be picked for the subsequent rebase branch.
+  - In general, these commits are used to maintain the codebase in ways that are
+    branch-specific, like the update of generated files or dependencies.
+- `UPSTREAM: 77870:`
+  - The number identifies a PR in upstream kubernetes
+    (i.e. `https://github.com/kubernetes/kubernetes/pull/<pr id>`)
+  - A commit with this message should only be picked into the subsequent rebase branch
+    if the commits of the referenced PR are not included in the upstream branch.
+  - To check if a given commit is included in the upstream branch, open the referenced
+    upstream PR and check any of its commits for the release tag (e.g. `v.1.25.0`)
+    targeted by the new rebase branch. For example:
+    - <img src="openshift-hack/commit-tag.png">
+
+With these guidelines in mind, pick the appropriate commits from the previous rebase
+branch into the new rebase branch. Create a new filter view in the spreadsheet to allow
+you get a view where `Action==p || Action==s` and copy paste the shas to `git cherry-pick`
+command. Use `tr '\n' ' ' <<< "<line_separated_commits>"` to get a space separated list
+from the copy&paste.
+
+Where it makes sense to do so, squash carried changes that are tightly coupled to
+simplify future rebases. If the commit message of a carry does not conform to
+expectations, feel free to revise and note the change in the spreadsheet row for the
+commit.
+
+If you first pick all the pick+squash commits first and push them for review it is easier for you
+and your reviewers to check the code changes and you squash it at the end.
+
+When filling in Clean column in the spreadsheet make sure to use the following
+number to express the complexity of the pick:
+- 0 - clean
+- 1 - format fixups
+- 2 - code fixups
+- 3 - logic changes
+
+Explicit commit rules:
+- Anything touching `openshift-hack/`, openshift specific READMEs or similar files
+  should be squashed to 1 commit named "UPSTREAM: <carry>: Add OpenShift specific files"
+- Updating generated files coming from kubernetes should be `<drop>` commit
+- Generated changes should never be mixed with non-generated changes. If a carry is
+  ever seen to contain generated changes, those changes should be dropped.
+
+## Update the hyperkube image version to the release tag
+
+The [hyperkube image](openshift-hack/images/hyperkube/Dockerfile.rhel)
+hard-codes the Kubernetes version in an image label. It's necessary to manually
+set this label to the new release tag. Prefix the commit summary with
+`UPSTREAM: <carry>: (squash)` and squash it before merging the rebase PR.
+
+This value, among other things, is used by ART to inject appropriate version of
+kubernetes during build process, so it always has to reflect correct level of
+kubernetes.
+
+## Update base-os and test images
+
+To be able to use the latest kubelet from a pull request, in this repository
+we build [machine-os-content image](openshift-hack/images/os/Dockerfile).
+Make sure that both `FROM` and `curl` operation in `RUN` command use appropriate
+OCP version which corresponds with what we have in the [hyperkube image](openshift-hack/images/hyperkube/Dockerfile.rhel).
+
+Similarly, update `FROM` in [test image](openshift-hack/images/tests/Dockerfile.rhel)
+to match the one from [hyperkube image](openshift-hack/images/hyperkube/Dockerfile.rhel).
+
+## Updating dependencies
+
+Once the commits are all picked from the previous rebase branch, and your PR
+is mostly ready, each of the following repositories need to be updated to depend
+on the upstream tag targeted by the rebase:
+
+- https://github.com/openshift/api
+- https://github.com/openshift/apiserver-library-go
+- https://github.com/openshift/client-go
+- https://github.com/openshift/library-go
+
+Often these repositories are updated in parallel by other team members, so make
+sure to ask around before starting the work of bumping their dependencies.
+
+Once the above repos have been updated to depend on the target release,
+it will be necessary to update `go.mod` to point to the appropriate revision
+of these repos by running `hack/pin-dependency.sh` for each of them and then running
+`hack/update-vendor.sh` (as per the [upstream documentation](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/vendor.md#adding-or-updating-a-dependency)).
+
+Make sure to commit the result of a vendoring update with `UPSTREAM: <drop>: bump(*)`.
+If you have already bumped the dependencies to get the repo to compile,
+don't forget to squash the commits before merging the PR.
+
+### Updating dependencies for pending bumps
+
+The upstream `hack/pin-dependency.sh` script only supports setting dependency
+for the original repository. To pin to a fork branch that has not yet been
+merged (i.e. to test a rebase ahead of shared library bumps having merged), the
+following `go mod` invocations are suggested:
+
+```
+go mod edit -replace github.com/openshift/<lib>=github.com/<username>/<lib>@SHA
+go mod tidy && go mod vendor
+```
+
+Alternatively, you can edit `go.mod` file manually with your favourite editor and use search&replace.
+
+## Review test annotation rules
+
+The names of upstream e2e tests are annotated according to the a set of
+[declarative rules](openshift-hack/e2e/annotate/rules.go). These annotations
+are used to group tests into suites and to skip tests that are known not to be
+incompatible with some or all configurations of OpenShift.
+
+When performing a rebase, it is important to review the rules to
+ensure they are still relevant:
+
+- [ ] Ensure that `[Disabled:Alpha]` rules are appropriate for the current kube
+      level. Alpha features that are not enabled by default should be targeted
+      by this annotation to ensure that tests of those features are skipped.
+- [ ] Add new skips (along with a bz to track resolution) where e2e tests fail
+      consistently.
+
+Test failures representing major issues affecting cluster capability will
+generally need to be addressed before merge of the rebase PR, but minor issues
+(e.g. tests that fail to execute correctly but don't appear to reflect a
+regression in behavior) can often be skipped and addressed post-merge.
+
+## Updating generated files
+
+- Update generated files by running `make update`
+  - This step depends on etcd being installed in the path, which can be
+    accomplished by running `hack/install-etcd.sh`.
+  - Alternatively, run it in the same container as CI is using for build_root that already has
+    the etcd at correct version
+```
+podman run -it --rm -v $( pwd ):/go/k8s.io/kubernetes:Z --workdir=/go/k8s.io/kubernetes registry.ci.openshift.org/openshift/release:rhel-8-release-golang-1.19-openshift-4.12 make update OS_RUN_WITHOUT_DOCKER=yes FORCE_HOST_GO=1
+```
+- Commit the resulting changes as `UPSTREAM: <drop>: make update`.
+
+## Building and testing
+
+- Build the code with `make`
+- Test the code with `make test`
+  - Where test failures are encountered and can't be trivially resolved, the
+    spreadsheet can be used to track those failures to their resolution. The
+    example spreadsheet should have a sheet that demonstrates this tracking.
+  - Where a test failure proves challenging to fix without specialized knowledge,
+    make sure to coordinate with the team(s) responsible for area(s) of focus
+    exhibiting test failure. If in doubt, ask for help!
+- Verify the code with `make verify`
+
+## Reacting to new commits
+
+Inevitably, a rebase will take long enough that new commits will end up being
+merged to the targeted openshift/kubernetes branch after the rebase is
+underway. The following strategy is suggested to minimize the cost of incorporating
+these new commits:
+
+- rename existing rebase branch (e.g. 1.25.0-beta.2 -> 1.25.0-beta.2-old)
+- create new rebase branch from HEAD of master
+- merge the target upstream tag (e.g. 1.25.0-beta.2) with strategy ours
+- pick all carries from renamed rebase branch (e.g. 1.25.0-beta.2-old)
+- pick new carries from the openshift/kubernetes target branch
+- add details of the new carries to the spreadsheet
+- update generated files
+
+With good tooling, the cost of this procedure should be ~10 minutes at
+most. Re-picking carries should not result in conflicts since the base of the
+rebase branch will be the same as before. The only potential sources of conflict
+will be the newly added commits.
+
+## Ensuring the stability of the release
+
+To ensure we don't regress the product by introducing a new level of kubernetes
+it is required to create a new sheet in the following spreadsheet and pass all
+the variants: https://docs.google.com/spreadsheets/d/1PBk3eqYaPbvY982k_a0W7EGx7CBCHTmKrN6FyNSTDeA/edit#gid=0
+
+NOTE: Double check with TRT team if the current variants in that spreadsheet
+are up-to-date.
+
+## Send email announcing upcoming merge
+
+Second email should be send close O(~3 days) to merging the bump:
+
+```
+Title: k8s <version> bump landing...
+
+<URL to your o/k PR> is bumping k8s to version <version>.
+The following repositories have been already bumped as well:
+
+<URLs to api/client-go/library-go/apiserver-library-go/operators>
+
+Followup work has been assigned to appropriate teams
+through bugzillas linked in the code. Please treat
+them as the highest priority after landing the bump.
+
+Finally, this means we are blocking ALL PRs to our
+kubernetes fork.
+```
+
+After sending the email block the merge queue, see below.
+
+## Blocking the merge queue
+
+Close to merging a rebase it is good practice to block any merges to openshift/kubernetes
+fork. To do that follow these steps:
+
+1. Open new issues in openshift/kubernetes
+2. Use `Master Branch Frozen For Kubernetes Merging | branch:master` as issue title
+3. Add `tide/merge-blocker` label to issues (you might need group lead for this)
+4. All PR's  (including the rebase) are now forbidden to merge to master branch
+5. Before landing the rebase PR, close this issue
+
+## Send email announcing work done
+
+Last email should be send after merging the bump as a
+reply to previous:
+
+```
+<URL to your o/k PR> just merged.
+It'll take some time to get newer kublet, but in the meantime we'll
+continue to monitor CI. I encourage everyone to hold off from
+merging any major changes to our kubernetes fork to provide clear CI
+signal for the next 2-3 days.
+
+The following bugs were opened during the process, please treat
+them as the highest priority and release blockers for your team:
+
+<list bugs and assigned team>
+```
+
+## Followup work
+
+1. Update cluster-kube-apiserver-operator `pre-release-lifecycle` alert's
+`removed_release` version similarly to https://github.com/openshift/cluster-kube-apiserver-operator/pull/1382.
+
+## Updating with `git merge`
+
+*This is the preferred way to update to patch releases of kubernetes*
+
+After the initial bump as described above it is possible to update
+to newer released version using `git merge`. To do that follow these steps:
+
+
+1. Fetch latest upstream changes:
+```
+git fetch upstream
+```
+   where `upstream` points at https://github.com/kubernetes/kubernetes/, and check
+   the incoming changes:
+```
+git log v1.25.0..v1.25.2 --ancestry-path --reverse --no-merges
+```
+2. (optional) Revert any commits that were merged into kubernetes between previous
+   update and current one.
+
+3. Fetch latest state of openshift fork, checkout the appropriate branch and
+   create a new branch for the bump
+```
+git fetch openshift
+git checkout openshift/release-4.12
+git checkout -b bump-1.25.2
+```
+   where `openshift` points at https://github.com/openshift/kubernetes/.
+
+4. Merge the changes from appropriate [released version](https://kubernetes.io/releases/patch-releases/#detailed-release-history-for-active-branches):
+```
+git merge v1.25.2
+```
+   Most likely you'll encounter conflicts, but most are around go.sum and go.mod
+   files, coming from newer versions, but at this point in time leave the conflicts
+   as they are and continue the merge.
+```
+git add --all
+git merge --continue
+```
+   This should create a commit titled `Merge tag 'v1.25.2' into bump-1.25.2`.
+
+5. Now return to the list of conflicts from previous step and fix all the files
+   picking appropriate changes, in most cases picking the newer version.
+   When done, commit all of them as another commit:
+```
+git add --all
+git commit -m "UPSTREAM: <drop>: manually resolve conflicts"
+```
+   This ensures the person reviewing the bump can easily review all the conflicts
+   and their resolution.
+
+6. (optional) Update openshift dependencies and run `go mod tidy` to have the
+   branch names resolved to proper go mod version. Remember to use the released
+   versions matching the branch you're modifying.
+   This is usually required ONLY if you know there has been changes in one of
+   the libraries that need to be applied to our fork, which happens rarely.
+   Also usually, this is done by the team introducing the changes in the libraries.
+
+7. Run `/bin/bash` in a container using the command and image described in [Updating generated files](#updating-generated-files)
+   section:
+```
+podman run -it --rm -v $( pwd ):/go/k8s.io/kubernetes:Z --workdir=/go/k8s.io/kubernetes registry.ci.openshift.org/openshift/release:rhel-8-release-golang-1.19-openshift-4.12 /bin/bash
+```
+   In the container run `hack/update-vendor.sh` and `make update OS_RUN_WITHOUT_DOCKER=yes FORCE_HOST_GO=1`.
+
+NOTE: Make sure to use the correct version of the image (both openshift and golang
+versions must be appropriate), as a reference check `openshift-hack/images/hyperkube/Dockerfile.rhel`
+file.
+
+NOTE: You might encounter problems when running the above, make sure to check [Potential problems](#potential-problems)
+section below.
+
+
+8. Update kubernetes version in `openshift-hack/images/hyperkube/Dockerfile.rhel`
+   and commit all of that as:
+```
+git commit -m "UPSTREAM: <drop>: hack/update-vendor.sh, make update and update image"
+```
+
+9. Congratulations, you can open a PR with updated k8s patch version!
+
+### Potential problems
+
+While running `make update` in step 7 above, you might encounter one of the following problems:
+
+```
+go: inconsistent vendoring in /go/k8s.io/kubernetes/_output/local/go/src/k8s.io/kubernetes/vendor/k8s.io/code-generator:
+```
+To solve it, edit `staging/src/k8s.io/code-generator/go.mod` removing this line: `k8s.io/code-generator => ../code-generator`.
+Try re-running `make update`, if the problem re-appears change directory to `staging/src/k8s.io/code-generator`
+and run `go mod tidy` and `go mod vendor`.
+
+NOTE: Make sure to bring back this line: `k8s.io/code-generator => ../code-generator` in `staging/src/k8s.io/code-generator/go.mod`
+after you've run `make update`, otherwise `verify` step will fail during submission.
+
+```
+etcd version 3.5.6 or greater required
+```
+Grab newer version of etcd from https://github.com/etcd-io/etcd/releases/ and place
+it in `/usr/local/bin/etcd`.
+
+## Updating with `rebase.sh` (experimental)
+
+The above steps are available as a script that will merge and rebase along the happy path without automatic conflict
+resolution and at the end will create a PR for you.
+
+Here are the steps:
+1. Create a new BugZilla with the respective OpenShift version to rebase (Target Release stays ---),
+   Prio&Severity to High with a proper description of the change logs.
+   See [BZ2021468](https://bugzilla.redhat.com/show_bug.cgi?id=2021468) as an example.
+2. It's best to start off with a fresh fork of [openshift/kubernetes](https://github.com/openshift/kubernetes/). Stay on the master branch.
+3. This script requires `jq`, `git`, `podman` and `bash`, `gh` is optional.
+4. In the root dir of that fork run:
+```
+openshift-hack/rebase.sh --k8s-tag=v1.25.2 --openshift-release=release-4.12 --bugzilla-id=2003027
+```
+
+where `k8s-tag` is the [kubernetes/kubernetes](https://github.com/kubernetes/kubernetes/) release tag, the `openshift-release`
+is the OpenShift release branch in [openshift/kubernetes](https://github.com/openshift/kubernetes/) and the `bugzilla-id` is the
+BugZilla ID created in step (1).
+
+5. In case of conflicts, it will ask you to step into another shell to resolve those. The script will continue by committing the resolution with `UPSTREAM: <drop>`.
+6. At the end, there will be a "rebase-$VERSION" branch pushed to your fork.
+7. If you have `gh` installed and are logged in, it will attempt to create a PR for you by opening a web browser.
diff --git a/build/pause/Dockerfile.Rhel b/build/pause/Dockerfile.Rhel
new file mode 100644
index 00000000000..64b64a4287f
--- /dev/null
+++ b/build/pause/Dockerfile.Rhel
@@ -0,0 +1,13 @@
+FROM registry.ci.openshift.org/ocp/builder:rhel-8-golang-1.20-openshift-4.14 AS builder
+WORKDIR /go/src/github.com/openshift/kubernetes/build/pause
+COPY . .
+RUN dnf install -y gcc glibc-static && \
+    mkdir -p bin && \
+    gcc -Os -Wall -Werror -static -o bin/pause ./linux/pause.c
+
+FROM registry.ci.openshift.org/ocp/4.14:base
+COPY --from=builder /go/src/github.com/openshift/kubernetes/build/pause/bin/pause /usr/bin/pod
+LABEL io.k8s.display-name="OpenShift Pod" \
+      io.k8s.description="This is a component of OpenShift and contains the binary that holds the pod namespaces." \
+      io.openshift.tags="openshift"
+ENTRYPOINT [ "/usr/bin/pod" ]
diff --git a/build/run.sh b/build/run.sh
index 3ecc2dacb77..c0eb0b83270 100755
--- a/build/run.sh
+++ b/build/run.sh
@@ -25,6 +25,12 @@ set -o pipefail
 KUBE_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
 source "$KUBE_ROOT/build/common.sh"
 
+# Allow running without docker (e.g. in openshift ci)
+if [[ "${OS_RUN_WITHOUT_DOCKER:-}" ]]; then
+  "${@}"
+  exit 0
+fi
+
 KUBE_RUN_COPY_OUTPUT="${KUBE_RUN_COPY_OUTPUT:-y}"
 
 kube::build::verify_prereqs
diff --git a/cmd/watch-termination/main.go b/cmd/watch-termination/main.go
new file mode 100644
index 00000000000..aa3aa880085
--- /dev/null
+++ b/cmd/watch-termination/main.go
@@ -0,0 +1,366 @@
+package main
+
+import (
+	"context"
+	"flag"
+	"fmt"
+	"io"
+	"io/ioutil"
+	"os"
+	"os/exec"
+	"os/signal"
+	"strings"
+	"sync"
+	"syscall"
+	"time"
+
+	"gopkg.in/natefinch/lumberjack.v2"
+
+	corev1 "k8s.io/api/core/v1"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/client-go/kubernetes"
+	corev1client "k8s.io/client-go/kubernetes/typed/core/v1"
+	"k8s.io/client-go/tools/clientcmd"
+	"k8s.io/client-go/util/retry"
+	"k8s.io/klog/v2"
+)
+
+func main() {
+	os.Exit(run())
+}
+
+func run() int {
+	terminationLog := flag.String("termination-log-file", "", "Write logs after SIGTERM to this file (in addition to stderr)")
+	terminationLock := flag.String("termination-touch-file", "", "Touch this file on SIGTERM and delete on termination")
+	processOverlapDetectionFile := flag.String("process-overlap-detection-file", "", "This file is present when the kube-apiserver initialization timed out while waiting for kubelet to terminate old process")
+	kubeconfigPath := flag.String("kubeconfig", "", "Optional kubeconfig used to create events")
+	gracefulTerminatioPeriod := flag.Duration("graceful-termination-duration", 105*time.Second, "The duration of the graceful termination period, e.g. 105s")
+
+	klog.InitFlags(nil)
+	flag.Set("v", "9")
+
+	// never log to stderr, only through our termination log writer (which sends it also to stderr)
+	flag.Set("logtostderr", "false")
+	flag.Set("stderrthreshold", "99")
+
+	flag.Parse()
+	args := flag.CommandLine.Args()
+
+	if len(args) == 0 {
+		fmt.Println("Missing command line")
+		return 1
+	}
+
+	// use special tee-like writer when termination log is set
+	termCh := make(chan struct{})
+	var stderr io.Writer = os.Stderr
+	var terminationLogger *terminationFileWriter
+	if len(*terminationLog) > 0 {
+		terminationLogger = &terminationFileWriter{
+			Writer:             os.Stderr,
+			fn:                 *terminationLog,
+			startFileLoggingCh: termCh,
+		}
+		stderr = terminationLogger
+
+		// do the klog file writer dance: klog writes to all outputs of lower
+		// severity. No idea why. So we discard for anything other than info.
+		// Otherwise, we would see errors multiple times.
+		klog.SetOutput(ioutil.Discard)
+		klog.SetOutputBySeverity("INFO", stderr)
+	}
+
+	var client kubernetes.Interface
+	if len(*kubeconfigPath) > 0 {
+		loader := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(&clientcmd.ClientConfigLoadingRules{ExplicitPath: *kubeconfigPath}, &clientcmd.ConfigOverrides{})
+		if cfg, err := loader.ClientConfig(); err != nil {
+			klog.Errorf("failed to load kubeconfig %q: %v", *kubeconfigPath, err)
+			return 1
+		} else {
+			client = kubernetes.NewForConfigOrDie(cfg)
+		}
+	}
+
+	if processOverlapDetectionFile != nil && len(*processOverlapDetectionFile) > 0 {
+		var deleteDetectionFileOnce sync.Once
+
+		if _, err := os.Stat(*processOverlapDetectionFile); err != nil && !os.IsNotExist(err) {
+			klog.Errorf("failed to read process overlap detection file %q: %v", *processOverlapDetectionFile, err)
+			return 1
+		} else if err == nil {
+			ref, err := eventReference()
+			if err != nil {
+				klog.Errorf("failed to get event target: %v", err)
+				return 1
+			}
+			go func() {
+				defer deleteDetectionFileOnce.Do(func() {
+					if err := os.Remove(*processOverlapDetectionFile); err != nil {
+						klog.Warningf("Failed to remove process overlap termination file %q: %v", *processOverlapDetectionFile, err)
+					}
+				})
+				if err := retry.OnError(retry.DefaultBackoff, func(err error) bool {
+					select {
+					case <-termCh:
+						// stop retry on termination
+						return false
+					default:
+					}
+					// every error is retriable
+					return true
+				}, func() error {
+					return eventf(client.CoreV1().Events(ref.Namespace), *ref, corev1.EventTypeWarning, "TerminationProcessOverlapDetected", "The kube-apiserver initialization timed out while waiting for kubelet to terminate old process")
+				}); err != nil {
+					klog.Warning(err)
+				}
+			}()
+		}
+	}
+
+	// touch file early. If the file is not removed on termination, we are not
+	// terminating cleanly via SIGTERM.
+	if len(*terminationLock) > 0 {
+		ref, err := eventReference()
+		if err != nil {
+			klog.Errorf("failed to get event target: %v", err)
+			return 1
+		}
+
+		if st, err := os.Stat(*terminationLock); err == nil {
+			podName := "unknown"
+			if v := os.Getenv("POD_NAME"); len(v) > 0 {
+				podName = v // pod name is always the same for static pods
+			}
+			msg := fmt.Sprintf("Previous pod %s started at %s did not terminate gracefully", podName, st.ModTime().String())
+
+			klog.Warning(msg)
+			_, _ = terminationLogger.WriteToTerminationLog([]byte(msg + "\n"))
+
+			go retry.OnError(retry.DefaultBackoff, func(err error) bool {
+				select {
+				case <-termCh:
+					// stop retry on termination
+					return false
+				default:
+				}
+				// every error is retriable
+				return true
+			}, func() error {
+				return eventf(client.CoreV1().Events(ref.Namespace), *ref, corev1.EventTypeWarning, "NonGracefulTermination", msg)
+			})
+
+			klog.Infof("Deleting old termination lock file %q", *terminationLock)
+			if err := os.Remove(*terminationLock); err != nil {
+				klog.Errorf("Old termination lock file deletion failed: %v", err)
+			}
+		}
+
+		// separation to see where the new one is starting
+		_, _ = terminationLogger.WriteToTerminationLog([]byte("---\n"))
+
+		klog.Infof("Touching termination lock file %q", *terminationLock)
+		if err := touch(*terminationLock); err != nil {
+			klog.Infof("Error touching %s: %v", *terminationLock, err)
+			// keep going
+		}
+
+		var deleteLockOnce sync.Once
+
+		if *gracefulTerminatioPeriod > 2*time.Second {
+			go func() {
+				<-termCh
+				<-time.After(*gracefulTerminatioPeriod - 2*time.Second)
+
+				deleteLockOnce.Do(func() {
+					klog.Infof("Graceful termination time nearly passed and kube-apiserver has still not terminated. Deleting termination lock file %q to avoid a false positive.", *terminationLock)
+					if err := os.Remove(*terminationLock); err != nil {
+						klog.Errorf("Termination lock file deletion failed: %v", err)
+					}
+
+					if err := eventf(client.CoreV1().Events(ref.Namespace), *ref, corev1.EventTypeWarning, "GracefulTerminationTimeout", "kube-apiserver did not terminate within %s", *gracefulTerminatioPeriod); err != nil {
+						klog.Error(err)
+					}
+				})
+			}()
+		}
+
+		defer deleteLockOnce.Do(func() {
+			klog.Infof("Deleting termination lock file %q", *terminationLock)
+			if err := os.Remove(*terminationLock); err != nil {
+				klog.Errorf("Termination lock file deletion failed: %v", err)
+			}
+		})
+	}
+
+	cmd := exec.Command(args[0], args[1:]...)
+	cmd.Stdout = os.Stdout
+	cmd.Stderr = stderr
+
+	// forward SIGTERM and SIGINT to child
+	sigCh := make(chan os.Signal, 1)
+	signal.Notify(sigCh, syscall.SIGTERM, syscall.SIGINT)
+	var wg sync.WaitGroup
+	wg.Add(1)
+	go func() {
+		defer wg.Done()
+		for s := range sigCh {
+			select {
+			case <-termCh:
+			default:
+				close(termCh)
+			}
+
+			klog.Infof("Received signal %s. Forwarding to sub-process %q.", s, args[0])
+
+			cmd.Process.Signal(s)
+		}
+	}()
+
+	klog.Infof("Launching sub-process %q", cmd)
+	rc := 0
+	if err := cmd.Run(); err != nil {
+		if exitError, ok := err.(*exec.ExitError); ok {
+			rc = exitError.ExitCode()
+		} else {
+			klog.Infof("Failed to launch %s: %v", args[0], err)
+			return 255
+		}
+	}
+
+	// remove signal handling
+	signal.Stop(sigCh)
+	close(sigCh)
+	wg.Wait()
+
+	klog.Infof("Termination finished with exit code %d", rc)
+	return rc
+}
+
+// terminationFileWriter forwards everything to the embedded writer. When
+// startFileLoggingCh is closed, everything is appended to the given file name
+// in addition.
+type terminationFileWriter struct {
+	io.Writer
+	fn                 string
+	startFileLoggingCh <-chan struct{}
+
+	logger io.Writer
+}
+
+func (w *terminationFileWriter) WriteToTerminationLog(bs []byte) (int, error) {
+	if w == nil {
+		return len(bs), nil
+	}
+
+	if w.logger == nil {
+		l := &lumberjack.Logger{
+			Filename:   w.fn,
+			MaxSize:    100,
+			MaxBackups: 3,
+			MaxAge:     28,
+			Compress:   false,
+		}
+		w.logger = l
+		fmt.Fprintf(os.Stderr, "Copying termination logs to %q\n", w.fn)
+	}
+	if n, err := w.logger.Write(bs); err != nil {
+		return n, err
+	} else if n != len(bs) {
+		return n, io.ErrShortWrite
+	}
+	return len(bs), nil
+}
+
+func (w *terminationFileWriter) Write(bs []byte) (int, error) {
+	// temporary hack to avoid logging sensitive tokens.
+	// TODO: drop when we moved to a non-sensitive storage format
+	if strings.Contains(string(bs), "URI=\"/apis/oauth.openshift.io/v1/oauthaccesstokens/") || strings.Contains(string(bs), "URI=\"/apis/oauth.openshift.io/v1/oauthauthorizetokens/") {
+		return len(bs), nil
+	}
+
+	select {
+	case <-w.startFileLoggingCh:
+		if n, err := w.WriteToTerminationLog(bs); err != nil {
+			return n, err
+		}
+	default:
+	}
+
+	return w.Writer.Write(bs)
+}
+
+func touch(fn string) error {
+	_, err := os.Stat(fn)
+	if os.IsNotExist(err) {
+		file, err := os.Create(fn)
+		if err != nil {
+			return err
+		}
+		defer file.Close()
+		return nil
+	}
+
+	currentTime := time.Now().Local()
+	return os.Chtimes(fn, currentTime, currentTime)
+}
+
+func eventf(client corev1client.EventInterface, ref corev1.ObjectReference, eventType, reason, messageFmt string, args ...interface{}) error {
+	t := metav1.Time{Time: time.Now()}
+	host, _ := os.Hostname() // expicitly ignore error. Empty host is fine
+
+	e := &corev1.Event{
+		ObjectMeta: metav1.ObjectMeta{
+			Name:      fmt.Sprintf("%v.%x", ref.Name, t.UnixNano()),
+			Namespace: ref.Namespace,
+		},
+		InvolvedObject: ref,
+		Reason:         reason,
+		Message:        fmt.Sprintf(messageFmt, args...),
+		FirstTimestamp: t,
+		LastTimestamp:  t,
+		Count:          1,
+		Type:           eventType,
+		Source:         corev1.EventSource{Component: "apiserver", Host: host},
+	}
+
+	_, err := client.Create(context.TODO(), e, metav1.CreateOptions{})
+
+	if err == nil {
+		klog.V(2).Infof("Event(%#v): type: '%v' reason: '%v' %v", e.InvolvedObject, e.Type, e.Reason, e.Message)
+	}
+
+	return err
+}
+
+func eventReference() (*corev1.ObjectReference, error) {
+	ns := os.Getenv("POD_NAMESPACE")
+	pod := os.Getenv("POD_NAME")
+	if len(ns) == 0 && len(pod) > 0 {
+		serviceAccountNamespaceFile := "/var/run/secrets/kubernetes.io/serviceaccount/namespace"
+		if _, err := os.Stat(serviceAccountNamespaceFile); err == nil {
+			bs, err := ioutil.ReadFile(serviceAccountNamespaceFile)
+			if err != nil {
+				return nil, err
+			}
+			ns = string(bs)
+		}
+	}
+	if len(ns) == 0 {
+		pod = ""
+		ns = "kube-system"
+	}
+	if len(pod) == 0 {
+		return &corev1.ObjectReference{
+			Kind:       "Namespace",
+			Name:       ns,
+			APIVersion: "v1",
+		}, nil
+	}
+
+	return &corev1.ObjectReference{
+		Kind:       "Pod",
+		Namespace:  ns,
+		Name:       pod,
+		APIVersion: "v1",
+	}, nil
+}
diff --git a/hack/lib/golang.sh b/hack/lib/golang.sh
index f3466ff76bd..b353bbc06f2 100755
--- a/hack/lib/golang.sh
+++ b/hack/lib/golang.sh
@@ -80,6 +80,8 @@ kube::golang::server_targets() {
     vendor/k8s.io/kube-aggregator
     vendor/k8s.io/apiextensions-apiserver
     cluster/gce/gci/mounter
+    cmd/watch-termination
+    openshift-hack/cmd/k8s-tests
   )
   echo "${targets[@]}"
 }
@@ -318,20 +320,7 @@ readonly KUBE_ALL_TARGETS=(
 )
 readonly KUBE_ALL_BINARIES=("${KUBE_ALL_TARGETS[@]##*/}")
 
-readonly KUBE_STATIC_LIBRARIES=(
-  apiextensions-apiserver
-  kube-aggregator
-  kube-apiserver
-  kube-controller-manager
-  kube-scheduler
-  kube-proxy
-  kube-log-runner
-  kubeadm
-  kubectl
-  kubectl-convert
-  kubemark
-  mounter
-)
+readonly KUBE_STATIC_LIBRARIES=()
 
 # Fully-qualified package names that we want to instrument for coverage information.
 readonly KUBE_COVERAGE_INSTRUMENTED_PACKAGES=(
@@ -361,7 +350,9 @@ kube::golang::is_statically_linked_library() {
   if [[ -n "${KUBE_CGO_OVERRIDES_LIST:+x}" ]]; then
     for e in "${KUBE_CGO_OVERRIDES_LIST[@]}"; do [[ "${1}" == *"/${e}" ]] && return 1; done;
   fi
-  for e in "${KUBE_STATIC_LIBRARIES[@]}"; do [[ "${1}" == *"/${e}" ]] && return 0; done;
+  if [[ -n "${KUBE_STATIC_LIBRARIES:+x}" ]]; then
+    for e in "${KUBE_STATIC_LIBRARIES[@]}"; do [[ "${1}" == *"/${e}" ]] && return 0; done;
+  fi
   if [[ -n "${KUBE_STATIC_OVERRIDES_LIST:+x}" ]]; then
     for e in "${KUBE_STATIC_OVERRIDES_LIST[@]}"; do [[ "${1}" == *"/${e}" ]] && return 0; done;
   fi
@@ -440,7 +431,7 @@ kube::golang::set_platform_envs() {
 
   # if CC is defined for platform then always enable it
   ccenv=$(echo "$platform" | awk -F/ '{print "KUBE_" toupper($1) "_" toupper($2) "_CC"}')
-  if [ -n "${!ccenv-}" ]; then 
+  if [ -n "${!ccenv-}" ]; then
     export CGO_ENABLED=1
     export CC="${!ccenv}"
   fi
@@ -465,27 +456,6 @@ kube::golang::create_gopath_tree() {
 #   env-var GO_VERSION is the desired go version to use, downloading it if needed (defaults to content of .go-version)
 #   env-var FORCE_HOST_GO set to a non-empty value uses the go version in the $PATH and skips ensuring $GO_VERSION is used
 kube::golang::verify_go_version() {
-  # default GO_VERSION to content of .go-version
-  GO_VERSION="${GO_VERSION:-"$(cat "${KUBE_ROOT}/.go-version")"}"
-  if [ "${GOTOOLCHAIN:-auto}" != 'auto' ]; then
-    # no-op, just respect GOTOOLCHAIN
-    :
-  elif [ -n "${FORCE_HOST_GO:-}" ]; then
-    # ensure existing host version is used, like before GOTOOLCHAIN existed
-    export GOTOOLCHAIN='local'
-  else
-    # otherwise, we want to ensure the go version matches GO_VERSION
-    GOTOOLCHAIN="go${GO_VERSION}"
-    export GOTOOLCHAIN
-    # if go is either not installed or too old to respect GOTOOLCHAIN then use gimme
-    if ! (command -v go >/dev/null && [ "$(go version | cut -d' ' -f3)" = "${GOTOOLCHAIN}" ]); then
-      export GIMME_ENV_PREFIX=${GIMME_ENV_PREFIX:-"${KUBE_OUTPUT}/.gimme/envs"}
-      export GIMME_VERSION_PREFIX=${GIMME_VERSION_PREFIX:-"${KUBE_OUTPUT}/.gimme/versions"}
-      # eval because the output of this is shell to set PATH etc.
-      eval "$("${KUBE_ROOT}/third_party/gimme/gimme" "${GO_VERSION}")"
-    fi
-  fi
-
   if [[ -z "$(command -v go)" ]]; then
     kube::log::usage_from_stdin <<EOF
 Can't find 'go' in PATH, please fix and retry.
@@ -576,7 +546,7 @@ kube::golang::setup_gomaxprocs() {
     if ! command -v ncpu >/dev/null 2>&1; then
       # shellcheck disable=SC2164
       pushd "${KUBE_ROOT}/hack/tools" >/dev/null
-      GO111MODULE=on go install ./ncpu || echo "Will not automatically set GOMAXPROCS"
+      GO111MODULE=on go install -mod=readonly ./ncpu || echo "Will not automatically set GOMAXPROCS"
       # shellcheck disable=SC2164
       popd >/dev/null
     fi
diff --git a/hack/make-rules/test.sh b/hack/make-rules/test.sh
index 4aa72730d83..deee3361840 100755
--- a/hack/make-rules/test.sh
+++ b/hack/make-rules/test.sh
@@ -52,6 +52,7 @@ kube::test::find_dirs() {
           -o -path './third_party/*' \
           -o -path './staging/*' \
           -o -path './vendor/*' \
+          -o -path './openshift-hack/e2e/*' \
         \) -prune \
       \) -name '*_test.go' -print0 | xargs -0n1 dirname | sed "s|^\./|${KUBE_GO_PACKAGE}/|" | LC_ALL=C sort -u
 
diff --git a/hack/make-rules/update.sh b/hack/make-rules/update.sh
index 1d8b5dbcac7..52d5155811c 100755
--- a/hack/make-rules/update.sh
+++ b/hack/make-rules/update.sh
@@ -36,6 +36,8 @@ if ! ${ALL} ; then
 fi
 
 BASH_TARGETS=(
+	update-kubensenter
+	update-test-annotations
 	update-codegen
 	update-generated-api-compatibility-data
 	update-generated-docs
diff --git a/hack/make-rules/verify.sh b/hack/make-rules/verify.sh
index a322a165e71..14bdfc08a96 100755
--- a/hack/make-rules/verify.sh
+++ b/hack/make-rules/verify.sh
@@ -39,6 +39,41 @@ EXCLUDED_PATTERNS=(
   "verify-openapi-docs-urls.sh"  # Spams docs URLs, don't run in CI.
   )
 
+# Excluded checks for openshift/kubernetes fork that are always skipped.
+EXCLUDED_PATTERNS+=(
+  "verify-boilerplate.sh"            # Carries do not require boilerplate
+  "verify-bazel.sh"                  # Bazel is not used downstream
+  "verify-no-vendor-cycles.sh"       # Incompatible with the way many carries are specified
+  "verify-publishing-bot.py"         # Verifies the upstream rules, which are not maintained in o/k
+  "verify-staging-meta-files.sh"     # Staging meta files are not maintained downstream
+)
+
+# Skipped checks for openshift/kubernetes fork that need to be fixed.
+#
+# Where a check is excluded due to 'inconsistent behavior between
+# local and ci execution', the fix will require finding a way to
+# compare current and generated results without 'cp -a' since this
+# command does not execute without error in downstream ci.
+EXCLUDED_PATTERNS+=(
+  "verify-codegen.sh"                       # TODO(marun) Fix inconsistent behavior between local and ci execution
+  "verify-conformance-yaml.sh"              # TODO(soltysh) I don't expect us needing this
+  "verify-e2e-test-ownership.sh"            # TODO(soltysh) Is it worth fixing this check?
+  "verify-external-dependencies-version.sh" # TODO(soltysh) I don't expect us needing this
+  "verify-generated-files-remake.sh"        # TODO(marun) Is it worth fixing this check?
+  "verify-generated-protobuf.sh"            # TODO(marun) Fix inconsistent behavior between local and ci execution
+  "verify-golangci-lint.sh"                 # TODO(soltysh) Need golangci-lint
+  "verify-golint.sh"                        # TODO(marun) Cleanup carried code
+  "verify-hack-tools.sh"                    # TODO(marun) Fix inconsistent behavior between local and ci execution
+  "verify-internal-modules.sh"              # TODO(soltysh) Currently fails on our ginkgo dependency
+  "verify-openapi-spec.sh"                  # TODO(marun) Fix inconsistent behavior between local and ci execution
+  "verify-shellcheck.sh"                    # TODO(soltysh) Fix problems in openshift-hack shells
+  "verify-spelling.sh"                      # TODO(marun) Need to ensure installation of misspell command
+  "verify-staticcheck.sh"                   # TODO(marun) Fix inconsistent behavior between local and ci execution
+  "verify-vendor-licenses.sh"               # TODO(marun) Fix inconsistent behavior between local and ci execution
+  "verify-structured-logging.sh"            # TODO(soltysh) I don't expect us needed it now
+  "verify-mocks.sh"                         # TODO(soltysh) I don't expect us needed mocks re-generation
+)
+
 # Exclude typecheck in certain cases, if they're running in a separate job.
 if [[ ${EXCLUDE_TYPECHECK:-} =~ ^[yY]$ ]]; then
   EXCLUDED_PATTERNS+=(
diff --git a/hack/update-codegen.sh b/hack/update-codegen.sh
index 201aca0b004..8c90dc77a9c 100755
--- a/hack/update-codegen.sh
+++ b/hack/update-codegen.sh
@@ -796,7 +796,7 @@ function codegen::subprojects() {
         CODEGEN_PKG="${codegen}" \
         UPDATE_API_KNOWN_VIOLATIONS="${UPDATE_API_KNOWN_VIOLATIONS}" \
         API_KNOWN_VIOLATIONS_DIR="${API_KNOWN_VIOLATIONS_DIR}" \
-            ./hack/update-codegen.sh > >(indent) 2> >(indent >&2)
+            GOFLAGS=-mod=readonly ./hack/update-codegen.sh > >(indent) 2> >(indent >&2)
         popd >/dev/null
     done
 }
diff --git a/hack/update-kubensenter.sh b/hack/update-kubensenter.sh
new file mode 120000
index 00000000000..1b263065ff4
--- /dev/null
+++ b/hack/update-kubensenter.sh
@@ -0,0 +1 @@
+../openshift-hack/update-kubensenter.sh
\ No newline at end of file
diff --git a/hack/update-test-annotations.sh b/hack/update-test-annotations.sh
new file mode 120000
index 00000000000..ecf920cd8d6
--- /dev/null
+++ b/hack/update-test-annotations.sh
@@ -0,0 +1 @@
+../openshift-hack/update-test-annotations.sh
\ No newline at end of file
diff --git a/hack/update-vendor.sh b/hack/update-vendor.sh
index f1c46982fa0..779a5afe630 100755
--- a/hack/update-vendor.sh
+++ b/hack/update-vendor.sh
@@ -356,7 +356,8 @@ xargs -L 100 go mod edit -fmt
 # disallow transitive dependencies on k8s.io/kubernetes
 loopback_deps=()
 kube::util::read-array loopback_deps < <(go mod graph | grep ' k8s.io/kubernetes' || true)
-if [[ -n ${loopback_deps[*]:+"${loopback_deps[*]}"} ]]; then
+## Allow apiserver-library-go to depend on k8s.io/kubernetes
+if [[ -n ${loopback_deps[*]:+"${loopback_deps[*]}"} && ! "${loopback_deps[*]}" =~ github.com/openshift/apiserver-library-go ]]; then
   kube::log::error "Disallowed transitive k8s.io/kubernetes dependencies exist via the following imports:" >&22 2>&1
   kube::log::error "${loopback_deps[@]}" >&22 2>&1
   exit 1
@@ -400,7 +401,7 @@ hack/update-vendor-licenses.sh
 kube::log::status "vendor: creating OWNERS file" >&11
 rm -f "vendor/OWNERS"
 cat <<__EOF__ > "vendor/OWNERS"
-# See the OWNERS docs at https://go.k8s.io/owners
+See the OWNERS docs at https://go.k8s.io/owners
 
 options:
   # make root approval non-recursive
diff --git a/hack/verify-govet-levee.sh b/hack/verify-govet-levee.sh
index f087d59d1b9..e2fff073370 100755
--- a/hack/verify-govet-levee.sh
+++ b/hack/verify-govet-levee.sh
@@ -32,7 +32,7 @@ PATH="${GOBIN}:${PATH}"
 
 # Install levee
 pushd "${KUBE_ROOT}/hack/tools" >/dev/null
-  GO111MODULE=on go install github.com/google/go-flow-levee/cmd/levee
+  GO111MODULE=on go install -mod=readonly github.com/google/go-flow-levee/cmd/levee
 popd >/dev/null
 
 # Prefer full path for interaction with make vet
@@ -43,6 +43,6 @@ CONFIG_FILE="${KUBE_ROOT}/hack/testdata/levee/levee-config.yaml"
 targets=()
 while IFS='' read -r line; do
   targets+=("${line}")
-done < <(go list --find -e ./... | grep -E -v "/(build|third_party|vendor|staging|clientset_generated|hack)/")
+done < <(GO111MODULE=on go list --find -e ./... | grep -E -v "/(build|third_party|vendor|staging|clientset_generated|hack)/")
 
-go vet -vettool="${LEVEE_BIN}" -config="${CONFIG_FILE}" "${targets[@]}"
+GO111MODULE=on go vet -vettool="${LEVEE_BIN}" -config="${CONFIG_FILE}" "${targets[@]}"
diff --git a/hack/verify-kubensenter.sh b/hack/verify-kubensenter.sh
new file mode 120000
index 00000000000..01e1608f153
--- /dev/null
+++ b/hack/verify-kubensenter.sh
@@ -0,0 +1 @@
+../openshift-hack/verify-kubensenter.sh
\ No newline at end of file
diff --git a/hack/verify-pkg-names.sh b/hack/verify-pkg-names.sh
index c9904ff980b..5fdb9af6ff7 100755
--- a/hack/verify-pkg-names.sh
+++ b/hack/verify-pkg-names.sh
@@ -25,7 +25,7 @@ KUBE_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
 source "${KUBE_ROOT}/hack/lib/init.sh"
 
 cd "${KUBE_ROOT}"
-if git --no-pager grep -E $'^(import |\t)[a-z]+[A-Z_][a-zA-Z]* "[^"]+"$' -- '**/*.go' ':(exclude)vendor/*' ':(exclude)**/*.pb.go'; then
+if git --no-pager grep -E $'^(import |\t)[a-z]+[A-Z_][a-zA-Z]* "[^"]+"$' -- '**/*.go' ':(exclude)vendor/*' ':(exclude)**/vendor/*' ':(exclude)**/*.pb.go'; then
   echo "!!! Some package aliases break go conventions."
   echo "To fix these errors, do not use capitalized or underlined characters"
   echo "in pkg aliases. Refer to https://blog.golang.org/package-names for more info."
diff --git a/hack/verify-test-annotations.sh b/hack/verify-test-annotations.sh
new file mode 120000
index 00000000000..a9cbed2d324
--- /dev/null
+++ b/hack/verify-test-annotations.sh
@@ -0,0 +1 @@
+../openshift-hack/verify-test-annotations.sh
\ No newline at end of file
diff --git a/hack/verify-typecheck-providerless.sh b/hack/verify-typecheck-providerless.sh
index 70e4ef9d50a..180443d51c8 100755
--- a/hack/verify-typecheck-providerless.sh
+++ b/hack/verify-typecheck-providerless.sh
@@ -26,7 +26,7 @@ KUBE_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
 cd "${KUBE_ROOT}"
 # verify the providerless build
 # https://github.com/kubernetes/enhancements/blob/master/keps/sig-cloud-provider/1179-building-without-in-tree-providers/README.md
-hack/verify-typecheck.sh --skip-test --tags=providerless --ignore-dirs=test
+# hack/verify-typecheck.sh --skip-test --tags=providerless --ignore-dirs=test
 
 # verify using go list
 if _out="$(go list -mod=readonly -tags "providerless" -e -json  k8s.io/kubernetes/cmd/kubelet/... \
@@ -35,10 +35,10 @@ if _out="$(go list -mod=readonly -tags "providerless" -e -json  k8s.io/kubernete
     echo "Verify typecheck for providerless tag failed. Found restricted packages." >&2
     exit 1
 fi
-if _out="$(go list -mod=readonly -tags "providerless" -e -json  k8s.io/kubernetes/cmd/kube-apiserver/... \
-  | grep -e Azure/azure-sdk-for-go -e github.com/aws/aws-sdk-go -e google.golang.org/api \
-         -e Azure/go-autorest -e oauth2/google)"; then
-    echo "${_out}" >&2
-    echo "Verify typecheck for providerless tag failed. Found restricted packages." >&2
-    exit 1
-fi
+# if _out="$(go list -mod=readonly -tags "providerless" -e -json  k8s.io/kubernetes/cmd/kube-apiserver/... \
+#   | grep -e Azure/azure-sdk-for-go -e github.com/aws/aws-sdk-go -e google.golang.org/api \
+#          -e Azure/go-autorest -e oauth2/google)"; then
+#     echo "${_out}" >&2
+#     echo "Verify typecheck for providerless tag failed. Found restricted packages." >&2
+#     exit 1
+# fi
diff --git a/hack/verify-vendor.sh b/hack/verify-vendor.sh
index 587f19561cf..8339fe87458 100755
--- a/hack/verify-vendor.sh
+++ b/hack/verify-vendor.sh
@@ -84,8 +84,12 @@ pushd "${KUBE_ROOT}" > /dev/null 2>&1
     ret=1
   fi
 
+  # Given that we don't intend to publish staging repos from our fork,
+  # it does not seem necessary to ensure that dependencies will match
+  # across staging repos when published.
+  #
   # Verify we are pinned to matching levels
-  hack/lint-dependencies.sh >&2
+  #hack/lint-dependencies.sh >&2
 popd > /dev/null 2>&1
 
 if [[ ${ret} -gt 0 ]]; then
diff --git a/openshift-hack/build-go.sh b/openshift-hack/build-go.sh
new file mode 100755
index 00000000000..dfc663d23a5
--- /dev/null
+++ b/openshift-hack/build-go.sh
@@ -0,0 +1,21 @@
+#!/usr/bin/env bash
+
+STARTTIME=$(date +%s)
+
+# shellcheck source=openshift-hack/lib/init.sh
+source "$(dirname "${BASH_SOURCE[0]}")/lib/init.sh"
+
+pushd "${OS_ROOT}" > /dev/null || exit 1
+  make all WHAT='cmd/kube-apiserver cmd/kube-controller-manager cmd/kube-scheduler cmd/kubelet'
+popd > /dev/null || exit 1
+
+os::build::version::git_vars
+
+if [[ "${OS_GIT_TREE_STATE:-dirty}" == "clean"  ]]; then
+  # only when we are building from a clean state can we claim to
+  # have created a valid set of binaries that can resemble a release
+  mkdir -p "${OS_OUTPUT_RELEASEPATH}"
+  echo "${OS_GIT_COMMIT}" > "${OS_OUTPUT_RELEASEPATH}/.commit"
+fi
+
+ret=$?; ENDTIME=$(date +%s); echo "$0 took $((ENDTIME - STARTTIME)) seconds"; exit "$ret"
diff --git a/openshift-hack/build-rpms.sh b/openshift-hack/build-rpms.sh
new file mode 100755
index 00000000000..4ef11711d48
--- /dev/null
+++ b/openshift-hack/build-rpms.sh
@@ -0,0 +1,130 @@
+#!/usr/bin/env bash
+
+# This script generates RPMs into _output/releases. All build
+# dependencies are required on the host. The build will be performed
+# by the upstream makefile called from the spec file.
+# shellcheck source=openshift-hack/lib/init.sh
+source "$(dirname "${BASH_SOURCE[0]}")/lib/init.sh"
+
+# Only build linux by default. Clearing this value will build all platforms
+OS_ONLY_BUILD_PLATFORMS="${OS_ONLY_BUILD_PLATFORMS:-linux/amd64}"
+
+function cleanup() {
+	return_code=$?
+	os::util::describe_return_code "${return_code}"
+	exit "${return_code}"
+}
+trap "cleanup" EXIT
+
+# check whether we are in a clean output state
+dirty="$( if [[ -d "${OS_OUTPUT}" ]]; then echo '1'; fi )"
+
+os::util::ensure::system_binary_exists rpmbuild
+os::util::ensure::system_binary_exists createrepo
+
+if [[ -n "${OS_BUILD_SRPM-}" ]]; then
+	srpm="a"
+else
+	srpm="b"
+fi
+
+os::build::rpm::get_nvra_vars
+
+OS_RPM_SPECFILE="$( find "${OS_ROOT}" -name '*.spec' )"
+OS_RPM_NAME="$( rpmspec -q --qf '%{name}\n' "${OS_RPM_SPECFILE}" | head -1 )"
+
+os::log::info "Building release RPMs for ${OS_RPM_SPECFILE} ..."
+
+rpm_tmp_dir="${BASETMPDIR}/rpm"
+
+# RPM requires the spec file be owned by the invoking user
+chown "$(id -u):$(id -g)" "${OS_RPM_SPECFILE}" || true
+
+if [[ -n "${dirty}" && "${OS_GIT_TREE_STATE}" == "dirty" ]]; then
+	os::log::warning "Repository is not clean, performing fast build and reusing _output"
+
+	# build and output from source to destination
+	rm -rf "${rpm_tmp_dir}"
+	mkdir -p "${rpm_tmp_dir}"
+	ln -fns "${OS_ROOT}" "${rpm_tmp_dir}/SOURCES"
+	ln -fns "${OS_ROOT}" "${rpm_tmp_dir}/BUILD"
+	rpmbuild -bb "${OS_RPM_SPECFILE}" \
+		--define "_sourcedir ${rpm_tmp_dir}/SOURCES" \
+		--define "_builddir ${rpm_tmp_dir}/BUILD" \
+		--define "skip_prep 1" \
+		--define "skip_dist ${SKIP_DIST:-1}" \
+		--define "version ${OS_RPM_VERSION}" \
+		--define "release ${OS_RPM_RELEASE}" \
+		--define "commit ${OS_GIT_COMMIT}" \
+		--define "os_git_vars ${OS_RPM_GIT_VARS}" \
+		--define "_topdir ${rpm_tmp_dir}"
+
+	mkdir -p "${OS_OUTPUT_RPMPATH}"
+	mv -f "${rpm_tmp_dir}"/RPMS/*/*.rpm "${OS_OUTPUT_RPMPATH}"
+
+else
+	rm -rf "${rpm_tmp_dir}/SOURCES"
+	mkdir -p "${rpm_tmp_dir}/SOURCES"
+	tar czf "${rpm_tmp_dir}/SOURCES/${OS_RPM_NAME}-${OS_RPM_VERSION}.tar.gz" \
+		--owner=0 --group=0 \
+		--exclude=_output --exclude=.git \
+		--transform "s|^|${OS_RPM_NAME}-${OS_RPM_VERSION}/|rSH" \
+		.
+
+	rpmbuild -b${srpm} "${OS_RPM_SPECFILE}" \
+		--define "skip_dist ${SKIP_DIST:-1}" \
+		--define "version ${OS_RPM_VERSION}" \
+		--define "release ${OS_RPM_RELEASE}" \
+		--define "commit ${OS_GIT_COMMIT}" \
+		--define "os_git_vars ${OS_RPM_GIT_VARS}" \
+		--define "_topdir ${rpm_tmp_dir}"
+
+	output_directory="$( find "${rpm_tmp_dir}" -type d -path "*/BUILD/${OS_RPM_NAME}-${OS_RPM_VERSION}/_output/local" )"
+	if [[ -z "${output_directory}" ]]; then
+		os::log::fatal 'No _output artifact directory found in rpmbuild artifacts!'
+	fi
+
+	# migrate the rpm artifacts to the output directory, must be clean or move will fail
+	make clean
+	mkdir -p "${OS_OUTPUT}"
+
+	# mv exits prematurely with status 1 in the following scenario: running as root,
+	# attempting to move a [directory tree containing a] symlink to a destination on
+	# an NFS volume exported with root_squash set.  This can occur when running this
+	# script on a Vagrant box.  The error shown is "mv: failed to preserve ownership
+	# for $FILE: Operation not permitted".  As a workaround, if
+	# ${output_directory} and ${OS_OUTPUT} are on different devices, use cp and
+	# rm instead.
+	if [[ $(stat -c %d "${output_directory}") == $(stat -c %d "${OS_OUTPUT}") ]]; then
+		mv "${output_directory}"/* "${OS_OUTPUT}"
+	else
+		cp -R "${output_directory}"/* "${OS_OUTPUT}"
+		rm -rf "${output_directory:?}"/*
+	fi
+
+	mkdir -p "${OS_OUTPUT_RPMPATH}"
+	if [[ -n "${OS_BUILD_SRPM-}" ]]; then
+		mv -f "${rpm_tmp_dir}"/SRPMS/*src.rpm "${OS_OUTPUT_RPMPATH}"
+	fi
+	mv -f "${rpm_tmp_dir}"/RPMS/*/*.rpm "${OS_OUTPUT_RPMPATH}"
+fi
+
+mkdir -p "${OS_OUTPUT_RELEASEPATH}"
+echo "${OS_GIT_COMMIT}" > "${OS_OUTPUT_RELEASEPATH}/.commit"
+
+repo_path="$( os::util::absolute_path "${OS_OUTPUT_RPMPATH}" )"
+createrepo "${repo_path}"
+
+echo "[${OS_RPM_NAME}-local-release]
+baseurl = file://${repo_path}
+gpgcheck = 0
+name = Release from Local Source for ${OS_RPM_NAME}
+enabled = 1
+" > "${repo_path}/local-release.repo"
+
+# DEPRECATED: preserve until jobs migrate to using local-release.repo
+cp "${repo_path}/local-release.repo" "${repo_path}/origin-local-release.repo"
+
+os::log::info "Repository file for \`yum\` or \`dnf\` placed at ${repo_path}/local-release.repo
+Install it with:
+$ mv '${repo_path}/local-release.repo' '/etc/yum.repos.d"
diff --git a/openshift-hack/cmd/k8s-tests/k8s-tests.go b/openshift-hack/cmd/k8s-tests/k8s-tests.go
new file mode 100644
index 00000000000..fedd8b16f01
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/k8s-tests.go
@@ -0,0 +1,98 @@
+package main
+
+import (
+	"encoding/json"
+	"flag"
+	"fmt"
+	"math/rand"
+	"os"
+	"sort"
+	"time"
+
+	"github.com/spf13/cobra"
+	"github.com/spf13/pflag"
+
+	utilflag "k8s.io/component-base/cli/flag"
+	"k8s.io/component-base/logs"
+	"k8s.io/kubernetes/test/e2e/framework"
+
+	// initialize framework extensions
+	_ "k8s.io/kubernetes/test/e2e/framework/debug/init"
+	_ "k8s.io/kubernetes/test/e2e/framework/metrics/init"
+)
+
+func main() {
+	logs.InitLogs()
+	defer logs.FlushLogs()
+
+	rand.Seed(time.Now().UTC().UnixNano())
+
+	pflag.CommandLine.SetNormalizeFunc(utilflag.WordSepNormalizeFunc)
+
+	root := &cobra.Command{
+		Long: "OpenShift Tests compatible wrapper",
+	}
+
+	root.AddCommand(
+		newRunTestCommand(),
+		newListTestsCommand(),
+	)
+
+	f := flag.CommandLine.Lookup("v")
+	root.PersistentFlags().AddGoFlag(f)
+	pflag.CommandLine = pflag.NewFlagSet("empty", pflag.ExitOnError)
+	flag.CommandLine = flag.NewFlagSet("empty", flag.ExitOnError)
+	framework.RegisterCommonFlags(flag.CommandLine)
+	framework.RegisterClusterFlags(flag.CommandLine)
+
+	if err := func() error {
+		return root.Execute()
+	}(); err != nil {
+		if ex, ok := err.(ExitError); ok {
+			fmt.Fprintf(os.Stderr, "Ginkgo exit error %d: %v\n", ex.Code, err)
+			os.Exit(ex.Code)
+		}
+		fmt.Fprintf(os.Stderr, "error: %v\n", err)
+		os.Exit(1)
+	}
+}
+
+func newRunTestCommand() *cobra.Command {
+	testOpt := NewTestOptions(os.Stdout, os.Stderr)
+
+	cmd := &cobra.Command{
+		Use:          "run-test NAME",
+		Short:        "Run a single test by name",
+		Long:         "Execute a single test.",
+		SilenceUsage: true,
+		RunE: func(cmd *cobra.Command, args []string) error {
+			if err := initializeTestFramework(os.Getenv("TEST_PROVIDER")); err != nil {
+				return err
+			}
+
+			return testOpt.Run(args)
+		},
+	}
+	return cmd
+}
+
+func newListTestsCommand() *cobra.Command {
+	cmd := &cobra.Command{
+		Use:          "list",
+		Short:        "List available tests",
+		Long:         "List the available tests in this binary.",
+		SilenceUsage: true,
+		RunE: func(cmd *cobra.Command, args []string) error {
+			tests := testsForSuite()
+			sort.Slice(tests, func(i, j int) bool { return tests[i].Name < tests[j].Name })
+			data, err := json.Marshal(tests)
+			if err != nil {
+				return err
+			}
+			fmt.Fprintf(os.Stdout, "%s\n", data)
+			return nil
+		},
+	}
+
+	return cmd
+}
diff --git a/openshift-hack/cmd/k8s-tests/provider.go b/openshift-hack/cmd/k8s-tests/provider.go
new file mode 100644
index 00000000000..cdc948a45c6
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/provider.go
@@ -0,0 +1,147 @@
+package main
+
+import (
+	"context"
+	"encoding/json"
+	"fmt"
+	"os"
+	"path/filepath"
+	"strings"
+
+	"github.com/onsi/ginkgo/v2"
+	"github.com/onsi/gomega"
+
+	corev1 "k8s.io/api/core/v1"
+	kclientset "k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/tools/clientcmd"
+	"k8s.io/kubernetes/openshift-hack/e2e"
+	conformancetestdata "k8s.io/kubernetes/test/conformance/testdata"
+	"k8s.io/kubernetes/test/e2e/framework"
+	"k8s.io/kubernetes/test/e2e/framework/testfiles"
+	"k8s.io/kubernetes/test/e2e/storage/external"
+	e2etestingmanifests "k8s.io/kubernetes/test/e2e/testing-manifests"
+	testfixtures "k8s.io/kubernetes/test/fixtures"
+
+	// this appears to inexplicably auto-register global flags.
+	_ "k8s.io/kubernetes/test/e2e/storage/drivers"
+
+	// these are loading important global flags that we need to get and set
+	_ "k8s.io/kubernetes/test/e2e"
+	_ "k8s.io/kubernetes/test/e2e/lifecycle"
+)
+
+// copied directly from github.com/openshift/origin/cmd/openshift-tests/provider.go
+// and github.com/openshift/origin/test/extended/util/test.go
+func initializeTestFramework(provider string) error {
+	providerInfo := &ClusterConfiguration{}
+	if err := json.Unmarshal([]byte(provider), &providerInfo); err != nil {
+		return fmt.Errorf("provider must be a JSON object with the 'type' key at a minimum: %v", err)
+	}
+	if len(providerInfo.ProviderName) == 0 {
+		return fmt.Errorf("provider must be a JSON object with the 'type' key")
+	}
+	config := &ClusterConfiguration{}
+	if err := json.Unmarshal([]byte(provider), config); err != nil {
+		return fmt.Errorf("provider must decode into the ClusterConfig object: %v", err)
+	}
+
+	// update testContext with loaded config
+	testContext := &framework.TestContext
+	testContext.Provider = config.ProviderName
+	testContext.CloudConfig = framework.CloudConfig{
+		ProjectID:   config.ProjectID,
+		Region:      config.Region,
+		Zone:        config.Zone,
+		Zones:       config.Zones,
+		NumNodes:    config.NumNodes,
+		MultiMaster: config.MultiMaster,
+		MultiZone:   config.MultiZone,
+		ConfigFile:  config.ConfigFile,
+	}
+	testContext.AllowedNotReadyNodes = -1
+	testContext.MinStartupPods = -1
+	testContext.MaxNodesToGather = 0
+	testContext.KubeConfig = os.Getenv("KUBECONFIG")
+
+	// allow the CSI tests to access test data, but only briefly
+	// TODO: ideally CSI would not use any of these test methods
+	// var err error
+	// exutil.WithCleanup(func() { err = initCSITests(dryRun) })
+	// TODO: for now I'm only initializing CSI directly, but we probably need that
+	// WithCleanup here as well
+	if err := initCSITests(); err != nil {
+		return err
+	}
+
+	if ad := os.Getenv("ARTIFACT_DIR"); len(strings.TrimSpace(ad)) == 0 {
+		os.Setenv("ARTIFACT_DIR", filepath.Join(os.TempDir(), "artifacts"))
+	}
+
+	testContext.DeleteNamespace = os.Getenv("DELETE_NAMESPACE") != "false"
+	testContext.VerifyServiceAccount = true
+	testfiles.AddFileSource(e2etestingmanifests.GetE2ETestingManifestsFS())
+	testfiles.AddFileSource(testfixtures.GetTestFixturesFS())
+	testfiles.AddFileSource(conformancetestdata.GetConformanceTestdataFS())
+	testContext.KubectlPath = "kubectl"
+	// context.KubeConfig = KubeConfigPath()
+	testContext.KubeConfig = os.Getenv("KUBECONFIG")
+
+	// "debian" is used when not set. At least GlusterFS tests need "custom".
+	// (There is no option for "rhel" or "centos".)
+	testContext.NodeOSDistro = "custom"
+	testContext.MasterOSDistro = "custom"
+
+	// load and set the host variable for kubectl
+	clientConfig := clientcmd.NewNonInteractiveDeferredLoadingClientConfig(&clientcmd.ClientConfigLoadingRules{ExplicitPath: testContext.KubeConfig}, &clientcmd.ConfigOverrides{})
+	cfg, err := clientConfig.ClientConfig()
+	if err != nil {
+		return err
+	}
+	testContext.Host = cfg.Host
+
+	// Ensure that Kube tests run privileged (like they do upstream)
+	testContext.CreateTestingNS = func(ctx context.Context, baseName string, c kclientset.Interface, labels map[string]string) (*corev1.Namespace, error) {
+		return e2e.CreateTestingNS(ctx, baseName, c, labels, true)
+	}
+
+	gomega.RegisterFailHandler(ginkgo.Fail)
+
+	framework.AfterReadingAllFlags(testContext)
+	testContext.DumpLogsOnFailure = true
+
+	// these constants are taken from kube e2e and used by tests
+	testContext.IPFamily = "ipv4"
+	if config.HasIPv6 && !config.HasIPv4 {
+		testContext.IPFamily = "ipv6"
+	}
+
+	testContext.ReportDir = os.Getenv("TEST_JUNIT_DIR")
+
+	return nil
+}
+
+const (
+	manifestEnvVar = "TEST_CSI_DRIVER_FILES"
+)
+
+// copied directly from github.com/openshift/origin/cmd/openshift-tests/csi.go
+// Initialize openshift/csi suite, i.e. define CSI tests from TEST_CSI_DRIVER_FILES.
+func initCSITests() error {
+	manifestList := os.Getenv(manifestEnvVar)
+	if manifestList != "" {
+		manifests := strings.Split(manifestList, ",")
+		for _, manifest := range manifests {
+			if err := external.AddDriverDefinition(manifest); err != nil {
+				return fmt.Errorf("failed to load manifest from %q: %s", manifest, err)
+			}
+			// Register the base dir of the manifest file as a file source.
+			// With this we can reference the CSI driver's storageClass
+			// in the manifest file (FromFile field).
+			testfiles.AddFileSource(testfiles.RootFileSource{
+				Root: filepath.Dir(manifest),
+			})
+		}
+	}
+
+	return nil
+}
diff --git a/openshift-hack/cmd/k8s-tests/runtest.go b/openshift-hack/cmd/k8s-tests/runtest.go
new file mode 100644
index 00000000000..0abff33438f
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/runtest.go
@@ -0,0 +1,143 @@
+package main
+
+import (
+	"fmt"
+	"io"
+	"os"
+	"regexp"
+	"strings"
+	"time"
+
+	"github.com/onsi/ginkgo/v2"
+	"github.com/onsi/ginkgo/v2/types"
+
+	"k8s.io/kubernetes/openshift-hack/e2e/annotate/generated"
+
+	// ensure all the ginkgo tests are loaded
+	_ "k8s.io/kubernetes/openshift-hack/e2e"
+)
+
+// TestOptions handles running a single test.
+type TestOptions struct {
+	Out    io.Writer
+	ErrOut io.Writer
+}
+
+var _ ginkgo.GinkgoTestingT = &TestOptions{}
+
+func NewTestOptions(out io.Writer, errOut io.Writer) *TestOptions {
+	return &TestOptions{
+		Out:    out,
+		ErrOut: errOut,
+	}
+}
+
+func (opt *TestOptions) Run(args []string) error {
+	if len(args) != 1 {
+		return fmt.Errorf("only a single test name may be passed")
+	}
+
+	// Ignore the upstream suite behavior within test execution
+	ginkgo.GetSuite().ClearBeforeAndAfterSuiteNodes()
+	tests := testsForSuite()
+	var test *TestCase
+	for _, t := range tests {
+		if t.Name == args[0] {
+			test = t
+			break
+		}
+	}
+	if test == nil {
+		return fmt.Errorf("no test exists with that name: %s", args[0])
+	}
+
+	suiteConfig, reporterConfig := ginkgo.GinkgoConfiguration()
+	suiteConfig.FocusStrings = []string{fmt.Sprintf("^ %s$", regexp.QuoteMeta(test.Name))}
+
+	// These settings are matched to upstream's ginkgo configuration. See:
+	// https://github.com/kubernetes/kubernetes/blob/v1.25.0/test/e2e/framework/test_context.go#L354-L355
+	// Randomize specs as well as suites
+	suiteConfig.RandomizeAllSpecs = true
+	// https://github.com/kubernetes/kubernetes/blob/v1.25.0/hack/ginkgo-e2e.sh#L172-L173
+	suiteConfig.Timeout = 24 * time.Hour
+	reporterConfig.NoColor = true
+	reporterConfig.Verbose = true
+
+	ginkgo.SetReporterConfig(reporterConfig)
+
+	cwd, err := os.Getwd()
+	if err != nil {
+		return err
+	}
+	ginkgo.GetSuite().RunSpec(test.spec, ginkgo.Labels{}, "Kubernetes e2e suite", cwd, ginkgo.GetFailer(), ginkgo.GetWriter(), suiteConfig, reporterConfig)
+
+	var summary types.SpecReport
+	for _, report := range ginkgo.GetSuite().GetReport().SpecReports {
+		if report.NumAttempts > 0 {
+			summary = report
+		}
+	}
+
+	switch {
+	case summary.State == types.SpecStatePassed:
+		// do nothing
+	case summary.State == types.SpecStateSkipped:
+		if len(summary.Failure.Message) > 0 {
+			fmt.Fprintf(opt.ErrOut, "skip [%s:%d]: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.Message)
+		}
+		if len(summary.Failure.ForwardedPanic) > 0 {
+			fmt.Fprintf(opt.ErrOut, "skip [%s:%d]: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.ForwardedPanic)
+		}
+		return ExitError{Code: 3}
+	case summary.State == types.SpecStateFailed, summary.State == types.SpecStatePanicked, summary.State == types.SpecStateInterrupted:
+		if len(summary.Failure.ForwardedPanic) > 0 {
+			if len(summary.Failure.Location.FullStackTrace) > 0 {
+				fmt.Fprintf(opt.ErrOut, "\n%s\n", summary.Failure.Location.FullStackTrace)
+			}
+			fmt.Fprintf(opt.ErrOut, "fail [%s:%d]: Test Panicked: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.ForwardedPanic)
+			return ExitError{Code: 1}
+		}
+		fmt.Fprintf(opt.ErrOut, "fail [%s:%d]: %s\n", lastFilenameSegment(summary.Failure.Location.FileName), summary.Failure.Location.LineNumber, summary.Failure.Message)
+		return ExitError{Code: 1}
+	default:
+		return fmt.Errorf("unrecognized test case outcome: %#v", summary)
+	}
+	return nil
+}
+
+func (opt *TestOptions) Fail() {
+	// this function allows us to pass TestOptions as the first argument,
+	// it's empty becase we have failure check mechanism implemented above.
+}
+
+func lastFilenameSegment(filename string) string {
+	if parts := strings.Split(filename, "/vendor/"); len(parts) > 1 {
+		return parts[len(parts)-1]
+	}
+	if parts := strings.Split(filename, "/src/"); len(parts) > 1 {
+		return parts[len(parts)-1]
+	}
+	return filename
+}
+
+func testsForSuite() []*TestCase {
+	var tests []*TestCase
+
+	// Don't build the tree multiple times, it results in multiple initing of tests
+	if !ginkgo.GetSuite().InPhaseBuildTree() {
+		ginkgo.GetSuite().BuildTree()
+	}
+
+	ginkgo.GetSuite().WalkTests(func(name string, spec types.TestSpec) {
+		testCase := &TestCase{
+			Name:      spec.Text(),
+			locations: spec.CodeLocations(),
+			spec:      spec,
+		}
+		if labels, ok := generated.Annotations[name]; ok {
+			testCase.Labels = labels
+		}
+		tests = append(tests, testCase)
+	})
+	return tests
+}
diff --git a/openshift-hack/cmd/k8s-tests/types.go b/openshift-hack/cmd/k8s-tests/types.go
new file mode 100644
index 00000000000..29a0b5b5efa
--- /dev/null
+++ b/openshift-hack/cmd/k8s-tests/types.go
@@ -0,0 +1,69 @@
+package main
+
+import (
+	"fmt"
+
+	"github.com/onsi/ginkgo/v2/types"
+)
+
+// copied directly from github.com/openshift/origin/test/extended/util/cluster/cluster.go
+type ClusterConfiguration struct {
+	ProviderName string `json:"type"`
+
+	// These fields (and the "type" tag for ProviderName) chosen to match
+	// upstream's e2e.CloudConfig.
+	ProjectID   string
+	Region      string
+	Zone        string
+	NumNodes    int
+	MultiMaster bool
+	MultiZone   bool
+	Zones       []string
+	ConfigFile  string
+
+	// Disconnected is set for test jobs without external internet connectivity
+	Disconnected bool
+
+	// SingleReplicaTopology is set for disabling disruptive tests or tests
+	// that require high availability
+	SingleReplicaTopology bool
+
+	// NetworkPlugin is the "official" plugin name
+	NetworkPlugin string
+	// NetworkPluginMode is an optional sub-identifier for the NetworkPlugin.
+	// (Currently it is only used for OpenShiftSDN.)
+	NetworkPluginMode string `json:",omitempty"`
+
+	// HasIPv4 and HasIPv6 determine whether IPv4-specific, IPv6-specific,
+	// and dual-stack-specific tests are run
+	HasIPv4 bool
+	HasIPv6 bool
+
+	// HasSCTP determines whether SCTP connectivity tests can be run in the cluster
+	HasSCTP bool
+
+	// IsProxied determines whether we are accessing the cluster through an HTTP proxy
+	IsProxied bool
+
+	// IsIBMROKS determines whether the cluster is Managed IBM Cloud (ROKS)
+	IsIBMROKS bool
+
+	// IsNoOptionalCapabilities indicates the cluster has no optional capabilities enabled
+	HasNoOptionalCapabilities bool
+}
+
+// copied directly from github.com/openshift/origin/pkg/test/ginkgo/test.go
+type TestCase struct {
+	Name      string
+	Labels    string
+	spec      types.TestSpec
+	locations []types.CodeLocation
+}
+
+type ExitError struct {
+	Code int
+}
+
+func (e ExitError) Error() string {
+	return fmt.Sprintf("exit with code %d", e.Code)
+}
diff --git a/openshift-hack/conformance-k8s.sh b/openshift-hack/conformance-k8s.sh
new file mode 100755
index 00000000000..245a29b0547
--- /dev/null
+++ b/openshift-hack/conformance-k8s.sh
@@ -0,0 +1,96 @@
+#!/bin/bash
+#
+# Runs the Kubernetes conformance suite against an OpenShift cluster
+#
+# Test prerequisites:
+#
+# * all nodes that users can run workloads under marked as schedulable
+#
+source "$(dirname "${BASH_SOURCE[0]}")/lib/init.sh"
+
+# Check inputs
+if [[ -z "${KUBECONFIG-}" ]]; then
+  os::log::fatal "KUBECONFIG must be set to a root account"
+fi
+test_report_dir="${ARTIFACT_DIR}"
+mkdir -p "${test_report_dir}"
+
+cat <<END > "${test_report_dir}/README.md"
+This conformance report is generated by the OpenShift CI infrastructure. The canonical source location for this test script is located at https://github.com/openshift/kubernetes/blob/master/openshift-hack/conformance-k8s.sh
+
+This file was generated by:
+
+  Commit $( git rev-parse HEAD || "<commit>" )
+  Tag    $( git describe || "<tag>" )
+
+To recreate these results
+
+1. Install an [OpenShift cluster](https://docs.openshift.com/container-platform/)
+2. Retrieve a \`.kubeconfig\` file with administrator credentials on that cluster and set the environment variable KUBECONFIG
+
+    export KUBECONFIG=PATH_TO_KUBECONFIG
+
+3. Clone the OpenShift source repository and change to that directory:
+
+    git clone https://github.com/openshift/kubernetes.git
+    cd kubernetes
+
+4. Place the \`oc\` binary for that cluster in your PATH
+5. Run the conformance test:
+
+    openshift-hack/conformance-k8s.sh
+
+Nightly conformance tests are run against release branches and reported https://openshift-gce-devel.appspot.com/builds/origin-ci-test/logs/periodic-ci-origin-conformance-k8s/
+END
+
+version="$(sed -rn 's/.*io.openshift.build.versions="kubernetes=(1.[0-9]+.[0-9]+(-rc.[0-9])?)"/v\1/p' openshift-hack/images/hyperkube/Dockerfile.rhel)"
+os::log::info "Running Kubernetes conformance suite for ${version}"
+
+# Execute OpenShift prerequisites
+# Disable container security
+oc adm policy add-scc-to-group privileged system:authenticated system:serviceaccounts
+oc adm policy add-scc-to-group anyuid system:authenticated system:serviceaccounts
+unschedulable="$( ( oc get nodes -o name -l 'node-role.kubernetes.io/master'; ) | wc -l )"
+# TODO: undo these operations
+
+# Execute Kubernetes prerequisites
+make WHAT=cmd/kubectl
+make WHAT=test/e2e/e2e.test
+make WHAT=vendor/github.com/onsi/ginkgo/v2/ginkgo
+PATH="${OS_ROOT}/_output/local/bin/$( os::build::host_platform ):${PATH}"
+export PATH
+
+kubectl version  > "${test_report_dir}/version.txt"
+echo "-----"    >> "${test_report_dir}/version.txt"
+oc version      >> "${test_report_dir}/version.txt"
+
+# Run the test, serial tests first, then parallel
+
+rc=0
+
+e2e_test="$( which e2e.test )"
+
+# shellcheck disable=SC2086
+ginkgo \
+  -nodes 1 -noColor '-focus=(\[Conformance\].*\[Serial\]|\[Serial\].*\[Conformance\])' \
+  ${e2e_test} -- \
+  -report-dir "${test_report_dir}" \
+  -allowed-not-ready-nodes ${unschedulable} \
+  2>&1 | tee -a "${test_report_dir}/e2e.log" || rc=1
+
+rename -v junit_ junit_serial_ "${test_report_dir}"/junit*.xml
+
+# shellcheck disable=SC2086
+ginkgo \
+  --timeout="24h" \
+  --output-interceptor-mode=none \
+  -nodes 4 -no-color '-skip=\[Serial\]' '-focus=\[Conformance\]' \
+  ${e2e_test} -- \
+  -report-dir "${test_report_dir}" \
+  -allowed-not-ready-nodes ${unschedulable} \
+  2>&1 | tee -a "${test_report_dir}/e2e.log" || rc=1
+
+echo
+echo "Run complete, results in ${test_report_dir}"
+
+exit $rc
diff --git a/openshift-hack/create-or-update-rebase-branch.sh b/openshift-hack/create-or-update-rebase-branch.sh
new file mode 100755
index 00000000000..c948eb87485
--- /dev/null
+++ b/openshift-hack/create-or-update-rebase-branch.sh
@@ -0,0 +1,67 @@
+#!/usr/bin/env bash
+
+set -o nounset
+set -o errexit
+set -o pipefail
+
+# This script is intended to simplify the maintaining a rebase branch for
+# openshift/kubernetes.
+#
+# - If the branch named by REBASE_BRANCH does not exist, it will be created by
+# branching from UPSTREAM_TAG and merging in TARGET_BRANCH with strategy
+# 'ours'.
+#
+# - If the branch named by REBASE_BRANCH exists, it will be renamed to
+# <branch-name>-<timestamp>, a new branch will be created as per above, and
+# carries from the renamed branch will be cherry-picked.
+
+UPSTREAM_TAG="${UPSTREAM_TAG:-}"
+if [[ -z "${UPSTREAM_TAG}" ]]; then
+  echo >&2 "UPSTREAM_TAG is required"
+  exit 1
+fi
+
+REBASE_BRANCH="${REBASE_BRANCH:-}"
+if [[ -z "${REBASE_BRANCH}" ]]; then
+  echo >&2 "REBASE_BRANCH is required"
+  exit 1
+fi
+
+TARGET_BRANCH="${TARGET_BRANCH:-master}"
+if [[ -z "${TARGET_BRANCH}" ]]; then
+  echo >&2 "TARGET_BRANCH is required"
+  exit 1
+fi
+
+echo "Ensuring target branch '${TARGET_BRANCH} is updated"
+git co "${TARGET_BRANCH}"
+git pull
+
+echo "Checking if '${REBASE_BRANCH}' exists"
+REBASE_IN_PROGRESS=
+if git show-ref --verify --quiet "refs/heads/${REBASE_BRANCH}"; then
+  REBASE_IN_PROGRESS=y
+fi
+
+# If a rebase is in progress, rename the existing branch
+if [[ "${REBASE_IN_PROGRESS}" ]]; then
+  TIMESTAMP="$(date +"%Y-%m-%d_%H-%M-%S")"
+  PREVIOUS_REBASE_BRANCH="${REBASE_BRANCH}.${TIMESTAMP}"
+  echo "Renaming rebase branch '${REBASE_BRANCH}' to '${PREVIOUS_REBASE_BRANCH}'"
+  git br -m "${REBASE_BRANCH}" "${PREVIOUS_REBASE_BRANCH}"
+fi
+
+echo "Branching upstream tag '${UPSTREAM_TAG}' to rebase branch '${REBASE_BRANCH}'"
+git co -b "${REBASE_BRANCH}" "${UPSTREAM_TAG}"
+
+echo "Merging target branch '${TARGET_BRANCH}' to rebase branch '${REBASE_BRANCH}'"
+git merge -s ours --no-edit "${TARGET_BRANCH}"
+
+if [[ "${REBASE_IN_PROGRESS}" ]]; then
+  echo "Cherry-picking carried commits from previous rebase branch '${PREVIOUS_REBASE_BRANCH}'"
+  # The first merge in the previous rebase branch should be the point at which
+  # the target branch was merged with the upstream tag. Any commits since this
+  # merge should be cherry-picked.
+  MERGE_SHA="$(git log --pretty=%H --merges --max-count=1 "${PREVIOUS_REBASE_BRANCH}" )"
+  git cherry-pick "${MERGE_SHA}..${PREVIOUS_REBASE_BRANCH}"
+fi
diff --git a/openshift-hack/e2e/annotate/annotate.go b/openshift-hack/e2e/annotate/annotate.go
new file mode 100644
index 00000000000..096ae2a00aa
--- /dev/null
+++ b/openshift-hack/e2e/annotate/annotate.go
@@ -0,0 +1,290 @@
+package annotate
+
+import (
+	"fmt"
+	"io/ioutil"
+	"os"
+	"os/exec"
+	"regexp"
+	"sort"
+	"strings"
+
+	"github.com/onsi/ginkgo/v2"
+	"github.com/onsi/ginkgo/v2/types"
+)
+
+var reHasSig = regexp.MustCompile(`\[sig-[\w-]+\]`)
+
+// Run generates tests annotations for the targeted package.
+// It accepts testMaps which defines labeling rules and filter
+// function to remove elements based on test name and their labels.
+func Run(testMaps map[string][]string, filter func(name string) bool) {
+	var errors []string
+
+	if len(os.Args) != 2 && len(os.Args) != 3 {
+		fmt.Fprintf(os.Stderr, "error: requires exactly one argument\n")
+		os.Exit(1)
+	}
+	filename := os.Args[len(os.Args)-1]
+
+	generator := newGenerator(testMaps)
+	ginkgo.GetSuite().BuildTree()
+	ginkgo.GetSuite().WalkTests(generator.generateRename)
+	if len(generator.errors) > 0 {
+		errors = append(errors, generator.errors...)
+	}
+
+	renamer := newRenamerFromGenerated(generator.output)
+	// generated file has a map[string]string in the following format:
+	// original k8s name: k8s name with our labels at the end
+	ginkgo.GetSuite().WalkTests(renamer.updateNodeText)
+	if len(renamer.missing) > 0 {
+		var names []string
+		for name := range renamer.missing {
+			names = append(names, name)
+		}
+		sort.Strings(names)
+		fmt.Fprintf(os.Stderr, "failed:\n%s\n", strings.Join(names, "\n"))
+		os.Exit(1)
+	}
+
+	// All tests must be associated with a sig (either upstream), or downstream
+	// If you get this error, you should add the [sig-X] tag to your test (if its
+	// in origin) or if it is upstream add a new rule to rules.go that assigns
+	// the test in question to the right sig.
+	//
+	// Upstream sigs map to teams (if you have representation on that sig, you
+	//   own those tests in origin)
+	// Downstream sigs: sig-imageregistry, sig-builds, sig-devex
+	for from, to := range generator.output {
+		if !reHasSig.MatchString(from) && !reHasSig.MatchString(to) {
+			errors = append(errors, fmt.Sprintf("all tests must define a [sig-XXXX] tag or have a rule %q", from))
+		}
+	}
+	if len(errors) > 0 {
+		sort.Strings(errors)
+		for _, s := range errors {
+			fmt.Fprintf(os.Stderr, "failed: %s\n", s)
+		}
+		os.Exit(1)
+	}
+
+	var pairs []string
+	for testName, labels := range generator.output {
+		if filter(fmt.Sprintf("%s%s", testName, labels)) {
+			continue
+		}
+		pairs = append(pairs, fmt.Sprintf("%q:\n%q,", testName, labels))
+	}
+	sort.Strings(pairs)
+	contents := fmt.Sprintf(`
+package generated
+
+import (
+	"fmt"
+	"github.com/onsi/ginkgo/v2"
+	"github.com/onsi/ginkgo/v2/types"
+)
+
+var Annotations = map[string]string{
+%s
+}
+
+func init() {
+	ginkgo.GetSuite().SetAnnotateFn(func(name string, node types.TestSpec) {
+		if newLabels, ok := Annotations[name]; ok {
+			node.AppendText(newLabels)
+		} else {
+			panic(fmt.Sprintf("unable to find test %%s", name))
+		}
+	})
+}
+`, strings.Join(pairs, "\n\n"))
+	if err := ioutil.WriteFile(filename, []byte(contents), 0644); err != nil {
+		fmt.Fprintf(os.Stderr, "error: %v", err)
+		os.Exit(1)
+	}
+	if _, err := exec.Command("gofmt", "-s", "-w", filename).Output(); err != nil {
+		fmt.Fprintf(os.Stderr, "error: %v", err)
+		os.Exit(1)
+	}
+}
+
+func newGenerator(testMaps map[string][]string) *ginkgoTestRenamer {
+	var allLabels []string
+	matches := make(map[string]*regexp.Regexp)
+	stringMatches := make(map[string][]string)
+
+	for label, items := range testMaps {
+		sort.Strings(items)
+		allLabels = append(allLabels, label)
+		var remain []string
+		for _, item := range items {
+			re := regexp.MustCompile(item)
+			if p, ok := re.LiteralPrefix(); ok {
+				stringMatches[label] = append(stringMatches[label], p)
+			} else {
+				remain = append(remain, item)
+			}
+		}
+		if len(remain) > 0 {
+			matches[label] = regexp.MustCompile(strings.Join(remain, `|`))
+		}
+	}
+	sort.Strings(allLabels)
+
+	excludedTestsFilter := regexp.MustCompile(strings.Join(ExcludedTests, `|`))
+
+	return &ginkgoTestRenamer{
+		allLabels:           allLabels,
+		stringMatches:       stringMatches,
+		matches:             matches,
+		excludedTestsFilter: excludedTestsFilter,
+		output:              make(map[string]string),
+	}
+}
+
+func newRenamerFromGenerated(names map[string]string) *ginkgoTestRenamer {
+	return &ginkgoTestRenamer{
+		output:  names,
+		missing: make(map[string]struct{}),
+	}
+}
+
+type ginkgoTestRenamer struct {
+	// keys defined in TestMaps in openshift-hack/e2e/annotate/rules.go
+	allLabels []string
+	// exact substrings to match to apply a particular label
+	stringMatches map[string][]string
+	// regular expressions to match to apply a particular label
+	matches map[string]*regexp.Regexp
+	// regular expression excluding permanently a set of tests
+	// see ExcludedTests in openshift-hack/e2e/annotate/rules.go
+	excludedTestsFilter *regexp.Regexp
+
+	// output from the generateRename and also input for updateNodeText
+	output map[string]string
+	// map of unmatched test names
+	missing map[string]struct{}
+	// a list of errors to display
+	errors []string
+}
+
+func (r *ginkgoTestRenamer) updateNodeText(name string, node types.TestSpec) {
+	if newLables, ok := r.output[name]; ok {
+		node.AppendText(newLables)
+	} else {
+		r.missing[name] = struct{}{}
+	}
+}
+
+func (r *ginkgoTestRenamer) generateRename(name string, node types.TestSpec) {
+	newLabels := ""
+	newName := name
+	for {
+		count := 0
+		for _, label := range r.allLabels {
+			// never apply a sig label twice
+			if strings.HasPrefix(label, "[sig-") && strings.Contains(newName, "[sig-") {
+				continue
+			}
+			if strings.Contains(newName, label) {
+				continue
+			}
+
+			var hasLabel bool
+			for _, segment := range r.stringMatches[label] {
+				hasLabel = strings.Contains(newName, segment)
+				if hasLabel {
+					break
+				}
+			}
+			if !hasLabel {
+				if re := r.matches[label]; re != nil {
+					hasLabel = r.matches[label].MatchString(newName)
+				}
+			}
+
+			if hasLabel {
+				count++
+				newLabels += " " + label
+				newName += " " + label
+			}
+		}
+		if count == 0 {
+			break
+		}
+	}
+
+	// Append suite name to test, if it doesn't already have one
+	if !r.excludedTestsFilter.MatchString(newName) && !strings.Contains(newName, "[Suite:") {
+		isSerial := strings.Contains(newName, "[Serial]")
+		isConformance := strings.Contains(newName, "[Conformance]")
+		switch {
+		case isSerial && isConformance:
+			newLabels += " [Suite:openshift/conformance/serial/minimal]"
+		case isSerial:
+			newLabels += " [Suite:openshift/conformance/serial]"
+		case isConformance:
+			newLabels += " [Suite:openshift/conformance/parallel/minimal]"
+		default:
+			newLabels += " [Suite:openshift/conformance/parallel]"
+		}
+	}
+	codeLocations := node.CodeLocations()
+	if isGoModulePath(codeLocations[len(codeLocations)-1].FileName, "k8s.io/kubernetes", "test/e2e") {
+		newLabels += " [Suite:k8s]"
+	}
+
+	if err := checkBalancedBrackets(newName); err != nil {
+		r.errors = append(r.errors, err.Error())
+	}
+	r.output[name] = newLabels
+}
+
+// isGoModulePath returns true if the packagePath reported by reflection is within a
+// module and given module path. When go mod is in use, module and modulePath are not
+// contiguous as they were in older golang versions with vendoring, so naive contains
+// tests fail.
+//
+// historically: ".../vendor/k8s.io/kubernetes/test/e2e"
+// go.mod:       "k8s.io/kubernetes@0.18.4/test/e2e"
+func isGoModulePath(packagePath, module, modulePath string) bool {
+	return regexp.MustCompile(fmt.Sprintf(`\b%s(@[^/]*|)/%s\b`, regexp.QuoteMeta(module), regexp.QuoteMeta(modulePath))).MatchString(packagePath)
+}
+
+// checkBalancedBrackets ensures that square brackets are balanced in generated test
+// names. If they are not, it returns an error with the name of the test and a guess
+// where the unmatched bracket(s) are.
+func checkBalancedBrackets(testName string) error {
+	stack := make([]int, 0, len(testName))
+	for idx, c := range testName {
+		if c == '[' {
+			stack = append(stack, idx)
+		} else if c == ']' {
+			// case when we start off with a ]
+			if len(stack) == 0 {
+				stack = append(stack, idx)
+			} else {
+				stack = stack[:len(stack)-1]
+			}
+		}
+	}
+
+	if len(stack) > 0 {
+		msg := testName + "\n"
+	outerLoop:
+		for i := 0; i < len(testName); i++ {
+			for _, loc := range stack {
+				if i == loc {
+					msg += "^"
+					continue outerLoop
+				}
+			}
+			msg += " "
+		}
+		return fmt.Errorf("unbalanced brackets in test name:\n%s\n", msg)
+	}
+
+	return nil
+}
diff --git a/openshift-hack/e2e/annotate/annotate_test.go b/openshift-hack/e2e/annotate/annotate_test.go
new file mode 100644
index 00000000000..614c902e29e
--- /dev/null
+++ b/openshift-hack/e2e/annotate/annotate_test.go
@@ -0,0 +1,55 @@
+package annotate
+
+import (
+	"fmt"
+	"os"
+	"testing"
+)
+
+func Test_checkBalancedBrackets(t *testing.T) {
+	tests := []struct {
+		testCase string
+		testName string
+		wantErr  bool
+	}{
+		{
+			testCase: "balanced brackets succeeds",
+			testName: "[sig-storage] Test that storage [apigroup:storage.openshift.io] actually works [Driver:azure][Serial][Late]",
+			wantErr:  false,
+		},
+		{
+			testCase: "unbalanced brackets errors",
+			testName: "[sig-storage] Test that storage [apigroup:storage.openshift.io actually works [Driver:azure][Serial][Late]",
+			wantErr:  true,
+		},
+		{
+			testCase: "start with close bracket errors",
+			testName: "[sig-storage] test with a random bracket ]",
+			wantErr:  true,
+		},
+		{
+			testCase: "multiple unbalanced brackets errors",
+			testName: "[sig-storage Test that storage [apigroup:storage.openshift.io actually works [Driver:azure]",
+			wantErr:  true,
+		},
+		{
+			testCase: "balanced deeply nested brackets succeeds",
+			testName: "[[[[[[some weird test with deeply nested brackets]]]]]]",
+			wantErr:  false,
+		},
+		{
+			testCase: "unbalanced deeply nested brackets errors",
+			testName: "[[[[[[some weird test with deeply nested brackets]]]]]",
+			wantErr:  true,
+		},
+	}
+	for _, tt := range tests {
+		t.Run(tt.testCase, func(t *testing.T) {
+			if err := checkBalancedBrackets(tt.testName); (err != nil) != tt.wantErr {
+				t.Errorf("checkBalancedBrackets() error = %v, wantErr %v", err, tt.wantErr)
+			} else if err != nil {
+				fmt.Fprintf(os.Stderr, "checkBalancedBrackets() success, found expected err = \n%s\n", err.Error())
+			}
+		})
+	}
+}
diff --git a/openshift-hack/e2e/annotate/cmd/main.go b/openshift-hack/e2e/annotate/cmd/main.go
new file mode 100644
index 00000000000..c1666ce9e04
--- /dev/null
+++ b/openshift-hack/e2e/annotate/cmd/main.go
@@ -0,0 +1,9 @@
+package main
+
+import (
+	"k8s.io/kubernetes/openshift-hack/e2e/annotate"
+)
+
+func main() {
+	annotate.Run(annotate.TestMaps, func(name string) bool { return false })
+}
diff --git a/openshift-hack/e2e/annotate/generated/zz_generated.annotations.go b/openshift-hack/e2e/annotate/generated/zz_generated.annotations.go
new file mode 100644
index 00000000000..951c880085e
--- /dev/null
+++ b/openshift-hack/e2e/annotate/generated/zz_generated.annotations.go
@@ -0,0 +1 @@
+package generated
diff --git a/openshift-hack/e2e/annotate/rules.go b/openshift-hack/e2e/annotate/rules.go
new file mode 100644
index 00000000000..bc1607135fe
--- /dev/null
+++ b/openshift-hack/e2e/annotate/rules.go
@@ -0,0 +1,383 @@
+package annotate
+
+import (
+	// ensure all the ginkgo tests are loaded
+	_ "k8s.io/kubernetes/openshift-hack/e2e"
+)
+
+var (
+	TestMaps = map[string][]string{
+		// alpha features that are not gated
+		"[Disabled:Alpha]": {
+			`\[Feature:StorageVersionAPI\]`,
+			`\[Feature:StatefulSetAutoDeletePVC\]`,
+			`\[Feature:ProxyTerminatingEndpoints\]`,
+			`\[Feature:UserNamespacesSupport\]`,
+			`\[Feature:ReadWriteOncePod\]`,
+			`\[Feature:SELinuxMountReadWriteOncePod\]`,
+			`\[Feature:PodSchedulingReadiness\]`,
+			`\[Feature:InPlacePodVerticalScaling\]`,
+			`\[Feature:RecoverVolumeExpansionFailure\]`,
+			`\[Feature:SELinux\]`,
+			`\[Feature:SidecarContainers\]`,
+			`\[Feature: PersistentVolumeLastPhaseTransitionTime\]`,
+			`\[Feature:WatchList\]`,
+		},
+		// tests for features that are not implemented in openshift
+		"[Disabled:Unimplemented]": {
+			`Monitoring`,               // Not installed, should be
+			`Cluster level logging`,    // Not installed yet
+			`Kibana`,                   // Not installed
+			`Ubernetes`,                // Can't set zone labels today
+			`kube-ui`,                  // Not installed by default
+			`Kubernetes Dashboard`,     // Not installed by default (also probably slow image pull)
+			`should proxy to cadvisor`, // we don't expose cAdvisor port directly for security reasons
+		},
+		// tests that rely on special configuration that we do not yet support
+		"[Disabled:SpecialConfig]": {
+			// GPU node needs to be available
+			`\[Feature:GPUDevicePlugin\]`,
+			`\[sig-scheduling\] GPUDevicePluginAcrossRecreate \[Feature:Recreate\]`,
+
+			`\[Feature:LocalStorageCapacityIsolation\]`, // relies on a separate daemonset?
+			`\[sig-cloud-provider-gcp\]`,                // these test require a different configuration - note that GCE tests from the sig-cluster-lifecycle were moved to the sig-cloud-provider-gcpcluster lifecycle see https://github.com/kubernetes/kubernetes/commit/0b3d50b6dccdc4bbd0b3e411c648b092477d79ac#diff-3b1910d08fb8fd8b32956b5e264f87cb
+
+			`kube-dns-autoscaler`, // Don't run kube-dns
+			`should check if Kubernetes master services is included in cluster-info`, // Don't run kube-dns
+			`DNS configMap`, // this tests dns federation configuration via configmap, which we don't support yet
+
+			`NodeProblemDetector`,                   // requires a non-master node to run on
+			`Advanced Audit should audit API calls`, // expects to be able to call /logs
+
+			`Firewall rule should have correct firewall rules for e2e cluster`, // Upstream-install specific
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=2079958
+			`\[sig-network\] \[Feature:Topology Hints\] should distribute endpoints evenly`,
+		},
+		// tests that are known broken and need to be fixed upstream or in openshift
+		// always add an issue here
+		"[Disabled:Broken]": {
+			`mount an API token into pods`,                              // We add 6 secrets, not 1
+			`ServiceAccounts should ensure a single API token exists`,   // We create lots of secrets
+			`unchanging, static URL paths for kubernetes api services`,  // the test needs to exclude URLs that are not part of conformance (/logs)
+			`Services should be able to up and down services`,           // we don't have wget installed on nodes
+			`KubeProxy should set TCP CLOSE_WAIT timeout`,               // the test require communication to port 11302 in the cluster nodes
+			`should check kube-proxy urls`,                              // previously this test was skipped b/c we reported -1 as the number of nodes, now we report proper number and test fails
+			`SSH`,                                                       // TRIAGE
+			`should implement service.kubernetes.io/service-proxy-name`, // this is an optional test that requires SSH. sig-network
+			`recreate nodes and ensure they function upon restart`,      // https://bugzilla.redhat.com/show_bug.cgi?id=1756428
+			`\[Driver: iscsi\]`,                                         // https://bugzilla.redhat.com/show_bug.cgi?id=1711627
+
+			"RuntimeClass should reject",
+
+			`Services should implement service.kubernetes.io/headless`,                    // requires SSH access to function, needs to be refactored
+			`ClusterDns \[Feature:Example\] should create pod that uses dns`,              // doesn't use bindata, not part of kube test binary
+			`Simple pod should return command exit codes should handle in-cluster config`, // kubectl cp doesn't work or is not preserving executable bit, we have this test already
+
+			// TODO(node): configure the cri handler for the runtime class to make this work
+			"should run a Pod requesting a RuntimeClass with a configured handler",
+			"should reject a Pod requesting a RuntimeClass with conflicting node selector",
+			"should run a Pod requesting a RuntimeClass with scheduling",
+
+			// A fix is in progress: https://github.com/openshift/origin/pull/24709
+			`Multi-AZ Clusters should spread the pods of a replication controller across zones`,
+
+			// Upstream assumes all control plane pods are in kube-system namespace and we should revert the change
+			// https://github.com/kubernetes/kubernetes/commit/176c8e219f4c7b4c15d34b92c50bfa5ba02b3aba#diff-28a3131f96324063dd53e17270d435a3b0b3bd8f806ee0e33295929570eab209R78
+			"MetricsGrabber should grab all metrics from a Kubelet",
+			"MetricsGrabber should grab all metrics from API server",
+			"MetricsGrabber should grab all metrics from a ControllerManager",
+			"MetricsGrabber should grab all metrics from a Scheduler",
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1906808
+			`ServiceAccounts should support OIDC discovery of service account issuer`,
+
+			// NFS umount is broken in kernels 5.7+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1854379
+			`\[sig-storage\].*\[Driver: nfs\] \[Testpattern: Dynamic PV \(default fs\)\].*subPath should be able to unmount after the subpath directory is deleted`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1986306
+			`\[sig-cli\] Kubectl client kubectl wait should ignore not found error with --for=delete`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1980141
+			`Netpol NetworkPolicy between server and client should enforce policy to allow traffic only from a pod in a different namespace based on PodSelector and NamespaceSelector`,
+			`Netpol NetworkPolicy between server and client should enforce policy to allow traffic from pods within server namespace based on PodSelector`,
+			`Netpol NetworkPolicy between server and client should enforce policy based on NamespaceSelector with MatchExpressions`,
+			`Netpol NetworkPolicy between server and client should enforce policy based on PodSelector with MatchExpressions`,
+			`Netpol NetworkPolicy between server and client should enforce policy based on PodSelector or NamespaceSelector`,
+			`Netpol NetworkPolicy between server and client should deny ingress from pods on other namespaces`,
+			`Netpol NetworkPolicy between server and client should enforce updated policy`,
+			`Netpol NetworkPolicy between server and client should enforce multiple, stacked policies with overlapping podSelectors`,
+			`Netpol NetworkPolicy between server and client should enforce policy based on any PodSelectors`,
+			`Netpol NetworkPolicy between server and client should enforce policy to allow traffic only from a different namespace, based on NamespaceSelector`,
+			`Netpol \[LinuxOnly\] NetworkPolicy between server and client using UDP should support a 'default-deny-ingress' policy`,
+			`Netpol \[LinuxOnly\] NetworkPolicy between server and client using UDP should enforce policy based on Ports`,
+			`Netpol \[LinuxOnly\] NetworkPolicy between server and client using UDP should enforce policy to allow traffic only from a pod in a different namespace based on PodSelector and NamespaceSelector`,
+
+			`Topology Hints should distribute endpoints evenly`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1908645
+			`\[sig-network\] Networking Granular Checks: Services should function for service endpoints using hostNetwork`,
+			`\[sig-network\] Networking Granular Checks: Services should function for pod-Service\(hostNetwork\)`,
+
+			// https://issues.redhat.com/browse/OCPBUGS-7125
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on different nodes`,
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on the same nodes`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1952460
+			`\[sig-network\] Firewall rule control plane should not expose well-known ports`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1988272
+			`\[sig-network\] Networking should provide Internet connection for containers \[Feature:Networking-IPv6\]`,
+			`\[sig-network\] Networking should provider Internet connection for containers using DNS`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1957894
+			`\[sig-node\] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1952457
+			`\[sig-node\] crictl should be able to run crictl on the node`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1953478
+			`\[sig-storage\] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV`,
+		},
+		// tests that need to be temporarily disabled while the rebase is in progress.
+		"[Disabled:RebaseInProgress]": {
+			// https://issues.redhat.com/browse/OCPBUGS-7297
+			`DNS HostNetwork should resolve DNS of partial qualified names for services on hostNetwork pods with dnsPolicy`,
+			`\[sig-network\] Connectivity Pod Lifecycle should be able to connect to other Pod from a terminating Pod`, // TODO(network): simple test in k8s 1.27, needs investigation
+			`\[sig-cli\] Kubectl client Kubectl prune with applyset should apply and prune objects`,                    // TODO(workloads): alpha feature in k8s 1.27. It's failing with `error: unknown flag: --applyset`. Needs investigation
+
+			// https://issues.redhat.com/browse/OCPBUGS-16760
+			`\[Feature:NodeLogQuery\]`,
+
+			// https://issues.redhat.com/browse/OCPBUGS-16922
+			`AdmissionWebhook \[Privileged:ClusterAdmin\] should reject mutating webhook configurations with invalid match conditions`,
+			`AdmissionWebhook \[Privileged:ClusterAdmin\] should be able to deny pod and configmap creation`,
+			`AdmissionWebhook \[Privileged:ClusterAdmin\] should be able to create and update validating webhook configurations with match conditions`,
+			`AdmissionWebhook \[Privileged:ClusterAdmin\] should be able to create and update mutating webhook configurations with match conditions`,
+			`AdmissionWebhook \[Privileged:ClusterAdmin\] should reject validating webhook configurations with invalid match conditions`,
+			`AdmissionWebhook \[Privileged:ClusterAdmin\] should mutate everything except 'skip-me' configmaps`,
+
+			// https://issues.redhat.com/browse/OCPBUGS-17194
+			`\[sig-node\] ImageCredentialProvider \[Feature:KubeletCredentialProviders\] should be able to create pod with image credentials fetched from external credential provider`,
+
+			// https://issues.redhat.com/browse/OCPBUGS-17202
+			`\[sig-apps\] StatefulSet Scaling StatefulSetStartOrdinal \[Feature:StatefulSetStartOrdinal\] Removing \.start\.ordinal`,
+		},
+		// tests that may work, but we don't support them
+		"[Disabled:Unsupported]": {
+			`\[Driver: rbd\]`,           // OpenShift 4.x does not support Ceph RBD (use CSI instead)
+			`\[Driver: ceph\]`,          // OpenShift 4.x does not support CephFS (use CSI instead)
+			`\[Driver: gluster\]`,       // OpenShift 4.x does not support Gluster
+			`Volumes GlusterFS`,         // OpenShift 4.x does not support Gluster
+			`GlusterDynamicProvisioner`, // OpenShift 4.x does not support Gluster
+
+			// Skip vSphere-specific storage tests. The standard in-tree storage tests for vSphere
+			// (prefixed with `In-tree Volumes [Driver: vsphere]`) are enough for testing this plugin.
+			// https://bugzilla.redhat.com/show_bug.cgi?id=2019115
+			`\[sig-storage\].*\[Feature:vsphere\]`,
+			// Also, our CI doesn't support topology, so disable those tests
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(delayed binding\)\] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies`,
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(delayed binding\)\] topology should provision a volume and schedule a pod with AllowedTopologies`,
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(immediate binding\)\] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies`,
+			`\[sig-storage\] In-tree Volumes \[Driver: vsphere\] \[Testpattern: Dynamic PV \(immediate binding\)\] topology should provision a volume and schedule a pod with AllowedTopologies`,
+		},
+		// tests too slow to be part of conformance
+		"[Slow]": {
+			`\[sig-scalability\]`,                          // disable from the default set for now
+			`should create and stop a working application`, // Inordinately slow tests
+
+			`\[Feature:PerformanceDNS\]`, // very slow
+
+			`validates that there exists conflict between pods with same hostPort and protocol but one using 0\.0\.0\.0 hostIP`, // 5m, really?
+		},
+		// tests that are known flaky
+		"[Flaky]": {
+			`Job should run a job to completion when tasks sometimes fail and are not locally restarted`, // seems flaky, also may require too many resources
+			// TODO(node): test works when run alone, but not in the suite in CI
+			`\[Feature:HPA\] Horizontal pod autoscaling \(scale resource: CPU\) \[sig-autoscaling\] ReplicationController light Should scale from 1 pod to 2 pods`,
+		},
+		// tests that must be run without competition
+		"[Serial]": {
+			`\[Disruptive\]`,
+			`\[Feature:Performance\]`, // requires isolation
+
+			`Service endpoints latency`, // requires low latency
+			`Clean up pods on node`,     // schedules up to max pods per node
+			`DynamicProvisioner should test that deleting a claim before the volume is provisioned deletes the volume`, // test is very disruptive to other tests
+
+			`Should be able to support the 1\.7 Sample API Server using the current Aggregator`, // down apiservices break other clients today https://bugzilla.redhat.com/show_bug.cgi?id=1623195
+
+			`\[Feature:HPA\] Horizontal pod autoscaling \(scale resource: CPU\) \[sig-autoscaling\] ReplicationController light Should scale from 1 pod to 2 pods`,
+
+			`should prevent Ingress creation if more than 1 IngressClass marked as default`, // https://bugzilla.redhat.com/show_bug.cgi?id=1822286
+
+			`\[sig-network\] IngressClass \[Feature:Ingress\] should set default value on new IngressClass`, //https://bugzilla.redhat.com/show_bug.cgi?id=1833583
+		},
+		// Tests that don't pass on disconnected, either due to requiring
+		// internet access for GitHub (e.g. many of the s2i builds), or
+		// because of pullthrough not supporting ICSP (https://bugzilla.redhat.com/show_bug.cgi?id=1918376)
+		"[Skipped:Disconnected]": {
+			// Internet access required
+			`\[sig-network\] Networking should provide Internet connection for containers`,
+		},
+		"[Skipped:azure]": {
+			"Networking should provide Internet connection for containers", // Azure does not allow ICMP traffic to internet.
+			// Azure CSI migration changed how we treat regions without zones.
+			// See https://bugzilla.redhat.com/bugzilla/show_bug.cgi?id=2066865
+			`\[sig-storage\] In-tree Volumes \[Driver: azure-disk\] \[Testpattern: Dynamic PV \(immediate binding\)\] topology should provision a volume and schedule a pod with AllowedTopologies`,
+			`\[sig-storage\] In-tree Volumes \[Driver: azure-disk\] \[Testpattern: Dynamic PV \(delayed binding\)\] topology should provision a volume and schedule a pod with AllowedTopologies`,
+		},
+		"[Skipped:gce]": {
+			// Requires creation of a different compute instance in a different zone and is not compatible with volumeBindingMode of WaitForFirstConsumer which we use in 4.x
+			`\[sig-storage\] Multi-AZ Cluster Volumes should only be allowed to provision PDs in zones where nodes exist`,
+
+			// The following tests try to ssh directly to a node. None of our nodes have external IPs
+			`\[k8s.io\] \[sig-node\] crictl should be able to run crictl on the node`,
+			`\[sig-storage\] Flexvolumes should be mountable`,
+			`\[sig-storage\] Detaching volumes should not work when mount is in progress`,
+
+			// We are using openshift-sdn to conceal metadata
+			`\[sig-auth\] Metadata Concealment should run a check-metadata-concealment job to completion`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1740959
+			`\[sig-api-machinery\] AdmissionWebhook should be able to deny pod and configmap creation`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1745720
+			`\[sig-storage\] CSI Volumes \[Driver: pd.csi.storage.gke.io\]\[Serial\]`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1749882
+			`\[sig-storage\] CSI Volumes CSI Topology test using GCE PD driver \[Serial\]`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1751367
+			`gce-localssd-scsi-fs`,
+
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1750851
+			// should be serial if/when it's re-enabled
+			`\[HPA\] Horizontal pod autoscaling \(scale resource: Custom Metrics from Stackdriver\)`,
+			`\[Feature:CustomMetricsAutoscaling\]`,
+		},
+		"[sig-node]": {
+			`\[NodeConformance\]`,
+			`NodeLease`,
+			`lease API`,
+			`\[NodeFeature`,
+			`\[NodeAlphaFeature`,
+			`Probing container`,
+			`Security Context When creating a`,
+			`Downward API should create a pod that prints his name and namespace`,
+			`Liveness liveness pods should be automatically restarted`,
+			`Secret should create a pod that reads a secret`,
+			`Pods should delete a collection of pods`,
+			`Pods should run through the lifecycle of Pods and PodStatus`,
+		},
+		"[sig-cluster-lifecycle]": {
+			`Feature:ClusterAutoscalerScalability`,
+			`recreate nodes and ensure they function`,
+		},
+		"[sig-arch]": {
+			// not run, assigned to arch as catch-all
+			`\[Feature:GKELocalSSD\]`,
+			`\[Feature:GKENodePool\]`,
+		},
+		// Tests that don't pass under openshift-sdn.
+		// These are skipped explicitly by openshift-hack/test-kubernetes-e2e.sh,
+		// but will also be skipped by openshift-tests in jobs that use openshift-sdn.
+		"[Skipped:Network/OpenShiftSDN]": {
+			`NetworkPolicy.*IPBlock`,    // feature is not supported by openshift-sdn
+			`NetworkPolicy.*[Ee]gress`,  // feature is not supported by openshift-sdn
+			`NetworkPolicy.*named port`, // feature is not supported by openshift-sdn
+
+			`NetworkPolicy between server and client should support a 'default-deny-all' policy`,            // uses egress feature
+			`NetworkPolicy between server and client should stop enforcing policies after they are deleted`, // uses egress feature
+		},
+
+		// These tests are skipped when openshift-tests needs to use a proxy to reach the
+		// cluster -- either because the test won't work while proxied, or because the test
+		// itself is testing a functionality using it's own proxy.
+		"[Skipped:Proxy]": {
+			// These tests setup their own proxy, which won't work when we need to access the
+			// cluster through a proxy.
+			`\[sig-cli\] Kubectl client Simple pod should support exec through an HTTP proxy`,
+			`\[sig-cli\] Kubectl client Simple pod should support exec through kubectl proxy`,
+
+			// Kube currently uses the x/net/websockets pkg, which doesn't work with proxies.
+			// See: https://github.com/kubernetes/kubernetes/pull/103595
+			`\[sig-node\] Pods should support retrieving logs from the container over websockets`,
+			`\[sig-cli\] Kubectl Port forwarding With a server listening on localhost should support forwarding over websockets`,
+			`\[sig-cli\] Kubectl Port forwarding With a server listening on 0.0.0.0 should support forwarding over websockets`,
+			`\[sig-node\] Pods should support remote command execution over websockets`,
+
+			// These tests are flacky and require internet access
+			// See https://bugzilla.redhat.com/show_bug.cgi?id=2019375
+			`\[sig-network\] DNS should resolve DNS of partial qualified names for services`,
+			`\[sig-network\] DNS should provide DNS for the cluster`,
+			// This test does not work when using in-proxy cluster, see https://bugzilla.redhat.com/show_bug.cgi?id=2084560
+			`\[sig-network\] Networking should provide Internet connection for containers`,
+		},
+
+		"[Skipped:SingleReplicaTopology]": {
+			`\[sig-apps\] Daemon set \[Serial\] should rollback without unnecessary restarts \[Conformance\]`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] doesn't evict pod with tolerations from tainted nodes`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] eventually evict pod with finite tolerations from tainted nodes`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] evicts pods from tainted nodes`,
+			`\[sig-node\] NoExecuteTaintManager Single Pod \[Serial\] removing taint cancels eviction \[Disruptive\] \[Conformance\]`,
+			`\[sig-node\] NoExecuteTaintManager Multiple Pods \[Serial\] evicts pods with minTolerationSeconds \[Disruptive\] \[Conformance\]`,
+			`\[sig-node\] NoExecuteTaintManager Multiple Pods \[Serial\] only evicts pods without tolerations from tainted nodes`,
+			`\[sig-cli\] Kubectl client Kubectl taint \[Serial\] should remove all the taints with the same key off a node`,
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on different nodes`,
+			`\[sig-network\] LoadBalancers should be able to preserve UDP traffic when server pod cycles for a LoadBalancer service on the same nodes`,
+		},
+
+		// Tests which can't be run/don't make sense to run against a cluster with all optional capabilities disabled
+		"[Skipped:NoOptionalCapabilities]": {
+			// Requires CSISnapshot capability
+			`\[Feature:VolumeSnapshotDataSource\]`,
+			// Requires Storage capability
+			`\[Driver: aws\]`,
+			`\[Feature:StorageProvider\]`,
+		},
+
+		// tests that don't pass under openshift-sdn multitenant mode
+		"[Skipped:Network/OpenShiftSDN/Multitenant]": {
+			`\[Feature:NetworkPolicy\]`, // not compatible with multitenant mode
+		},
+		// tests that don't pass under OVN Kubernetes
+		"[Skipped:Network/OVNKubernetes]": {
+			// ovn-kubernetes does not support named ports
+			`NetworkPolicy.*named port`,
+		},
+
+		"[Skipped:ibmroks]": {
+			// Calico is allowing the request to timeout instead of returning 'REFUSED'
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825021 - ROKS: calico SDN results in a request timeout when accessing services with no endpoints
+			`\[sig-network\] Services should be rejected when no endpoints exist`,
+
+			// Nodes in ROKS have access to secrets in the cluster to handle encryption
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825013 - ROKS: worker nodes have access to secrets in the cluster
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting a non-existent configmap should exit with the Forbidden error, not a NotFound error`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting a non-existent secret should exit with the Forbidden error, not a NotFound error`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting a secret for a workload the node has access to should succeed`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting an existing configmap should exit with the Forbidden error`,
+			`\[sig-auth\] \[Feature:NodeAuthorizer\] Getting an existing secret should exit with the Forbidden error`,
+
+			// Access to node external address is blocked from pods within a ROKS cluster by Calico
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825016 - e2e: NodeAuthenticator tests use both external and internal addresses for node
+			`\[sig-auth\] \[Feature:NodeAuthenticator\] The kubelet's main port 10250 should reject requests with no credentials`,
+			`\[sig-auth\] \[Feature:NodeAuthenticator\] The kubelet can delegate ServiceAccount tokens to the API server`,
+
+			// Mode returned by RHEL7 worker contains an extra character not expected by the test: dgtrwx vs dtrwx
+			// https://bugzilla.redhat.com/show_bug.cgi?id=1825024 - e2e: Failing test - HostPath should give a volume the correct mode
+			`\[sig-storage\] HostPath should give a volume the correct mode`,
+		},
+	}
+
+	ExcludedTests = []string{
+		`\[Disabled:`,
+		`\[Disruptive\]`,
+		`\[Skipped\]`,
+		`\[Slow\]`,
+		`\[Flaky\]`,
+		`\[Local\]`,
+	}
+)
diff --git a/openshift-hack/e2e/annotate/rules_test.go b/openshift-hack/e2e/annotate/rules_test.go
new file mode 100644
index 00000000000..b929a573d90
--- /dev/null
+++ b/openshift-hack/e2e/annotate/rules_test.go
@@ -0,0 +1,88 @@
+package annotate
+
+import (
+	"testing"
+
+	"github.com/onsi/ginkgo/v2/types"
+)
+
+type testNode struct {
+	text string
+}
+
+func (n *testNode) CodeLocations() []types.CodeLocation {
+	return []types.CodeLocation{{FileName: "k8s.io/kubernetes"}}
+}
+
+func (n *testNode) Text() string {
+	return n.text
+}
+
+func (n *testNode) AppendText(text string) {
+	n.text += text
+}
+
+func TestStockRules(t *testing.T) {
+	tests := []struct {
+		name string
+
+		testName string
+
+		expectedLabel string
+		expectedText  string
+	}{
+		{
+			name:          "simple serial match",
+			testName:      "[Serial] test",
+			expectedLabel: " [Suite:openshift/conformance/serial]",
+			expectedText:  "[Serial] test [Suite:openshift/conformance/serial]",
+		},
+		{
+			name:          "don't tag skipped",
+			testName:      `[Serial] example test [Skipped:gce]`,
+			expectedLabel: ` [Suite:openshift/conformance/serial]`,
+			expectedText:  `[Serial] example test [Skipped:gce] [Suite:openshift/conformance/serial]`, // notice that this isn't categorized into any of our buckets
+		},
+		{
+			name:          "not skipped",
+			testName:      `[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]`,
+			expectedLabel: ` [Suite:openshift/conformance/parallel/minimal]`,
+			expectedText:  `[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance] [Suite:openshift/conformance/parallel/minimal]`,
+		},
+		{
+			name:          "should skip localssd on gce",
+			testName:      `[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted`,
+			expectedLabel: ` [Skipped:gce] [Suite:openshift/conformance/serial]`,
+			expectedText:  `[sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: gce-localssd-scsi-fs] [Serial] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted [Skipped:gce] [Suite:openshift/conformance/serial]`, // notice that this isn't categorized into any of our buckets
+		},
+		{
+			name:          "should skip NetworkPolicy tests on multitenant",
+			testName:      `should do something with NetworkPolicy`,
+			expectedLabel: ` [Suite:openshift/conformance/parallel]`,
+			expectedText:  `should do something with NetworkPolicy [Suite:openshift/conformance/parallel]`,
+		},
+	}
+
+	for _, test := range tests {
+		t.Run(test.name, func(t *testing.T) {
+			testRenamer := newGenerator(TestMaps)
+			testNode := &testNode{
+				text: test.testName,
+			}
+
+			testRenamer.generateRename(test.testName, testNode)
+			changed := testRenamer.output[test.testName]
+
+			if e, a := test.expectedLabel, changed; e != a {
+				t.Error(a)
+			}
+			testRenamer = newRenamerFromGenerated(map[string]string{test.testName: test.expectedLabel})
+			testRenamer.updateNodeText(test.testName, testNode)
+
+			if e, a := test.expectedText, testNode.Text(); e != a {
+				t.Logf(e)
+				t.Error(a)
+			}
+		})
+	}
+}
diff --git a/openshift-hack/e2e/include.go b/openshift-hack/e2e/include.go
new file mode 100644
index 00000000000..3275a49bfc1
--- /dev/null
+++ b/openshift-hack/e2e/include.go
@@ -0,0 +1,23 @@
+package e2e
+
+// This file should import all the packages defining k8s e2e tests that are
+// relevant to openshift. It is intended to affect:
+//
+// - what is included in the k8s-e2e.test binary built from this package
+// - the annotations generated by the annotate package
+
+import (
+	_ "k8s.io/kubernetes/test/e2e"
+	_ "k8s.io/kubernetes/test/e2e/apimachinery"
+	_ "k8s.io/kubernetes/test/e2e/apps"
+	_ "k8s.io/kubernetes/test/e2e/auth"
+	_ "k8s.io/kubernetes/test/e2e/autoscaling"
+	_ "k8s.io/kubernetes/test/e2e/common"
+	_ "k8s.io/kubernetes/test/e2e/instrumentation"
+	_ "k8s.io/kubernetes/test/e2e/kubectl"
+	_ "k8s.io/kubernetes/test/e2e/network"
+	_ "k8s.io/kubernetes/test/e2e/node"
+	_ "k8s.io/kubernetes/test/e2e/scheduling"
+	_ "k8s.io/kubernetes/test/e2e/storage"
+	_ "k8s.io/kubernetes/test/e2e/storage/csi_mock"
+)
diff --git a/openshift-hack/e2e/kube_e2e_test.go b/openshift-hack/e2e/kube_e2e_test.go
new file mode 100644
index 00000000000..86c8821b823
--- /dev/null
+++ b/openshift-hack/e2e/kube_e2e_test.go
@@ -0,0 +1,102 @@
+package e2e
+
+//go:generate go run -mod vendor ./annotate/cmd -- ./annotate/generated/zz_generated.annotations.go
+
+// This file duplicates most of test/e2e/e2e_test.go but limits the included
+// tests (via include.go) to tests that are relevant to openshift.
+
+import (
+	"flag"
+	"fmt"
+	"math/rand"
+	"os"
+	"testing"
+	"time"
+
+	"gopkg.in/yaml.v2"
+
+	// Never, ever remove the line with "/ginkgo". Without it,
+	// the ginkgo test runner will not detect that this
+	// directory contains a Ginkgo test suite.
+	// See https://github.com/kubernetes/kubernetes/issues/74827
+	// "github.com/onsi/ginkgo/v2"
+
+	"k8s.io/component-base/version"
+	conformancetestdata "k8s.io/kubernetes/test/conformance/testdata"
+	"k8s.io/kubernetes/test/e2e"
+	"k8s.io/kubernetes/test/e2e/framework"
+	"k8s.io/kubernetes/test/e2e/framework/testfiles"
+	e2etestingmanifests "k8s.io/kubernetes/test/e2e/testing-manifests"
+	testfixtures "k8s.io/kubernetes/test/fixtures"
+	"k8s.io/kubernetes/test/utils/image"
+
+	// Ensure test annotation
+	_ "k8s.io/kubernetes/openshift-hack/e2e/annotate/generated"
+)
+
+func TestMain(m *testing.M) {
+	var versionFlag bool
+	flag.CommandLine.BoolVar(&versionFlag, "version", false, "Displays version information.")
+
+	// Register test flags, then parse flags.
+	e2e.HandleFlags()
+
+	if framework.TestContext.ListImages {
+		for _, v := range image.GetImageConfigs() {
+			fmt.Println(v.GetE2EImage())
+		}
+		os.Exit(0)
+	}
+	if versionFlag {
+		fmt.Printf("%s\n", version.Get())
+		os.Exit(0)
+	}
+
+	// Enable embedded FS file lookup as fallback
+	testfiles.AddFileSource(e2etestingmanifests.GetE2ETestingManifestsFS())
+	testfiles.AddFileSource(testfixtures.GetTestFixturesFS())
+	testfiles.AddFileSource(conformancetestdata.GetConformanceTestdataFS())
+
+	if framework.TestContext.ListConformanceTests {
+		var tests []struct {
+			Testname    string `yaml:"testname"`
+			Codename    string `yaml:"codename"`
+			Description string `yaml:"description"`
+			Release     string `yaml:"release"`
+			File        string `yaml:"file"`
+		}
+
+		data, err := testfiles.Read("test/conformance/testdata/conformance.yaml")
+		if err != nil {
+			fmt.Fprintln(os.Stderr, err)
+			os.Exit(1)
+		}
+		if err := yaml.Unmarshal(data, &tests); err != nil {
+			fmt.Fprintln(os.Stderr, err)
+			os.Exit(1)
+		}
+		if err := yaml.NewEncoder(os.Stdout).Encode(tests); err != nil {
+			fmt.Fprintln(os.Stderr, err)
+			os.Exit(1)
+		}
+		os.Exit(0)
+	}
+
+	framework.AfterReadingAllFlags(&framework.TestContext)
+
+	// TODO: Deprecating repo-root over time... instead just use gobindata_util.go , see #23987.
+	// Right now it is still needed, for example by
+	// test/e2e/framework/ingress/ingress_utils.go
+	// for providing the optional secret.yaml file and by
+	// test/e2e/framework/util.go for cluster/log-dump.
+	if framework.TestContext.RepoRoot != "" {
+		testfiles.AddFileSource(testfiles.RootFileSource{Root: framework.TestContext.RepoRoot})
+	}
+
+	rand.Seed(time.Now().UnixNano())
+	os.Exit(m.Run())
+}
+
+func TestE2E(t *testing.T) {
+	e2e.RunE2ETests(t)
+}
diff --git a/openshift-hack/e2e/namespace.go b/openshift-hack/e2e/namespace.go
new file mode 100644
index 00000000000..d82fabb56d3
--- /dev/null
+++ b/openshift-hack/e2e/namespace.go
@@ -0,0 +1,160 @@
+package e2e
+
+import (
+	"context"
+	"fmt"
+	"runtime/debug"
+	"strings"
+
+	"github.com/onsi/ginkgo/v2"
+
+	corev1 "k8s.io/api/core/v1"
+	rbacv1 "k8s.io/api/rbac/v1"
+	apierrs "k8s.io/apimachinery/pkg/api/errors"
+	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
+	"k8s.io/apimachinery/pkg/util/wait"
+	kclientset "k8s.io/client-go/kubernetes"
+	rbacv1client "k8s.io/client-go/kubernetes/typed/rbac/v1"
+	"k8s.io/client-go/util/retry"
+	"k8s.io/kubernetes/test/e2e/framework"
+
+	projectv1 "github.com/openshift/api/project/v1"
+	securityv1client "github.com/openshift/client-go/security/clientset/versioned"
+)
+
+// CreateTestingNS ensures that kubernetes e2e tests have their service accounts in the privileged and anyuid SCCs
+func CreateTestingNS(ctx context.Context, baseName string, c kclientset.Interface, labels map[string]string, isKubeNamespace bool) (*corev1.Namespace, error) {
+	if !strings.HasPrefix(baseName, "e2e-") {
+		baseName = "e2e-" + baseName
+	}
+
+	if isKubeNamespace {
+		if labels == nil {
+			labels = map[string]string{}
+		}
+		labels["security.openshift.io/disable-securitycontextconstraints"] = "true"
+	}
+
+	ns, err := framework.CreateTestingNS(ctx, baseName, c, labels)
+	if err != nil {
+		return ns, err
+	}
+
+	if !isKubeNamespace {
+		return ns, err
+	}
+
+	// Add anyuid and privileged permissions for upstream tests
+	clientConfig, err := framework.LoadConfig()
+	if err != nil {
+		return ns, err
+	}
+
+	securityClient, err := securityv1client.NewForConfig(clientConfig)
+	if err != nil {
+		return ns, err
+	}
+	framework.Logf("About to run a Kube e2e test, ensuring namespace/%s is privileged", ns.Name)
+	// add the "privileged" scc to ensure pods that explicitly
+	// request extra capabilities are not rejected
+	addE2EServiceAccountsToSCC(ctx, securityClient, []corev1.Namespace{*ns}, "privileged")
+	// add the "anyuid" scc to ensure pods that don't specify a
+	// uid don't get forced into a range (mimics upstream
+	// behavior)
+	addE2EServiceAccountsToSCC(ctx, securityClient, []corev1.Namespace{*ns}, "anyuid")
+	// add the "hostmount-anyuid" scc to ensure pods using hostPath
+	// can execute tests
+	addE2EServiceAccountsToSCC(ctx, securityClient, []corev1.Namespace{*ns}, "hostmount-anyuid")
+
+	// The intra-pod test requires that the service account have
+	// permission to retrieve service endpoints.
+	rbacClient, err := rbacv1client.NewForConfig(clientConfig)
+	if err != nil {
+		return ns, err
+	}
+	addRoleToE2EServiceAccounts(ctx, rbacClient, []corev1.Namespace{*ns}, "view")
+
+	// in practice too many kube tests ignore scheduling constraints
+	allowAllNodeScheduling(ctx, c, ns.Name)
+
+	return ns, err
+}
+
+var longRetry = wait.Backoff{Steps: 100}
+
+// TODO: ideally this should be rewritten to use dynamic client, not to rely on openshift types
+func addE2EServiceAccountsToSCC(ctx context.Context, securityClient securityv1client.Interface, namespaces []corev1.Namespace, sccName string) {
+	// Because updates can race, we need to set the backoff retries to be > than the number of possible
+	// parallel jobs starting at once. Set very high to allow future high parallelism.
+	err := retry.RetryOnConflict(longRetry, func() error {
+		scc, err := securityClient.SecurityV1().SecurityContextConstraints().Get(ctx, sccName, metav1.GetOptions{})
+		if err != nil {
+			if apierrs.IsNotFound(err) {
+				return nil
+			}
+			return err
+		}
+
+		for _, ns := range namespaces {
+			scc.Groups = append(scc.Groups, fmt.Sprintf("system:serviceaccounts:%s", ns.Name))
+		}
+		if _, err := securityClient.SecurityV1().SecurityContextConstraints().Update(ctx, scc, metav1.UpdateOptions{}); err != nil {
+			return err
+		}
+		return nil
+	})
+	if err != nil {
+		fatalErr(err)
+	}
+}
+
+func fatalErr(msg interface{}) {
+	// the path that leads to this being called isn't always clear...
+	fmt.Fprintln(ginkgo.GinkgoWriter, string(debug.Stack()))
+	framework.Failf("%v", msg)
+}
+
+func addRoleToE2EServiceAccounts(ctx context.Context, rbacClient rbacv1client.RbacV1Interface, namespaces []corev1.Namespace, roleName string) {
+	err := retry.RetryOnConflict(longRetry, func() error {
+		for _, ns := range namespaces {
+			if ns.Status.Phase != corev1.NamespaceTerminating {
+				_, err := rbacClient.RoleBindings(ns.Name).Create(ctx, &rbacv1.RoleBinding{
+					ObjectMeta: metav1.ObjectMeta{GenerateName: "default-" + roleName, Namespace: ns.Name},
+					RoleRef: rbacv1.RoleRef{
+						Kind: "ClusterRole",
+						Name: roleName,
+					},
+					Subjects: []rbacv1.Subject{
+						{Name: "default", Namespace: ns.Name, Kind: rbacv1.ServiceAccountKind},
+					},
+				}, metav1.CreateOptions{})
+				if err != nil {
+					framework.Logf("Warning: Failed to add role to e2e service account: %v", err)
+				}
+			}
+		}
+		return nil
+	})
+	if err != nil {
+		fatalErr(err)
+	}
+}
+
+// allowAllNodeScheduling sets the annotation on namespace that allows all nodes to be scheduled onto.
+func allowAllNodeScheduling(ctx context.Context, c kclientset.Interface, namespace string) {
+	err := retry.RetryOnConflict(longRetry, func() error {
+		ns, err := c.CoreV1().Namespaces().Get(ctx, namespace, metav1.GetOptions{})
+		if err != nil {
+			return err
+		}
+		if ns.Annotations == nil {
+			ns.Annotations = make(map[string]string)
+		}
+		ns.Annotations[projectv1.ProjectNodeSelector] = ""
+		_, err = c.CoreV1().Namespaces().Update(ctx, ns, metav1.UpdateOptions{})
+		return err
+	})
+	if err != nil {
+		fatalErr(err)
+	}
+}
diff --git a/openshift-hack/images/OWNERS b/openshift-hack/images/OWNERS
new file mode 100644
index 00000000000..7b196b0fb70
--- /dev/null
+++ b/openshift-hack/images/OWNERS
@@ -0,0 +1,11 @@
+reviewers:
+  - smarterclayton
+  - giuseppe
+  - JacobTanenbaum
+  - pweil-
+  - pecameron
+  - sdodson
+approvers:
+  - smarterclayton
+  - pweil-
+  - sdodson
diff --git a/openshift-hack/images/hyperkube/Dockerfile.rhel b/openshift-hack/images/hyperkube/Dockerfile.rhel
new file mode 100644
index 00000000000..021f6a64c20
--- /dev/null
+++ b/openshift-hack/images/hyperkube/Dockerfile.rhel
@@ -0,0 +1,16 @@
+FROM registry.ci.openshift.org/ocp/builder:rhel-8-golang-1.20-openshift-4.14 AS builder
+WORKDIR /go/src/k8s.io/kubernetes
+COPY . .
+RUN make WHAT='cmd/kube-apiserver cmd/kube-controller-manager cmd/kube-scheduler cmd/kubelet cmd/watch-termination openshift-hack/cmd/k8s-tests' && \
+    mkdir -p /tmp/build && \
+    cp openshift-hack/images/hyperkube/hyperkube openshift-hack/images/hyperkube/kubensenter /tmp/build && \
+    cp /go/src/k8s.io/kubernetes/_output/local/bin/linux/$(go env GOARCH)/{kube-apiserver,kube-controller-manager,kube-scheduler,kubelet,watch-termination,k8s-tests} \
+    /tmp/build
+
+FROM registry.ci.openshift.org/ocp/4.14:base
+RUN yum install -y --setopt=tsflags=nodocs --setopt=skip_missing_names_on_install=False iproute && yum clean all
+COPY --from=builder /tmp/build/* /usr/bin/
+LABEL io.k8s.display-name="OpenShift Kubernetes Server Commands" \
+      io.k8s.description="OpenShift is a platform for developing, building, and deploying containerized applications." \
+      io.openshift.tags="openshift,hyperkube" \
+      io.openshift.build.versions="kubernetes=1.28.2"
diff --git a/openshift-hack/images/hyperkube/OWNERS b/openshift-hack/images/hyperkube/OWNERS
new file mode 100644
index 00000000000..e8146784930
--- /dev/null
+++ b/openshift-hack/images/hyperkube/OWNERS
@@ -0,0 +1,5 @@
+reviewers:
+  - smarterclayton
+  - sdodson
+approvers:
+  - smarterclayton
diff --git a/openshift-hack/images/hyperkube/hyperkube b/openshift-hack/images/hyperkube/hyperkube
new file mode 100755
index 00000000000..cfed9cd737c
--- /dev/null
+++ b/openshift-hack/images/hyperkube/hyperkube
@@ -0,0 +1,57 @@
+#!/usr/bin/env bash
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+BINS=(
+  kube-apiserver
+  kube-controller-manager
+  kube-scheduler
+  kubelet
+)
+
+function array_contains() {
+  local search="$1"
+  local element
+  shift
+  for element; do
+    if [[ "${element}" == "${search}" ]]; then
+      return 0
+     fi
+  done
+  return 1
+}
+
+function print_usage() {
+  cat <<EOF
+Usage:
+  $(basename "$0") [command]
+Available Commands:
+  help                     Help about any command
+  kube-apiserver
+  kube-controller-manager
+  kube-scheduler
+  kubelet
+EOF
+  exit 0
+}
+
+function main() {
+  if [[ "$#" -lt 1 || "${1:-}" == "--help" || "${1:-}" == "help" ]]; then
+    print_usage
+  fi
+  if ! array_contains "$1" "${BINS[@]}"; then
+    echo "$1: command not supported"
+    print_usage
+  fi
+  command=${1}
+  shift
+  if ! command -v "${command}" &>/dev/null; then
+    echo "${command}: command not found"
+    exit 1
+  fi
+  exec "${command}" "${@}"
+}
+
+main "${@}"
\ No newline at end of file
diff --git a/openshift-hack/images/hyperkube/kubensenter b/openshift-hack/images/hyperkube/kubensenter
new file mode 100644
index 00000000000..56ab26ee9e9
--- /dev/null
+++ b/openshift-hack/images/hyperkube/kubensenter
@@ -0,0 +1,117 @@
+#!/bin/bash
+
+# shellcheck disable=SC2016
+usage() {
+    echo "A command line wrapper to run commands or shells inside the"
+    echo "kubens.service mount namespace."
+    echo
+    echo "Usage:"
+    echo "    $(basename "$0") [--verbose|--quiet] [command ...]"
+    echo
+    echo 'Autodetect whether the `kubens.service` has pinned a mount namespace in a'
+    echo 'well-known location, and if so, join it by passing it and the user-specified'
+    echo 'command to nsenter(1). If `kubens.service` has not set up the mount namespace,'
+    echo 'the user-specified command is still executed by nsenter(1) but no namespace is'
+    echo 'entered.'
+    echo
+    echo 'If $KUBENSMNT is set in the environment, skip autodetection and attempt to join'
+    echo 'that mount namespace by passing it and the user-specified command to'
+    echo 'nsenter(1). If the mount namespace is missing or invalid, the command will'
+    echo 'fail.'
+    echo
+    echo 'In either case, if no command is given on the command line, nsenter(1) will'
+    echo 'spawn a new interactive shell which will be inside the mount namespace if'
+    echo 'detected.'
+    exit 1
+}
+
+LOGLEVEL=${KUBENSENTER_LOG:-1}
+_log() {
+    local level=$1; shift
+    if [[ $level -le $LOGLEVEL ]]; then
+        echo "kubensenter: $*" >&2
+    fi
+}
+
+info() {
+    _log 1 "$*"
+}
+
+debug() {
+    _log 2 "$*"
+}
+
+# Returns 0 if the argument given is a mount namespace
+ismnt() {
+    local nsfs
+    nsfs=$(findmnt -o SOURCE -n -t nsfs "$1")
+    [[ $nsfs =~ ^nsfs\[mnt:\[ ]]
+}
+
+# Set KUBENSMNT to the default location that kubens.service uses if KUBENSMNT isn't already set.
+DEFAULT_KUBENSMNT=${DEFAULT_KUBENSMNT:-"/run/kubens/mnt"}
+autodetect() {
+    local default=$DEFAULT_KUBENSMNT
+    if [[ -n $KUBENSMNT ]]; then
+        debug "Autodetect: \$KUBENSMNT already set"
+        return 0
+    fi
+    if [[ ! -e $default ]]; then
+        debug "Autodetect: No mount namespace found at $default"
+        return 1
+    fi
+    if ! ismnt "$default"; then
+        info "Autodetect: Stale or mismatched namespace at $default"
+        return 1
+    fi
+    KUBENSMNT=$default
+    info "Autodetect: kubens.service namespace found at $KUBENSMNT"
+    return 0
+}
+
+# Wrap the user-given command in nsenter, joining the mount namespace set in $KUBENSMNT if set
+kubensenter() {
+    local nsarg
+    if [[ -n $KUBENSMNT ]]; then
+        debug "Joining mount namespace in $KUBENSMNT"
+        nsarg=$(printf -- "--mount=%q" "$KUBENSMNT")
+    else
+        debug "KUBENSMNT not set; running normally"
+        # Intentional fallthrough to run nsenter anyway:
+        # - If $@ is non-empty, nsenter effectively runs `exec "$@"`
+        # - If $@ is empty, nsenter spawns a new shell
+    fi
+    # Using 'exec' is important here; Without it, systemd may have trouble
+    # seeing the underlying process especially if it's using 'Type=notify'
+    # semantics.
+    # shellcheck disable=SC2086
+    # ^- Intentionally collapse $nsarg if not set (and we've already shell-quoted it above if we did set it)
+    exec nsenter $nsarg "$@"
+}
+
+main() {
+    while [[ -n $1 ]]; do
+        case "$1" in
+            -h | --help)
+                usage
+                ;;
+            -v | --verbose)
+                shift
+                ((LOGLEVEL++))
+                ;;
+            -q | --quiet)
+                shift
+                ((LOGLEVEL--))
+                ;;
+            *)
+                break
+                ;;
+        esac
+    done
+
+    autodetect
+    kubensenter "$@"
+}
+
+# bash modulino
+[[ "${BASH_SOURCE[0]}" == "$0" ]] && main "$@"
diff --git a/openshift-hack/images/os/Dockerfile b/openshift-hack/images/os/Dockerfile
new file mode 100644
index 00000000000..eb19c615dbf
--- /dev/null
+++ b/openshift-hack/images/os/Dockerfile
@@ -0,0 +1,27 @@
+# this should match whatever we have in https://github.com/openshift/release/blob/master/ci-operator/config/openshift/kubernetes/openshift-kubernetes-master.yaml
+# - context_dir: openshift-hack/images/os/
+#   from: base
+#   inputs:
+#     base-machine-with-rpms:
+#       as:
+#       - centos:stream9 <-- here
+#     machine-os-content-base:
+#       as:
+#       - registry.svc.ci.openshift.org/openshift/origin-v4.0:machine-os-content
+#   to: machine-os-content
+FROM centos:stream9 As build
+
+# the registry is defined here:
+# https://github.com/openshift/release/blob/b45a09d248b8cdb8fe3bf5f3cfa0b4fee57d04c8/ci-operator/config/openshift/kubernetes/openshift-kubernetes-release-4.10.yaml#L68
+COPY --from=registry.svc.ci.openshift.org/openshift/origin-v4.0:machine-os-content /srv/ /srv/
+COPY --from=registry.svc.ci.openshift.org/openshift/origin-v4.0:machine-os-content /extensions/ /extensions/
+WORKDIR /
+COPY install.sh .
+RUN ./install.sh
+
+FROM scratch
+COPY --from=build /srv/ /srv/
+COPY --from=build /extensions/ /extensions/
+
+LABEL io.openshift.build.version-display-names="machine-os=rhcos image for testing openshift kubernetes kubelet only- if you see this outside of PR runs for openshift kubernetes- you found an urgent blocker bug"
+LABEL io.openshift.build.versions="machine-os=1.2.3-testing-if-you-see-this-outside-of-PR-runs-for-openshift-kubernetes-you-found-an-urgent-blocker-bug"
diff --git a/openshift-hack/images/os/install.sh b/openshift-hack/images/os/install.sh
new file mode 100755
index 00000000000..fc01366cf54
--- /dev/null
+++ b/openshift-hack/images/os/install.sh
@@ -0,0 +1,40 @@
+#!/usr/bin/env bash
+
+set -xeou pipefail
+
+yum install -y ostree rpm-ostree yum-utils selinux-policy-targeted xfsprogs
+curl http://base-4-10-rhel8.ocp.svc > /etc/yum.repos.d/rhel8.repo
+
+commit=$( find /srv -name *.commit | sed -Ee 's|.*objects/(.+)/(.+)\.commit|\1\2|' | head -1 )
+mkdir /tmp/working && cd /tmp/working
+rpm-ostree db list --repo /srv/repo $commit > /tmp/packages
+
+PACKAGES=(openshift-hyperkube)
+yumdownloader -y --disablerepo=* --enablerepo=built --destdir=/tmp/rpms "${PACKAGES[@]}"
+if ! grep -q cri-o /tmp/packages; then
+  yumdownloader -y --disablerepo=* --enablerepo=rhel-8* --destdir=/tmp/rpms cri-o cri-tools
+fi
+
+ls /tmp/rpms/ && (cd /tmp/rpms/ && ls ${PACKAGES[@]/%/*})
+for i in $(find /tmp/rpms/ -name *.rpm); do
+  echo "Extracting $i ..."; rpm2cpio $i | cpio -div
+done
+
+if [[ -d etc ]]; then
+  mv etc usr/
+fi
+
+mkdir -p /tmp/tmprootfs/etc
+
+ostree --repo=/srv/repo checkout \
+  -U $commit \
+  --subpath /usr/etc/selinux \
+  /tmp/tmprootfs/etc/selinux
+
+ostree --repo=/srv/repo commit \
+  --parent=$commit \
+  --tree=ref=$commit \
+  --tree=dir=. \
+  --selinux-policy /tmp/tmprootfs \
+  -s "origin-ci-dev overlay RPMs" \
+  --branch=origin-ci-dev
diff --git a/openshift-hack/images/tests/Dockerfile.rhel b/openshift-hack/images/tests/Dockerfile.rhel
new file mode 100644
index 00000000000..e5fcc55f941
--- /dev/null
+++ b/openshift-hack/images/tests/Dockerfile.rhel
@@ -0,0 +1,21 @@
+FROM registry.ci.openshift.org/ocp/builder:rhel-8-golang-1.20-openshift-4.14 AS builder
+WORKDIR /go/src/k8s.io/kubernetes
+COPY . .
+RUN make WHAT=openshift-hack/e2e/k8s-e2e.test; \
+    make WHAT=vendor/github.com/onsi/ginkgo/v2/ginkgo; \
+    mkdir -p /tmp/build; \
+    cp /go/src/k8s.io/kubernetes/_output/local/bin/linux/$(go env GOARCH)/k8s-e2e.test /tmp/build/; \
+    cp /go/src/k8s.io/kubernetes/_output/local/bin/linux/$(go env GOARCH)/ginkgo /tmp/build/; \
+    cp /go/src/k8s.io/kubernetes/openshift-hack/test-kubernetes-e2e.sh /tmp/build/
+
+
+FROM registry.ci.openshift.org/ocp/4.14:tools
+COPY --from=builder /tmp/build/k8s-e2e.test /usr/bin/
+COPY --from=builder /tmp/build/ginkgo /usr/bin/
+COPY --from=builder /tmp/build/test-kubernetes-e2e.sh /usr/bin/
+RUN yum install --setopt=tsflags=nodocs -y git gzip util-linux && yum clean all && rm -rf /var/cache/yum/* && \
+    git config --system user.name test && \
+    git config --system user.email test@test.com && \
+    chmod g+w /etc/passwd
+LABEL io.k8s.display-name="Kubernetes End-to-End Tests" \
+      io.openshift.tags="k8s,tests,e2e"
diff --git a/openshift-hack/images/tests/OWNERS b/openshift-hack/images/tests/OWNERS
new file mode 100644
index 00000000000..e8146784930
--- /dev/null
+++ b/openshift-hack/images/tests/OWNERS
@@ -0,0 +1,5 @@
+reviewers:
+  - smarterclayton
+  - sdodson
+approvers:
+  - smarterclayton
diff --git a/openshift-hack/kubensenter.env b/openshift-hack/kubensenter.env
new file mode 100644
index 00000000000..c37c5bbab28
--- /dev/null
+++ b/openshift-hack/kubensenter.env
@@ -0,0 +1,16 @@
+# Configure which version of kubensenter we need to synchronize
+
+# Define the github repo where we should fetch the kubensenter script
+REPO="github.com/containers/kubensmnt"
+
+# The specific commit or tag of the kubensenter script
+# Note: Should be an explicit tag or commit SHA - Setting to a branch name will cause unexpected verification failures in the future.
+COMMIT=v1.2.0 # (36e5652992df9a3d4abc3d8f02a33c2e364efda9)
+
+# The branch name or tag glob to resolve when 'update-kubensenter.sh --to-latest' is run:
+# - If this resolves to a branch, COMMIT will be set to the latest commit hash on that branch.
+# - If this resolves to a tag name, COMMIT will be set to that tag.
+# - May contain a glob expression such as "v1.1.*" that would match any of the following:
+#     v1.1.0 v1.1.3 v1.1.22-rc1"
+#TARGET="main"
+TARGET="v1.2.*"
diff --git a/openshift-hack/lib/build/binaries.sh b/openshift-hack/lib/build/binaries.sh
new file mode 100644
index 00000000000..e3c71254f37
--- /dev/null
+++ b/openshift-hack/lib/build/binaries.sh
@@ -0,0 +1,457 @@
+#!/usr/bin/env bash
+
+# This library holds utility functions for building
+# and placing Golang binaries for multiple arches.
+
+# os::build::binaries_from_targets take a list of build targets and return the
+# full go package to be built
+function os::build::binaries_from_targets() {
+  local target
+  for target; do
+    if [[ -z "${target}" ]]; then
+      continue
+    fi
+    echo "${OS_GO_PACKAGE}/${target}"
+  done
+}
+readonly -f os::build::binaries_from_targets
+
+# Asks golang what it thinks the host platform is.  The go tool chain does some
+# slightly different things when the target platform matches the host platform.
+function os::build::host_platform() {
+  echo "$(go env GOHOSTOS)/$(go env GOHOSTARCH)"
+}
+readonly -f os::build::host_platform
+
+# Create a user friendly version of host_platform for end users
+function os::build::host_platform_friendly() {
+  local platform=${1:-}
+  if [[ -z "${platform}" ]]; then
+    platform=$(os::build::host_platform)
+  fi
+  if [[ $platform == "windows/amd64" ]]; then
+    echo "windows"
+  elif [[ $platform == "darwin/amd64" ]]; then
+    echo "mac"
+  elif [[ $platform == "linux/386" ]]; then
+    echo "linux-32bit"
+  elif [[ $platform == "linux/amd64" ]]; then
+    echo "linux-64bit"
+  elif [[ $platform == "linux/ppc64le" ]]; then
+    echo "linux-powerpc64"
+  elif [[ $platform == "linux/arm64" ]]; then
+    echo "linux-arm64"
+  elif [[ $platform == "linux/s390x" ]]; then
+    echo "linux-s390"
+  else
+    echo "$(go env GOHOSTOS)-$(go env GOHOSTARCH)"
+  fi
+}
+readonly -f os::build::host_platform_friendly
+
+# This converts from platform/arch to PLATFORM_ARCH, host platform will be
+# considered if no parameter passed
+function os::build::platform_arch() {
+  local platform=${1:-}
+  if [[ -z "${platform}" ]]; then
+    platform=$(os::build::host_platform)
+  fi
+
+  echo "${platform}" | tr '[:lower:]/' '[:upper:]_'
+}
+readonly -f os::build::platform_arch
+
+# os::build::setup_env will check that the `go` commands is available in
+# ${PATH}. If not running on Travis, it will also check that the Go version is
+# good enough for the Kubernetes build.
+#
+# Output Vars:
+#   export GOPATH - A modified GOPATH to our created tree along with extra
+#     stuff.
+#   export GOBIN - This is actively unset if already set as we want binaries
+#     placed in a predictable place.
+function os::build::setup_env() {
+  os::util::ensure::system_binary_exists 'go'
+
+  if [[ -z "$(which sha256sum)" ]]; then
+    sha256sum() {
+      return 0
+    }
+  fi
+
+  # Travis continuous build uses a head go release that doesn't report
+  # a version number, so we skip this check on Travis.  It's unnecessary
+  # there anyway.
+  if [[ "${TRAVIS:-}" != "true" ]]; then
+    os::golang::verify_go_version
+  fi
+  # For any tools that expect this to be set (it is default in golang 1.6),
+  # force vendor experiment.
+  export GO15VENDOREXPERIMENT=1
+
+  unset GOBIN
+
+  # create a local GOPATH in _output
+  GOPATH="${OS_OUTPUT}/go"
+  OS_TARGET_BIN="${OS_OUTPUT}/go/bin"
+  local go_pkg_dir="${GOPATH}/src/${OS_GO_PACKAGE}"
+  local go_pkg_basedir
+  go_pkg_basedir="$(dirname "${go_pkg_dir}")"
+
+  mkdir -p "${go_pkg_basedir}"
+  rm -f "${go_pkg_dir}"
+
+  # TODO: This symlink should be relative.
+  ln -s "${OS_ROOT}" "${go_pkg_dir}"
+
+  # lots of tools "just don't work" unless we're in the GOPATH
+  cd "${go_pkg_dir}" || exit 1
+
+  # Append OS_EXTRA_GOPATH to the GOPATH if it is defined.
+  if [[ -n ${OS_EXTRA_GOPATH:-} ]]; then
+    GOPATH="${GOPATH}:${OS_EXTRA_GOPATH}"
+  fi
+
+  export GOPATH
+  export OS_TARGET_BIN
+}
+readonly -f os::build::setup_env
+
+# Build static binary targets.
+#
+# Input:
+#   $@ - targets and go flags.  If no targets are set then all binaries targets
+#     are built.
+#   OS_BUILD_PLATFORMS - Incoming variable of targets to build for.  If unset
+#     then just the host architecture is built.
+function os::build::build_static_binaries() {
+  CGO_ENABLED=0 os::build::build_binaries -installsuffix=cgo "$@"
+}
+readonly -f os::build::build_static_binaries
+
+# Build binary targets specified
+#
+# Input:
+#   $@ - targets and go flags.  If no targets are set then all binaries targets
+#     are built.
+#   OS_BUILD_PLATFORMS - Incoming variable of targets to build for.  If unset
+#     then just the host architecture is built.
+function os::build::build_binaries() {
+  if [[ $# -eq 0 ]]; then
+    return
+  fi
+  local -a binaries=( "$@" )
+  # Create a sub-shell so that we don't pollute the outer environment
+  ( os::build::internal::build_binaries "${binaries[@]+"${binaries[@]}"}" )
+}
+
+# Build binary targets specified. Should always be run in a sub-shell so we don't leak GOBIN
+#
+# Input:
+#   $@ - targets and go flags.  If no targets are set then all binaries targets
+#     are built.
+#   OS_BUILD_PLATFORMS - Incoming variable of targets to build for.  If unset
+#     then just the host architecture is built.
+os::build::internal::build_binaries() {
+    # Check for `go` binary and set ${GOPATH}.
+    os::build::setup_env
+
+    # Fetch the version.
+    local version_ldflags
+    version_ldflags=$(os::build::ldflags)
+
+    local goflags
+    # Use eval to preserve embedded quoted strings.
+    eval "goflags=(${OS_GOFLAGS:-})"
+    gogcflags="${GOGCFLAGS:-}"
+
+    local arg
+    for arg; do
+      if [[ "${arg}" == -* ]]; then
+        # Assume arguments starting with a dash are flags to pass to go.
+        goflags+=("${arg}")
+      fi
+    done
+
+    os::build::export_targets "$@"
+
+    if [[ ! "${targets[*]:+${targets[*]}}" || ! "${binaries[*]:+${binaries[*]}}" ]]; then
+      return 0
+    fi
+
+    local -a nonstatics=()
+    local -a tests=()
+    for binary in "${binaries[@]-}"; do
+      if [[ "${binary}" =~ ".test"$ ]]; then
+        tests+=("$binary")
+      else
+        nonstatics+=("$binary")
+      fi
+    done
+
+    local pkgdir="${OS_OUTPUT_PKGDIR}"
+    if [[ "${CGO_ENABLED-}" == "0" ]]; then
+      pkgdir+="/static"
+    fi
+
+    local host_platform
+    host_platform=$(os::build::host_platform)
+    local platform
+    for platform in "${platforms[@]+"${platforms[@]}"}"; do
+      echo "++ Building go targets for ${platform}:" "${targets[@]}"
+      mkdir -p "${OS_OUTPUT_BINPATH}/${platform}"
+
+      # output directly to the desired location
+      if [[ "$platform" == "$host_platform" ]]; then
+        export GOBIN="${OS_OUTPUT_BINPATH}/${platform}"
+      else
+        unset GOBIN
+      fi
+
+      local platform_gotags_envvar
+      platform_gotags_envvar=OS_GOFLAGS_TAGS_$(os::build::platform_arch "${platform}")
+      local platform_gotags_test_envvar
+      platform_gotags_test_envvar=OS_GOFLAGS_TAGS_TEST_$(os::build::platform_arch "${platform}")
+
+      # work around https://github.com/golang/go/issues/11887
+      local local_ldflags="${version_ldflags}"
+      if [[ "${platform}" == "darwin/amd64" ]]; then
+        local_ldflags+=" -s"
+      fi
+
+      #Add Windows File Properties/Version Info and Icon Resource for oc.exe
+      if [[ "$platform" == "windows/amd64" ]]; then
+        os::build::generate_windows_versioninfo
+      fi
+
+      if [[ ${#nonstatics[@]} -gt 0 ]]; then
+        GOOS=${platform%/*} GOARCH=${platform##*/} go install \
+          -tags "${OS_GOFLAGS_TAGS-} ${!platform_gotags_envvar:-}" \
+          -ldflags="${local_ldflags}" \
+          "${goflags[@]:+${goflags[@]}}" \
+          -gcflags "${gogcflags}" \
+          "${nonstatics[@]}"
+
+        # GOBIN is not supported on cross-compile in Go 1.5+ - move to the correct target
+        if [[ "$platform" != "$host_platform" ]]; then
+          local platform_src="/${platform//\//_}"
+          mv "${OS_TARGET_BIN}/${platform_src}/"* "${OS_OUTPUT_BINPATH}/${platform}/"
+        fi
+      fi
+
+      if [[ "$platform" == "windows/amd64" ]]; then
+        os::build::clean_windows_versioninfo
+      fi
+
+      for test in "${tests[@]:+${tests[@]}}"; do
+        local outfile
+        outfile="${OS_OUTPUT_BINPATH}/${platform}/$(basename "${test}")"
+        # disabling cgo allows use of delve
+        CGO_ENABLED="${OS_TEST_CGO_ENABLED:-}" GOOS=${platform%/*} GOARCH=${platform##*/} go test \
+          -tags "${OS_GOFLAGS_TAGS-} ${!platform_gotags_test_envvar:-}" \
+          -ldflags "${local_ldflags}" \
+          -i -c -o "${outfile}" \
+          "${goflags[@]:+${goflags[@]}}" \
+          "$(dirname "${test}")"
+      done
+    done
+
+    os::build::check_binaries
+}
+readonly -f os::build::build_binaries
+
+ # Generates the set of target packages, binaries, and platforms to build for.
+# Accepts binaries via $@, and platforms via OS_BUILD_PLATFORMS, or defaults to
+# the current platform.
+function os::build::export_targets() {
+  platforms=("${OS_BUILD_PLATFORMS[@]:+${OS_BUILD_PLATFORMS[@]}}")
+
+  targets=()
+  local arg
+  for arg; do
+    if [[ "${arg}" != -* ]]; then
+      targets+=("${arg}")
+    fi
+  done
+
+  binaries=($(os::build::binaries_from_targets "${targets[@]-}"))
+}
+readonly -f os::build::export_targets
+
+# This will take $@ from $GOPATH/bin and copy them to the appropriate
+# place in ${OS_OUTPUT_BINDIR}
+#
+# If OS_RELEASE_ARCHIVE is set, tar archives prefixed with OS_RELEASE_ARCHIVE for
+# each of OS_BUILD_PLATFORMS are created.
+#
+# Ideally this wouldn't be necessary and we could just set GOBIN to
+# OS_OUTPUT_BINDIR but that won't work in the face of cross compilation.  'go
+# install' will place binaries that match the host platform directly in $GOBIN
+# while placing cross compiled binaries into `platform_arch` subdirs.  This
+# complicates pretty much everything else we do around packaging and such.
+function os::build::place_bins() {
+  (
+    local host_platform
+    host_platform=$(os::build::host_platform)
+
+    if [[ "${OS_RELEASE_ARCHIVE-}" != "" ]]; then
+      os::build::version::get_vars
+      mkdir -p "${OS_OUTPUT_RELEASEPATH}"
+    fi
+
+    os::build::export_targets "$@"
+    for platform in "${platforms[@]+"${platforms[@]}"}"; do
+      # The substitution on platform_src below will replace all slashes with
+      # underscores.  It'll transform darwin/amd64 -> darwin_amd64.
+      local platform_src="/${platform//\//_}"
+
+      # Skip this directory if the platform has no binaries.
+      if [[ ! -d "${OS_OUTPUT_BINPATH}/${platform}" ]]; then
+        continue
+      fi
+
+      # Create an array of binaries to release. Append .exe variants if the platform is windows.
+      local -a binaries=()
+      for binary in "${targets[@]}"; do
+        binary=$(basename "$binary")
+        if [[ $platform == "windows/amd64" ]]; then
+          binaries+=("${binary}.exe")
+        else
+          binaries+=("${binary}")
+        fi
+      done
+
+      # If no release archive was requested, we're done.
+      if [[ "${OS_RELEASE_ARCHIVE-}" == "" ]]; then
+        continue
+      fi
+
+      # Create a temporary bin directory containing only the binaries marked for release.
+      local release_binpath
+      release_binpath=$(mktemp -d "openshift.release.${OS_RELEASE_ARCHIVE}.XXX")
+      for binary in "${binaries[@]}"; do
+        cp "${OS_OUTPUT_BINPATH}/${platform}/${binary}" "${release_binpath}/"
+      done
+
+      # Create the release archive.
+      platform="$( os::build::host_platform_friendly "${platform}" )"
+      if [[ ${OS_RELEASE_ARCHIVE} == "openshift-origin" ]]; then
+        for file in "${OS_BINARY_RELEASE_CLIENT_EXTRA[@]}"; do
+          cp "${file}" "${release_binpath}/"
+        done
+        if [[ $platform == "linux-64bit" ]]; then
+          OS_RELEASE_ARCHIVE="openshift-origin-server" os::build::archive::tar "${OS_BINARY_RELEASE_SERVER_LINUX[@]}"
+        elif [[ $platform == "linux-powerpc64" ]]; then
+          OS_RELEASE_ARCHIVE="openshift-origin-server" os::build::archive::tar "${OS_BINARY_RELEASE_SERVER_LINUX[@]}"
+        elif [[ $platform == "linux-arm64" ]]; then
+          OS_RELEASE_ARCHIVE="openshift-origin-server" os::build::archive::tar "${OS_BINARY_RELEASE_SERVER_LINUX[@]}"
+        elif [[ $platform == "linux-s390" ]]; then
+          OS_RELEASE_ARCHIVE="openshift-origin-server" os::build::archive::tar "${OS_BINARY_RELEASE_SERVER_LINUX[@]}"
+        else
+          echo "++ ERROR: No release type defined for $platform"
+        fi
+      else
+        if [[ $platform == "linux-64bit" || $platform == "linux-powerpc64" || $platform == "linux-arm64" || $platform == "linux-s390" ]]; then
+          os::build::archive::tar "./*"
+        else
+          echo "++ ERROR: No release type defined for $platform"
+        fi
+      fi
+      rm -rf "${release_binpath}"
+    done
+  )
+}
+readonly -f os::build::place_bins
+
+# os::build::release_sha calculates a SHA256 checksum over the contents of the
+# built release directory.
+function os::build::release_sha() {
+  pushd "${OS_OUTPUT_RELEASEPATH}" &> /dev/null || exit 1
+  find . -maxdepth 1 -type f | xargs sha256sum > CHECKSUM
+  popd &> /dev/null || exit 1
+}
+readonly -f os::build::release_sha
+
+# os::build::make_openshift_binary_symlinks makes symlinks for the openshift
+# binary in _output/local/bin/${platform}
+function os::build::make_openshift_binary_symlinks() {
+  platform=$(os::build::host_platform)
+}
+readonly -f os::build::make_openshift_binary_symlinks
+
+# DEPRECATED: will be removed
+function os::build::ldflag() {
+  local key=${1}
+  local val=${2}
+
+  echo "-X ${key}=${val}"
+}
+readonly -f os::build::ldflag
+
+# os::build::require_clean_tree exits if the current Git tree is not clean.
+function os::build::require_clean_tree() {
+  if ! git diff-index --quiet HEAD -- || test "$(git ls-files --exclude-standard --others | wc -l)" != 0; then
+    echo "You can't have any staged or dirty files in $(pwd) for this command."
+    echo "Either commit them or unstage them to continue."
+    exit 1
+  fi
+}
+readonly -f os::build::require_clean_tree
+
+# os::build::commit_range takes one or two arguments - if the first argument is an
+# integer, it is assumed to be a pull request and the local origin/pr/# branch is
+# used to determine the common range with the second argument. If the first argument
+# is not an integer, it is assumed to be a Git commit range and output directly.
+function os::build::commit_range() {
+  local remote
+  remote="${UPSTREAM_REMOTE:-origin}"
+  if [[ "$1" =~ ^-?[0-9]+$ ]]; then
+    local target
+    target="$(git rev-parse "${remote}/pr/$1")"
+    if [[ $? -ne 0 ]]; then
+      echo "Branch does not exist, or you have not configured ${remote}/pr/* style branches from GitHub" 1>&2
+      exit 1
+    fi
+
+    local base
+    base="$(git merge-base "${target}" "$2")"
+    if [[ $? -ne 0 ]]; then
+      echo "Branch has no common commits with $2" 1>&2
+      exit 1
+    fi
+    if [[ "${base}" == "${target}" ]]; then
+
+      # DO NOT TRUST THIS CODE
+      merged="$(git rev-list --reverse "${target}".."$2" --ancestry-path | head -1)"
+      if [[ -z "${merged}" ]]; then
+        echo "Unable to find the commit that merged ${remote}/pr/$1" 1>&2
+        exit 1
+      fi
+      #if [[ $? -ne 0 ]]; then
+      #  echo "Unable to find the merge commit for $1: ${merged}" 1>&2
+      #  exit 1
+      #fi
+      echo "++ pr/$1 appears to have merged at ${merged}" 1>&2
+      leftparent="$(git rev-list --parents -n 1 "${merged}" | cut -f2 -d ' ')"
+      if [[ $? -ne 0 ]]; then
+        echo "Unable to find the left-parent for the merge of for $1" 1>&2
+        exit 1
+      fi
+      base="$(git merge-base "${target}" "${leftparent}")"
+      if [[ $? -ne 0 ]]; then
+        echo "Unable to find the common commit between ${leftparent} and $1" 1>&2
+        exit 1
+      fi
+      echo "${base}..${target}"
+      exit 0
+      #echo "Branch has already been merged to upstream master, use explicit range instead" 1>&2
+      #exit 1
+    fi
+
+    echo "${base}...${target}"
+    exit 0
+  fi
+
+  echo "$1"
+}
+readonly -f os::build::commit_range
diff --git a/openshift-hack/lib/build/rpm.sh b/openshift-hack/lib/build/rpm.sh
new file mode 100644
index 00000000000..275602de6f0
--- /dev/null
+++ b/openshift-hack/lib/build/rpm.sh
@@ -0,0 +1,95 @@
+#!/usr/bin/env bash
+
+# This library holds utilities for building RPMs from Origin.
+
+# os::build::rpm::generate_nevra_vars determines the NEVRA of the RPMs
+# that would be built from the current git state.
+#
+# Globals:
+#  - OS_GIT_VERSION
+# Arguments:
+#  - None
+# Exports:
+#  - OS_RPM_VERSION
+#  - OS_RPM_RELEASE
+#  - OS_RPM_ARCHITECTURE
+function os::build::rpm::get_nvra_vars() {
+	# the package name can be overwritten but is normally 'origin'
+	OS_RPM_ARCHITECTURE="$(uname -i)"
+
+	# we can extract the package version from the build version
+	os::build::version::get_vars
+	if [[ "${OS_GIT_VERSION}" =~ ^v([0-9](\.[0-9]+)*)(.*) ]]; then
+		OS_RPM_VERSION="${BASH_REMATCH[1]}"
+		metadata="${BASH_REMATCH[3]}"
+	else
+		os::log::fatal "Malformed \$OS_GIT_VERSION: ${OS_GIT_VERSION}"
+	fi
+
+	# we can generate the package release from the git version metadata
+	# OS_GIT_VERSION will always have metadata, but either contain
+	# pre-release information _and_ build metadata, or only the latter.
+	# Build metadata may or may not contain the number of commits past
+	# the last tag. If no commit number exists, we are on a tag and use 0.
+	# ex.
+	#    -alpha.0+shasums-123-dirty
+	#    -alpha.0+shasums-123
+	#    -alpha.0+shasums-dirty
+	#    -alpha.0+shasums
+	#    +shasums-123-dirty
+	#    +shasums-123
+	#    +shasums-dirty
+	#    +shasums
+	if [[ "${metadata:0:1}" == "+" ]]; then
+		# we only have build metadata, but need to massage it so
+		# we can generate a valid RPM release from it
+		if [[ "${metadata}" =~ ^\+([a-z0-9]{7,40})(-([0-9]+))?(-dirty)?$ ]]; then
+			build_sha="${BASH_REMATCH[1]}"
+			build_num="${BASH_REMATCH[3]:-0}"
+		else
+			os::log::fatal "Malformed git version metadata: ${metadata}"
+		fi
+		OS_RPM_RELEASE="1.${build_num}.${build_sha}"
+	elif [[ "${metadata:0:1}" == "-" ]]; then
+		# we have both build metadata and pre-release info
+		if [[ "${metadata}" =~ ^-([^\+]+)\+([a-z0-9]{7,40})(-([0-9]+))?(-dirty)?$ ]]; then
+			pre_release="${BASH_REMATCH[1]}"
+			build_sha="${BASH_REMATCH[2]}"
+			build_num="${BASH_REMATCH[4]:-0}"
+		else
+			os::log::fatal "Malformed git version metadata: ${metadata}"
+		fi
+		OS_RPM_RELEASE="0.${pre_release}.${build_num}.${build_sha}"
+	else
+		os::log::fatal "Malformed git version metadata: ${metadata}"
+	fi
+
+	OS_RPM_GIT_VARS=$( os::build::version::save_vars | tr '\n' ' ' )
+
+	export OS_RPM_VERSION OS_RPM_RELEASE OS_RPM_ARCHITECTURE OS_RPM_GIT_VARS
+}
+
+
+# os::build::rpm::format_nvra formats the rpm NVRA vars generated by
+# os::build::rpm::get_nvra_vars and will generate them if necessary
+#
+# Globals:
+#  - OS_RPM_NAME
+#  - OS_RPM_VERSION
+#  - OS_RPM_RELEASE
+#  - OS_RPM_ARCHITECTURE
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::build::rpm::format_nvra() {
+	if [[ -z "${OS_RPM_VERSION:-}" || -z "${OS_RPM_RELEASE:-}" ]]; then
+		os::build::rpm::get_nvra_vars
+	fi
+	if [[ -z "${OS_RPM_NAME-}" ]]; then
+		OS_RPM_SPECFILE="$( find "${OS_ROOT}" -name '*.spec' )"
+		OS_RPM_NAME="$( rpmspec -q --qf '%{name}\n' "${OS_RPM_SPECFILE}" | head -1 )"
+	fi
+
+	echo "${OS_RPM_NAME}-${OS_RPM_VERSION}-${OS_RPM_RELEASE}.${OS_RPM_ARCHITECTURE}"
+}
diff --git a/openshift-hack/lib/build/version.sh b/openshift-hack/lib/build/version.sh
new file mode 100644
index 00000000000..ea52257486f
--- /dev/null
+++ b/openshift-hack/lib/build/version.sh
@@ -0,0 +1,88 @@
+#!/usr/bin/env bash
+
+# This library holds utility functions for determining
+# product versions from Git repository state.
+
+# os::build::version::get_vars loads the standard version variables as
+# ENV vars
+function os::build::version::get_vars() {
+	if [[ -n "${OS_VERSION_FILE-}" ]]; then
+		if [[ -f "${OS_VERSION_FILE}" ]]; then
+			source "${OS_VERSION_FILE}"
+			return
+		fi
+		if [[ ! -d "${OS_ROOT}/.git" ]]; then
+			os::log::fatal "No version file at ${OS_VERSION_FILE}"
+		fi
+		os::log::warning "No version file at ${OS_VERSION_FILE}, falling back to git versions"
+	fi
+	os::build::version::git_vars
+}
+readonly -f os::build::version::get_vars
+
+# os::build::version::git_vars looks up the current Git vars if they have not been calculated.
+function os::build::version::git_vars() {
+	if [[ -n "${OS_GIT_VERSION-}" ]]; then
+		return 0
+ 	fi
+
+	local git=(git --work-tree "${OS_ROOT}")
+
+	if [[ -n ${OS_GIT_COMMIT-} ]] || OS_GIT_COMMIT=$("${git[@]}" rev-parse --short "HEAD^{commit}" 2>/dev/null); then
+		if [[ -z ${OS_GIT_TREE_STATE-} ]]; then
+			# Check if the tree is dirty.  default to dirty
+			if git_status=$("${git[@]}" status --porcelain 2>/dev/null) && [[ -z ${git_status} ]]; then
+				OS_GIT_TREE_STATE="clean"
+			else
+				OS_GIT_TREE_STATE="dirty"
+			fi
+		fi
+		# Use git describe to find the version based on annotated tags.
+		if [[ -n ${OS_GIT_VERSION-} ]] || OS_GIT_VERSION=$(sed -rn 's/.*io.openshift.build.versions="kubernetes=(1.[0-9]+.[0-9]+(-rc.[0-9])?)"/v\1/p' openshift-hack/images/hyperkube/Dockerfile.rhel); then
+			# combine GIT_COMMIT with GIT_VERSION which is being read from the above Dockerfile
+			OS_GIT_VERSION+="+${OS_GIT_COMMIT:0:7}"
+			# Try to match the "git describe" output to a regex to try to extract
+			# the "major" and "minor" versions and whether this is the exact tagged
+			# version or whether the tree is between two tagged versions.
+			if [[ "${OS_GIT_VERSION}" =~ ^v([0-9]+)\.([0-9]+)\.([0-9]+)(\.[0-9]+)*([-].*)?$ ]]; then
+				OS_GIT_MAJOR=${BASH_REMATCH[1]}
+				OS_GIT_MINOR=${BASH_REMATCH[2]}
+				OS_GIT_PATCH=${BASH_REMATCH[3]}
+			fi
+
+			if [[ "${OS_GIT_TREE_STATE}" == "dirty" ]]; then
+				# git describe --dirty only considers changes to existing files, but
+				# that is problematic since new untracked .go files affect the build,
+				# so use our idea of "dirty" from git status instead.
+				OS_GIT_VERSION+="-dirty"
+			fi
+		fi
+	fi
+
+}
+readonly -f os::build::version::git_vars
+
+# Saves the environment flags to $1
+function os::build::version::save_vars() {
+    # Set the kube vars to the os vars to ensure correct versioning
+    # when using rpmbuild. This is necessary to ensure the kube build
+    # tooling correctly sets the version of binaries when building
+    # from source.
+	cat <<EOF
+OS_GIT_COMMIT='${OS_GIT_COMMIT-}'
+OS_GIT_TREE_STATE='${OS_GIT_TREE_STATE-}'
+OS_GIT_VERSION='${OS_GIT_VERSION-}'
+OS_GIT_MAJOR='${OS_GIT_MAJOR-}'
+OS_GIT_MINOR='${OS_GIT_MINOR-}'
+OS_GIT_PATCH='${OS_GIT_PATCH-}'
+KUBE_GIT_MAJOR='${OS_GIT_MAJOR-}'
+KUBE_GIT_MINOR='${OS_GIT_MINOR-}'
+KUBE_GIT_COMMIT='${OS_GIT_COMMIT-}'
+KUBE_GIT_TREE_STATE='${OS_GIT_TREE_STATE-}'
+KUBE_GIT_VERSION='${OS_GIT_VERSION-}'
+KUBE_GIT_VERSION='${OS_GIT_VERSION-}'
+ETCD_GIT_VERSION='${ETCD_GIT_VERSION-}'
+ETCD_GIT_COMMIT='${ETCD_GIT_COMMIT-}'
+EOF
+}
+readonly -f os::build::version::save_vars
diff --git a/openshift-hack/lib/cmd.sh b/openshift-hack/lib/cmd.sh
new file mode 100644
index 00000000000..5034b55a544
--- /dev/null
+++ b/openshift-hack/lib/cmd.sh
@@ -0,0 +1,645 @@
+#!/usr/bin/env bash
+# This utility file contains functions that wrap commands to be tested. All wrapper functions run commands
+# in a sub-shell and redirect all output. Tests in test-cmd *must* use these functions for testing.
+
+# expect_success runs the cmd and expects an exit code of 0
+function os::cmd::expect_success() {
+	if [[ $# -ne 1 ]]; then echo "os::cmd::expect_success expects only one argument, got $#"; return 1; fi
+	local cmd=$1
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}"
+}
+readonly -f os::cmd::expect_success
+
+# expect_failure runs the cmd and expects a non-zero exit code
+function os::cmd::expect_failure() {
+	if [[ $# -ne 1 ]]; then echo "os::cmd::expect_failure expects only one argument, got $#"; return 1; fi
+	local cmd=$1
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::failure_func"
+}
+readonly -f os::cmd::expect_failure
+
+# expect_success_and_text runs the cmd and expects an exit code of 0
+# as well as running a grep test to find the given string in the output
+function os::cmd::expect_success_and_text() {
+	if [[ $# -ne 2 ]]; then echo "os::cmd::expect_success_and_text expects two arguments, got $#"; return 1; fi
+	local cmd=$1
+	local expected_text=$2
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::success_func" "${expected_text}"
+}
+readonly -f os::cmd::expect_success_and_text
+
+# expect_failure_and_text runs the cmd and expects a non-zero exit code
+# as well as running a grep test to find the given string in the output
+function os::cmd::expect_failure_and_text() {
+	if [[ $# -ne 2 ]]; then echo "os::cmd::expect_failure_and_text expects two arguments, got $#"; return 1; fi
+	local cmd=$1
+	local expected_text=$2
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::failure_func" "${expected_text}"
+}
+readonly -f os::cmd::expect_failure_and_text
+
+# expect_success_and_not_text runs the cmd and expects an exit code of 0
+# as well as running a grep test to ensure the given string is not in the output
+function os::cmd::expect_success_and_not_text() {
+	if [[ $# -ne 2 ]]; then echo "os::cmd::expect_success_and_not_text expects two arguments, got $#"; return 1; fi
+	local cmd=$1
+	local expected_text=$2
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::success_func" "${expected_text}" "os::cmd::internal::failure_func"
+}
+readonly -f os::cmd::expect_success_and_not_text
+
+# expect_failure_and_not_text runs the cmd and expects a non-zero exit code
+# as well as running a grep test to ensure the given string is not in the output
+function os::cmd::expect_failure_and_not_text() {
+	if [[ $# -ne 2 ]]; then echo "os::cmd::expect_failure_and_not_text expects two arguments, got $#"; return 1; fi
+	local cmd=$1
+	local expected_text=$2
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::failure_func" "${expected_text}" "os::cmd::internal::failure_func"
+}
+readonly -f os::cmd::expect_failure_and_not_text
+
+# expect_code runs the cmd and expects a given exit code
+function os::cmd::expect_code() {
+	if [[ $# -ne 2 ]]; then echo "os::cmd::expect_code expects two arguments, got $#"; return 1; fi
+	local cmd=$1
+	local expected_cmd_code=$2
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::specific_code_func ${expected_cmd_code}"
+}
+readonly -f os::cmd::expect_code
+
+# expect_code_and_text runs the cmd and expects the given exit code
+# as well as running a grep test to find the given string in the output
+function os::cmd::expect_code_and_text() {
+	if [[ $# -ne 3 ]]; then echo "os::cmd::expect_code_and_text expects three arguments, got $#"; return 1; fi
+	local cmd=$1
+	local expected_cmd_code=$2
+	local expected_text=$3
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::specific_code_func ${expected_cmd_code}" "${expected_text}"
+}
+readonly -f os::cmd::expect_code_and_text
+
+# expect_code_and_not_text runs the cmd and expects the given exit code
+# as well as running a grep test to ensure the given string is not in the output
+function os::cmd::expect_code_and_not_text() {
+	if [[ $# -ne 3 ]]; then echo "os::cmd::expect_code_and_not_text expects three arguments, got $#"; return 1; fi
+	local cmd=$1
+	local expected_cmd_code=$2
+	local expected_text=$3
+
+	os::cmd::internal::expect_exit_code_run_grep "${cmd}" "os::cmd::internal::specific_code_func ${expected_cmd_code}" "${expected_text}" "os::cmd::internal::failure_func"
+}
+readonly -f os::cmd::expect_code_and_not_text
+
+millisecond=1
+second=$(( 1000 * millisecond ))
+minute=$(( 60 * second ))
+
+# os::cmd::try_until_success runs the cmd in a small interval until either the command succeeds or times out
+# the default time-out for os::cmd::try_until_success is 60 seconds.
+# the default interval for os::cmd::try_until_success is 200ms
+function os::cmd::try_until_success() {
+	if [[ $# -lt 1 ]]; then echo "os::cmd::try_until_success expects at least one arguments, got $#"; return 1; fi
+	local cmd=$1
+	local duration=${2:-$minute}
+	local interval=${3:-0.2}
+
+	os::cmd::internal::run_until_exit_code "${cmd}" "os::cmd::internal::success_func" "${duration}" "${interval}"
+}
+readonly -f os::cmd::try_until_success
+
+# os::cmd::try_until_failure runs the cmd until either the command fails or times out
+# the default time-out for os::cmd::try_until_failure is 60 seconds.
+function os::cmd::try_until_failure() {
+	if [[ $# -lt 1 ]]; then echo "os::cmd::try_until_failure expects at least one argument, got $#"; return 1; fi
+	local cmd=$1
+	local duration=${2:-$minute}
+	local interval=${3:-0.2}
+
+	os::cmd::internal::run_until_exit_code "${cmd}" "os::cmd::internal::failure_func" "${duration}" "${interval}"
+}
+readonly -f os::cmd::try_until_failure
+
+# os::cmd::try_until_text runs the cmd until either the command outputs the desired text or times out
+# the default time-out for os::cmd::try_until_text is 60 seconds.
+function os::cmd::try_until_text() {
+	if [[ $# -lt 2 ]]; then echo "os::cmd::try_until_text expects at least two arguments, got $#"; return 1; fi
+	local cmd=$1
+	local text=$2
+	local duration=${3:-$minute}
+	local interval=${4:-0.2}
+
+	os::cmd::internal::run_until_text "${cmd}" "${text}" "os::cmd::internal::success_func" "${duration}" "${interval}"
+}
+readonly -f os::cmd::try_until_text
+
+# os::cmd::try_until_not_text runs the cmd until either the command doesnot output the text or times out
+# the default time-out for os::cmd::try_until_not_text is 60 seconds.
+function os::cmd::try_until_not_text() {
+	if [[ $# -lt 2 ]]; then echo "os::cmd::try_until_not_text expects at least two arguments, got $#"; return 1; fi
+	local cmd=$1
+	local text=$2
+	local duration=${3:-$minute}
+	local interval=${4:-0.2}
+
+	os::cmd::internal::run_until_text "${cmd}" "${text}" "os::cmd::internal::failure_func" "${duration}" "${interval}"
+}
+readonly -f os::cmd::try_until_text
+
+# Functions in the os::cmd::internal namespace are discouraged from being used outside of os::cmd
+
+# In order to harvest stderr and stdout at the same time into different buckets, we need to stick them into files
+# in an intermediate step
+os_cmd_internal_tmpdir="${TMPDIR:-"/tmp"}/cmd"
+os_cmd_internal_tmpout="${os_cmd_internal_tmpdir}/tmp_stdout.log"
+os_cmd_internal_tmperr="${os_cmd_internal_tmpdir}/tmp_stderr.log"
+
+# os::cmd::internal::expect_exit_code_run_grep runs the provided test command and expects a specific
+# exit code from that command as well as the success of a specified `grep` invocation. Output from the
+# command to be tested is suppressed unless either `VERBOSE=1` or the test fails. This function bypasses
+# any error exiting settings or traps set by upstream callers by masking the return code of the command
+# with the return code of setting the result variable on failure.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - VERBOSE
+# Arguments:
+#  - 1: the command to run
+#  - 2: command evaluation assertion to use
+#  - 3: text to test for
+#  - 4: text assertion to use
+# Returns:
+#  - 0: if all assertions met
+#  - 1: if any assertions fail
+function os::cmd::internal::expect_exit_code_run_grep() {
+	local cmd=$1
+	# default expected cmd code to 0 for success
+	local cmd_eval_func=${2:-os::cmd::internal::success_func}
+	# default to nothing
+	local grep_args=${3:-}
+	# default expected test code to 0 for success
+	local test_eval_func=${4:-os::cmd::internal::success_func}
+
+	local -a junit_log
+
+	os::cmd::internal::init_tempdir
+	os::test::junit::declare_test_start
+
+	local name
+    name=$(os::cmd::internal::describe_call "${cmd}" "${cmd_eval_func}" "${grep_args}" "${test_eval_func}")
+	local preamble="Running ${name}..."
+	echo "${preamble}"
+	# for ease of parsing, we want the entire declaration on one line, so we replace '\n' with ';'
+	junit_log+=( "${name//$'\n'/;}" )
+
+	local start_time
+    start_time=$(os::cmd::internal::seconds_since_epoch)
+
+	local cmd_result
+    cmd_result=$( os::cmd::internal::run_collecting_output "${cmd}"; echo $? )
+	local cmd_succeeded
+    cmd_succeeded=$( ${cmd_eval_func} "${cmd_result}"; echo $? )
+
+	local test_result=0
+	if [[ -n "${grep_args}" ]]; then
+		test_result=$( os::cmd::internal::run_collecting_output 'grep -Eq "'"${grep_args}"'" <(os::cmd::internal::get_results)'; echo $? )
+	fi
+	local test_succeeded
+    test_succeeded=$( ${test_eval_func} "${test_result}"; echo $? )
+
+	local end_time
+    end_time=$(os::cmd::internal::seconds_since_epoch)
+	local time_elapsed
+    time_elapsed=$(echo "scale=3; ${end_time} - ${start_time}" | bc | xargs printf '%5.3f') # in decimal seconds, we need leading zeroes for parsing later
+
+	# clear the preamble so we can print out the success or error message
+	os::text::clear_string "${preamble}"
+
+	local return_code
+	if (( cmd_succeeded && test_succeeded )); then
+		os::text::print_green "SUCCESS after ${time_elapsed}s: ${name}"
+		junit_log+=( "SUCCESS after ${time_elapsed}s: ${name//$'\n'/;}" )
+
+		if [[ -n ${VERBOSE-} ]]; then
+			os::cmd::internal::print_results
+		fi
+		return_code=0
+	else
+	    local cause
+        cause=$(os::cmd::internal::assemble_causes "${cmd_succeeded}" "${test_succeeded}")
+
+		os::text::print_red_bold "FAILURE after ${time_elapsed}s: ${name}: ${cause}"
+		junit_log+=( "FAILURE after ${time_elapsed}s: ${name//$'\n'/;}: ${cause}" )
+
+		os::text::print_red "$(os::cmd::internal::print_results)"
+		return_code=1
+	fi
+
+	junit_log+=( "$(os::cmd::internal::print_results)" )
+	# append inside of a subshell so that IFS doesn't get propagated out
+	( IFS=$'\n'; echo "${junit_log[*]}" >> "${JUNIT_REPORT_OUTPUT:-/dev/null}" )
+	os::test::junit::declare_test_end
+	return "${return_code}"
+}
+readonly -f os::cmd::internal::expect_exit_code_run_grep
+
+# os::cmd::internal::init_tempdir initializes the temporary directory
+function os::cmd::internal::init_tempdir() {
+	mkdir -p "${os_cmd_internal_tmpdir}"
+	rm -f "${os_cmd_internal_tmpdir}"/tmp_std{out,err}.log
+}
+readonly -f os::cmd::internal::init_tempdir
+
+# os::cmd::internal::describe_call determines the file:line of the latest function call made
+# from outside of this file in the call stack, and the name of the function being called from
+# that line, returning a string describing the call
+function os::cmd::internal::describe_call() {
+	local cmd=$1
+	local cmd_eval_func=$2
+	local grep_args=${3:-}
+	local test_eval_func=${4:-}
+
+	local caller_id
+    caller_id=$(os::cmd::internal::determine_caller)
+	local full_name="${caller_id}: executing '${cmd}'"
+
+	local cmd_expectation
+    cmd_expectation=$(os::cmd::internal::describe_expectation "${cmd_eval_func}")
+	local full_name="${full_name} expecting ${cmd_expectation}"
+
+	if [[ -n "${grep_args}" ]]; then
+		local text_expecting=
+		case "${test_eval_func}" in
+		"os::cmd::internal::success_func")
+			text_expecting="text" ;;
+		"os::cmd::internal::failure_func")
+			text_expecting="not text" ;;
+		esac
+		full_name="${full_name} and ${text_expecting} '${grep_args}'"
+	fi
+
+	echo "${full_name}"
+}
+readonly -f os::cmd::internal::describe_call
+
+# os::cmd::internal::determine_caller determines the file relative to the OpenShift Origin root directory
+# and line number of the function call to the outer os::cmd wrapper function
+function os::cmd::internal::determine_caller() {
+	local call_depth=
+	local len_sources="${#BASH_SOURCE[@]}"
+	for (( i=0; i<len_sources; i++ )); do
+		if grep -q "hack/lib/cmd\.sh$" "${BASH_SOURCE[i]}"; then
+			call_depth=i
+			break
+		fi
+	done
+
+	local caller_file="${BASH_SOURCE[${call_depth}]}"
+    caller_file="$( os::util::repository_relative_path "${caller_file}" )"
+	local caller_line="${BASH_LINENO[${call_depth}-1]}"
+	echo "${caller_file}:${caller_line}"
+}
+readonly -f os::cmd::internal::determine_caller
+
+# os::cmd::internal::describe_expectation describes a command return code evaluation function
+function os::cmd::internal::describe_expectation() {
+	local func=$1
+	case "${func}" in
+	"os::cmd::internal::success_func")
+		echo "success" ;;
+	"os::cmd::internal::failure_func")
+		echo "failure" ;;
+	"os::cmd::internal::specific_code_func"*[0-9])
+	    local code
+        code=$(echo "${func}" | grep -Eo "[0-9]+$")
+		echo "exit code ${code}" ;;
+	"")
+		echo "any result"
+	esac
+}
+readonly -f os::cmd::internal::describe_expectation
+
+# os::cmd::internal::seconds_since_epoch returns the number of seconds elapsed since the epoch
+# with milli-second precision
+function os::cmd::internal::seconds_since_epoch() {
+    local ns
+    ns=$(date +%s%N)
+	# if `date` doesn't support nanoseconds, return second precision
+	if [[ "$ns" == *N ]]; then
+		date "+%s.000"
+		return
+	fi
+	bc <<< "scale=3; ${ns}/1000000000"
+}
+readonly -f os::cmd::internal::seconds_since_epoch
+
+# os::cmd::internal::run_collecting_output runs the command given, piping stdout and stderr into
+# the given files, and returning the exit code of the command
+function os::cmd::internal::run_collecting_output() {
+	local cmd=$1
+
+	local result=
+	eval "${cmd}" 1>>"${os_cmd_internal_tmpout}" 2>>"${os_cmd_internal_tmperr}" || result=$?
+	local result=${result:-0} # if we haven't set result yet, the command succeeded
+
+	return "${result}"
+}
+readonly -f os::cmd::internal::run_collecting_output
+
+# os::cmd::internal::success_func determines if the input exit code denotes success
+# this function returns 0 for false and 1 for true to be compatible with arithmetic tests
+function os::cmd::internal::success_func() {
+	local exit_code=$1
+
+	# use a negated test to get output correct for (( ))
+	[[ "${exit_code}" -ne "0" ]]
+	return $?
+}
+readonly -f os::cmd::internal::success_func
+
+# os::cmd::internal::failure_func determines if the input exit code denotes failure
+# this function returns 0 for false and 1 for true to be compatible with arithmetic tests
+function os::cmd::internal::failure_func() {
+	local exit_code=$1
+
+	# use a negated test to get output correct for (( ))
+	[[ "${exit_code}" -eq "0" ]]
+	return $?
+}
+readonly -f os::cmd::internal::failure_func
+
+# os::cmd::internal::specific_code_func determines if the input exit code matches the given code
+# this function returns 0 for false and 1 for true to be compatible with arithmetic tests
+function os::cmd::internal::specific_code_func() {
+	local expected_code=$1
+	local exit_code=$2
+
+	# use a negated test to get output correct for (( ))
+	[[ "${exit_code}" -ne "${expected_code}" ]]
+	return $?
+}
+readonly -f os::cmd::internal::specific_code_func
+
+# os::cmd::internal::get_results prints the stderr and stdout files
+function os::cmd::internal::get_results() {
+	cat "${os_cmd_internal_tmpout}" "${os_cmd_internal_tmperr}"
+}
+readonly -f os::cmd::internal::get_results
+
+# os::cmd::internal::get_last_results prints the stderr and stdout from the last attempt
+function os::cmd::internal::get_last_results() {
+	awk 'BEGIN { RS = "\x1e" } END { print $0 }' "${os_cmd_internal_tmpout}"
+	awk 'BEGIN { RS = "\x1e" } END { print $0 }' "${os_cmd_internal_tmperr}"
+}
+readonly -f os::cmd::internal::get_last_results
+
+# os::cmd::internal::mark_attempt marks the end of an attempt in the stdout and stderr log files
+# this is used to make the try_until_* output more concise
+function os::cmd::internal::mark_attempt() {
+	echo -e '\x1e' >> "${os_cmd_internal_tmpout}"
+	echo -e '\x1e' >> "${os_cmd_internal_tmperr}"
+}
+readonly -f os::cmd::internal::mark_attempt
+
+# os::cmd::internal::compress_output compresses an output file into timeline representation
+function os::cmd::internal::compress_output() {
+	local logfile=$1
+
+	awk -f "${OS_ROOT}/hack/lib/compress.awk" "${logfile}"
+}
+readonly -f os::cmd::internal::compress_output
+
+# os::cmd::internal::print_results pretty-prints the stderr and stdout files. If attempt separators
+# are present, this function returns a concise view of the stdout and stderr output files using a
+# timeline format, where consecutive output lines that are the same are condensed into one line
+# with a counter
+function os::cmd::internal::print_results() {
+	if [[ -s "${os_cmd_internal_tmpout}" ]]; then
+		echo "Standard output from the command:"
+		if grep -q $'\x1e' "${os_cmd_internal_tmpout}"; then
+			os::cmd::internal::compress_output "${os_cmd_internal_tmpout}"
+		else
+			cat "${os_cmd_internal_tmpout}"; echo
+		fi
+	else
+		echo "There was no output from the command."
+	fi
+
+	if [[ -s "${os_cmd_internal_tmperr}" ]]; then
+		echo "Standard error from the command:"
+		if grep -q $'\x1e' "${os_cmd_internal_tmperr}"; then
+			os::cmd::internal::compress_output "${os_cmd_internal_tmperr}"
+		else
+			cat "${os_cmd_internal_tmperr}"; echo
+		fi
+	else
+		echo "There was no error output from the command."
+	fi
+}
+readonly -f os::cmd::internal::print_results
+
+# os::cmd::internal::assemble_causes determines from the two input booleans which part of the test
+# failed and generates a nice delimited list of failure causes
+function os::cmd::internal::assemble_causes() {
+	local cmd_succeeded=$1
+	local test_succeeded=$2
+
+	local causes=()
+	if (( ! cmd_succeeded )); then
+		causes+=("the command returned the wrong error code")
+	fi
+	if (( ! test_succeeded )); then
+		causes+=("the output content test failed")
+	fi
+
+	local list
+    list=$(printf '; %s' "${causes[@]}")
+	echo "${list:2}"
+}
+readonly -f os::cmd::internal::assemble_causes
+
+# os::cmd::internal::run_until_exit_code runs the provided command until the exit code test given
+# succeeds or the timeout given runs out. Output from the command to be tested is suppressed unless
+# either `VERBOSE=1` or the test fails. This function bypasses any error exiting settings or traps
+# set by upstream callers by masking the return code of the command with the return code of setting
+# the result variable on failure.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - VERBOSE
+# Arguments:
+#  - 1: the command to run
+#  - 2: command evaluation assertion to use
+#  - 3: timeout duration
+#  - 4: interval duration
+# Returns:
+#  - 0: if all assertions met before timeout
+#  - 1: if timeout occurs
+function os::cmd::internal::run_until_exit_code() {
+	local cmd=$1
+	local cmd_eval_func=$2
+	local duration=$3
+	local interval=$4
+
+	local -a junit_log
+
+	os::cmd::internal::init_tempdir
+	os::test::junit::declare_test_start
+
+	local description
+    description=$(os::cmd::internal::describe_call "${cmd}" "${cmd_eval_func}")
+	local duration_seconds
+    duration_seconds=$(echo "scale=3; $(( duration )) / 1000" | bc | xargs printf '%5.3f')
+	local description="${description}; re-trying every ${interval}s until completion or ${duration_seconds}s"
+	local preamble="Running ${description}..."
+	echo "${preamble}"
+	# for ease of parsing, we want the entire declaration on one line, so we replace '\n' with ';'
+	junit_log+=( "${description//$'\n'/;}" )
+
+	local start_time
+    start_time=$(os::cmd::internal::seconds_since_epoch)
+
+	local deadline=$(( $(date +%s000) + duration ))
+	local cmd_succeeded=0
+	while [ "$(date +%s000)" -lt $deadline ]; do
+	    local cmd_result
+        cmd_result=$( os::cmd::internal::run_collecting_output "${cmd}"; echo $? )
+		cmd_succeeded=$( ${cmd_eval_func} "${cmd_result}"; echo $? )
+		if (( cmd_succeeded )); then
+			break
+		fi
+		sleep "${interval}"
+		os::cmd::internal::mark_attempt
+	done
+
+	local end_time
+    end_time=$(os::cmd::internal::seconds_since_epoch)
+	local time_elapsed
+    time_elapsed=$(echo "scale=9; ${end_time} - ${start_time}" | bc | xargs printf '%5.3f') # in decimal seconds, we need leading zeroes for parsing later
+
+	# clear the preamble so we can print out the success or error message
+	os::text::clear_string "${preamble}"
+
+	local return_code
+	if (( cmd_succeeded )); then
+		os::text::print_green "SUCCESS after ${time_elapsed}s: ${description}"
+		junit_log+=( "SUCCESS after ${time_elapsed}s: ${description//$'\n'/;}" )
+
+		if [[ -n ${VERBOSE-} ]]; then
+			os::cmd::internal::print_results
+		fi
+		return_code=0
+	else
+		os::text::print_red_bold "FAILURE after ${time_elapsed}s: ${description}: the command timed out"
+		junit_log+=( "FAILURE after ${time_elapsed}s: ${description//$'\n'/;}: the command timed out" )
+
+		os::text::print_red "$(os::cmd::internal::print_results)"
+		return_code=1
+	fi
+
+	junit_log+=( "$(os::cmd::internal::print_results)" )
+	( IFS=$'\n'; echo "${junit_log[*]}" >> "${JUNIT_REPORT_OUTPUT:-/dev/null}" )
+	os::test::junit::declare_test_end
+	return "${return_code}"
+}
+readonly -f os::cmd::internal::run_until_exit_code
+
+# os::cmd::internal::run_until_text runs the provided command until the assertion function succeeds with
+# the given text on the command output or the timeout given runs out. This can be used to run until the
+# output does or does not contain some text. Output from the command to be tested is suppressed unless
+# either `VERBOSE=1` or the test fails. This function bypasses any error exiting settings or traps
+# set by upstream callers by masking the return code of the command with the return code of setting
+# the result variable on failure.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - VERBOSE
+# Arguments:
+#  - 1: the command to run
+#  - 2: text to test for
+#  - 3: text assertion to use
+#  - 4: timeout duration
+#  - 5: interval duration
+# Returns:
+#  - 0: if all assertions met before timeout
+#  - 1: if timeout occurs
+function os::cmd::internal::run_until_text() {
+	local cmd=$1
+	local text=$2
+	local test_eval_func=${3:-os::cmd::internal::success_func}
+	local duration=$4
+	local interval=$5
+
+	local -a junit_log
+
+	os::cmd::internal::init_tempdir
+	os::test::junit::declare_test_start
+
+	local description
+    description=$(os::cmd::internal::describe_call "${cmd}" "" "${text}" "${test_eval_func}")
+	local duration_seconds
+    duration_seconds=$(echo "scale=3; $(( duration )) / 1000" | bc | xargs printf '%5.3f')
+	local description="${description}; re-trying every ${interval}s until completion or ${duration_seconds}s"
+	local preamble="Running ${description}..."
+	echo "${preamble}"
+	# for ease of parsing, we want the entire declaration on one line, so we replace '\n' with ';'
+	junit_log+=( "${description//$'\n'/;}" )
+
+	local start_time
+    start_time=$(os::cmd::internal::seconds_since_epoch)
+
+	local deadline
+    deadline=$(( $(date +%s000) + duration ))
+	local test_succeeded=0
+	while [ "$(date +%s000)" -lt $deadline ]; do
+	    local cmd_result=
+        cmd_result=$( os::cmd::internal::run_collecting_output "${cmd}"; echo $? )
+		local test_result
+		test_result=$( os::cmd::internal::run_collecting_output 'grep -Eq "'"${text}"'" <(os::cmd::internal::get_last_results)'; echo $? )
+		test_succeeded=$( ${test_eval_func} "${test_result}"; echo $? )
+
+		if (( test_succeeded )); then
+			break
+		fi
+		sleep "${interval}"
+		os::cmd::internal::mark_attempt
+	done
+
+	local end_time
+    end_time=$(os::cmd::internal::seconds_since_epoch)
+	local time_elapsed
+    time_elapsed=$(echo "scale=9; ${end_time} - ${start_time}" | bc | xargs printf '%5.3f') # in decimal seconds, we need leading zeroes for parsing later
+
+    # clear the preamble so we can print out the success or error message
+    os::text::clear_string "${preamble}"
+
+	local return_code
+	if (( test_succeeded )); then
+		os::text::print_green "SUCCESS after ${time_elapsed}s: ${description}"
+		junit_log+=( "SUCCESS after ${time_elapsed}s: ${description//$'\n'/;}" )
+
+		if [[ -n ${VERBOSE-} ]]; then
+			os::cmd::internal::print_results
+		fi
+		return_code=0
+	else
+		os::text::print_red_bold "FAILURE after ${time_elapsed}s: ${description}: the command timed out"
+		junit_log+=( "FAILURE after ${time_elapsed}s: ${description//$'\n'/;}: the command timed out" )
+
+		os::text::print_red "$(os::cmd::internal::print_results)"
+		return_code=1
+	fi
+
+	junit_log+=( "$(os::cmd::internal::print_results)" )
+	( IFS=$'\n'; echo "${junit_log[*]}" >> "${JUNIT_REPORT_OUTPUT:-/dev/null}" )
+	os::test::junit::declare_test_end
+	return "${return_code}"
+}
+readonly -f os::cmd::internal::run_until_text
diff --git a/openshift-hack/lib/constants.sh b/openshift-hack/lib/constants.sh
new file mode 100755
index 00000000000..3552d53115d
--- /dev/null
+++ b/openshift-hack/lib/constants.sh
@@ -0,0 +1,324 @@
+#!/usr/bin/env bash
+
+# This script provides constants for the Golang binary build process
+
+readonly OS_GO_PACKAGE=github.com/openshift/origin
+
+readonly OS_BUILD_ENV_GOLANG="${OS_BUILD_ENV_GOLANG:-1.15}"
+readonly OS_BUILD_ENV_IMAGE="${OS_BUILD_ENV_IMAGE:-openshift/origin-release:golang-${OS_BUILD_ENV_GOLANG}}"
+readonly OS_REQUIRED_GO_VERSION="go${OS_BUILD_ENV_GOLANG}"
+readonly OS_GLIDE_MINOR_VERSION="13"
+readonly OS_REQUIRED_GLIDE_VERSION="0.$OS_GLIDE_MINOR_VERSION"
+
+readonly OS_GOFLAGS_TAGS="include_gcs include_oss containers_image_openpgp"
+readonly OS_GOFLAGS_TAGS_LINUX_AMD64="gssapi selinux"
+readonly OS_GOFLAGS_TAGS_LINUX_S390X="gssapi selinux"
+readonly OS_GOFLAGS_TAGS_LINUX_ARM64="gssapi selinux"
+readonly OS_GOFLAGS_TAGS_LINUX_PPC64LE="gssapi selinux"
+
+readonly OS_OUTPUT_BASEPATH="${OS_OUTPUT_BASEPATH:-_output}"
+readonly OS_BASE_OUTPUT="${OS_ROOT}/${OS_OUTPUT_BASEPATH}"
+readonly OS_OUTPUT_SCRIPTPATH="${OS_OUTPUT_SCRIPTPATH:-"${OS_BASE_OUTPUT}/scripts"}"
+
+readonly OS_OUTPUT_SUBPATH="${OS_OUTPUT_SUBPATH:-${OS_OUTPUT_BASEPATH}/local}"
+readonly OS_OUTPUT="${OS_ROOT}/${OS_OUTPUT_SUBPATH}"
+readonly OS_OUTPUT_RELEASEPATH="${OS_OUTPUT}/releases"
+readonly OS_OUTPUT_RPMPATH="${OS_OUTPUT_RELEASEPATH}/rpms"
+readonly OS_OUTPUT_BINPATH="${OS_OUTPUT}/bin"
+readonly OS_OUTPUT_PKGDIR="${OS_OUTPUT}/pkgdir"
+
+readonly OS_IMAGE_COMPILE_TARGETS_LINUX=(
+  vendor/k8s.io/kubernetes/cmd/kube-apiserver
+  vendor/k8s.io/kubernetes/cmd/kube-controller-manager
+  vendor/k8s.io/kubernetes/cmd/kube-scheduler
+  vendor/k8s.io/kubernetes/cmd/kubelet
+)
+readonly OS_SCRATCH_IMAGE_COMPILE_TARGETS_LINUX=(
+  ""
+)
+readonly OS_IMAGE_COMPILE_BINARIES=("${OS_SCRATCH_IMAGE_COMPILE_TARGETS_LINUX[@]##*/}" "${OS_IMAGE_COMPILE_TARGETS_LINUX[@]##*/}")
+
+readonly OS_GOVET_BLACKLIST=(
+)
+
+#If you update this list, be sure to get the images/origin/Dockerfile
+readonly OS_BINARY_RELEASE_SERVER_LINUX=(
+  './*'
+)
+readonly OS_BINARY_RELEASE_CLIENT_EXTRA=(
+  ${OS_ROOT}/README.md
+  ${OS_ROOT}/LICENSE
+)
+
+# os::build::get_product_vars exports variables that we expect to change
+# depending on the distribution of Origin
+function os::build::get_product_vars() {
+  export OS_BUILD_LDFLAGS_IMAGE_PREFIX="${OS_IMAGE_PREFIX:-"openshift/origin"}"
+  export OS_BUILD_LDFLAGS_DEFAULT_IMAGE_STREAMS="${OS_BUILD_LDFLAGS_DEFAULT_IMAGE_STREAMS:-"centos7"}"
+}
+
+# os::build::ldflags calculates the -ldflags argument for building OpenShift
+function os::build::ldflags() {
+  # Run this in a subshell to prevent settings/variables from leaking.
+  set -o errexit
+  set -o nounset
+  set -o pipefail
+
+  cd "${OS_ROOT}"
+
+  os::build::version::get_vars
+  os::build::get_product_vars
+
+  local buildDate="$(date -u +'%Y-%m-%dT%H:%M:%SZ')"
+
+  declare -a ldflags=(
+    "-s"
+    "-w"
+  )
+
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/pkg/version.majorFromGit" "${OS_GIT_MAJOR}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/pkg/version.minorFromGit" "${OS_GIT_MINOR}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/pkg/version.versionFromGit" "${OS_GIT_VERSION}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/pkg/version.commitFromGit" "${OS_GIT_COMMIT}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/pkg/version.gitTreeState" "${OS_GIT_TREE_STATE}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/pkg/version.buildDate" "${buildDate}"))
+
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/component-base/version.gitMajor" "${KUBE_GIT_MAJOR}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/component-base/version.gitMinor" "${KUBE_GIT_MINOR}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/component-base/version.gitCommit" "${OS_GIT_COMMIT}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/component-base/version.gitVersion" "${KUBE_GIT_VERSION}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/component-base/version.buildDate" "${buildDate}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/component-base/version.gitTreeState" "clean"))
+
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/client-go/pkg/version.gitMajor" "${KUBE_GIT_MAJOR}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/client-go/pkg/version.gitMinor" "${KUBE_GIT_MINOR}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/client-go/pkg/version.gitCommit" "${OS_GIT_COMMIT}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/client-go/pkg/version.gitVersion" "${KUBE_GIT_VERSION}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/client-go/pkg/version.buildDate" "${buildDate}"))
+  ldflags+=($(os::build::ldflag "${OS_GO_PACKAGE}/vendor/k8s.io/client-go/pkg/version.gitTreeState" "clean")
+)
+
+  # The -ldflags parameter takes a single string, so join the output.
+  echo "${ldflags[*]-}"
+}
+readonly -f os::build::ldflags
+
+# os::util::list_go_src_files lists files we consider part of our project
+# source code, useful for tools that iterate over source to provide vet-
+# ting or linting, etc.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::util::list_go_src_files() {
+	find . -not \( \
+		\( \
+		-wholename './_output' \
+		-o -wholename './.*' \
+		-o -wholename './pkg/assets/bindata.go' \
+		-o -wholename './pkg/assets/*/bindata.go' \
+		-o -wholename './pkg/oc/clusterup/manifests/bindata.go' \
+		-o -wholename './openshift.local.*' \
+		-o -wholename './test/extended/testdata/bindata.go' \
+		-o -wholename '*/vendor/*' \
+		-o -wholename './assets/bower_components/*' \
+		\) -prune \
+	\) -name '*.go' | sort -u
+}
+readonly -f os::util::list_go_src_files
+
+# os::util::list_go_src_dirs lists dirs in origin/ and cmd/ dirs excluding
+# doc.go, useful for tools that iterate over source to provide vetting or
+# linting, or for godep-save etc.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::util::list_go_src_dirs() {
+    go list -e ./... | grep -Ev "/(third_party|vendor|staging|clientset_generated)/" | LC_ALL=C sort -u
+}
+readonly -f os::util::list_go_src_dirs
+
+# os::util::list_go_deps outputs the list of dependencies for the project.
+function os::util::list_go_deps() {
+  go list -f '{{.ImportPath}}{{.Imports}}' ./test/... ./pkg/... ./cmd/... ./vendor/k8s.io/... | tr '[]' '  ' |
+    sed -e 's|github.com/openshift/origin/vendor/||g' |
+    sed -e 's|k8s.io/kubernetes/staging/src/||g'
+}
+
+# os::util::list_test_packages_under lists all packages containing Golang test files that we
+# want to run as unit tests under the given base dir in the source tree
+function os::util::list_test_packages_under() {
+    local basedir=$*
+
+    # we do not quote ${basedir} to allow for multiple arguments to be passed in as well as to allow for
+    # arguments that use expansion, e.g. paths containing brace expansion or wildcards
+    # we do not quote ${basedir} to allow for multiple arguments to be passed in as well as to allow for
+    # arguments that use expansion, e.g. paths containing brace expansion or wildcards
+    find ${basedir} -not \(                   \
+        \(                                    \
+              -path 'vendor'                  \
+              -o -path '*_output'             \
+              -o -path '*.git'                \
+              -o -path '*openshift.local.*'   \
+              -o -path '*vendor/*'            \
+              -o -path '*assets/node_modules' \
+              -o -path '*test/*'              \
+              -o -path '*pkg/proxy'           \
+              -o -path '*k8s.io/kubernetes/cluster/gce*' \
+        \) -prune                             \
+    \) -name '*_test.go' | xargs -n1 dirname | sort -u | xargs -n1 printf "${OS_GO_PACKAGE}/%s\n"
+
+    local kubernetes_path="vendor/k8s.io/kubernetes"
+
+    if [[ -n "${TEST_KUBE-}" ]]; then
+      # we need to find all of the kubernetes test suites, excluding those we directly whitelisted before, the end-to-end suite, and
+      # cmd wasn't done before using glide and constantly flakes
+      # the forked etcd packages are used only by the gce etcd containers
+      find -L vendor/k8s.io/{api,apimachinery,apiserver,client-go,kube-aggregator,kubernetes} -not \( \
+        \(                                                                                          \
+          -path "${kubernetes_path}/staging"                                                        \
+          -o -path "${kubernetes_path}/cmd"                                                         \
+          -o -path "${kubernetes_path}/test"                                                        \
+          -o -path "${kubernetes_path}/third_party/forked/etcd*"                                    \
+          -o -path "${kubernetes_path}/cluster/gce" \
+       \) -prune                                                                                   \
+      \) -name '*_test.go' | cut -f 2- -d / | xargs -n1 dirname | sort -u | xargs -n1 printf "${OS_GO_PACKAGE}/vendor/%s\n"
+    else
+      echo "${OS_GO_PACKAGE}/vendor/k8s.io/api/..."
+      echo "${OS_GO_PACKAGE}/vendor/k8s.io/kubernetes/pkg/api/..."
+      echo "${OS_GO_PACKAGE}/vendor/k8s.io/kubernetes/pkg/apis/..."
+    fi
+}
+readonly -f os::util::list_test_packages_under
+
+# Generates the .syso file used to add compile-time VERSIONINFO metadata to the
+# Windows binary.
+function os::build::generate_windows_versioninfo() {
+  os::build::version::get_vars
+  local major="${OS_GIT_MAJOR}"
+  local minor="${OS_GIT_MINOR%+}"
+  local patch="${OS_GIT_PATCH}"
+  local windows_versioninfo_file=`mktemp --suffix=".versioninfo.json"`
+  cat <<EOF >"${windows_versioninfo_file}"
+{
+       "FixedFileInfo":
+       {
+               "FileVersion": {
+                       "Major": ${major},
+                       "Minor": ${minor},
+                       "Patch": ${patch}
+               },
+               "ProductVersion": {
+                       "Major": ${major},
+                       "Minor": ${minor},
+                       "Patch": ${patch}
+               },
+               "FileFlagsMask": "3f",
+               "FileFlags ": "00",
+               "FileOS": "040004",
+               "FileType": "01",
+               "FileSubType": "00"
+       },
+       "StringFileInfo":
+       {
+               "Comments": "",
+               "CompanyName": "Red Hat, Inc.",
+               "InternalName": "openshift client",
+               "FileVersion": "${OS_GIT_VERSION}",
+               "InternalName": "oc",
+               "LegalCopyright": "Â© Red Hat, Inc. Licensed under the Apache License, Version 2.0",
+               "LegalTrademarks": "",
+               "OriginalFilename": "oc.exe",
+               "PrivateBuild": "",
+               "ProductName": "OpenShift Client",
+               "ProductVersion": "${OS_GIT_VERSION}",
+               "SpecialBuild": ""
+       },
+       "VarFileInfo":
+       {
+               "Translation": {
+                       "LangID": "0409",
+                       "CharsetID": "04B0"
+               }
+       }
+}
+EOF
+  goversioninfo -o ${OS_ROOT}/vendor/github.com/openshift/oc/cmd/oc/oc.syso ${windows_versioninfo_file}
+}
+readonly -f os::build::generate_windows_versioninfo
+
+# Removes the .syso file used to add compile-time VERSIONINFO metadata to the
+# Windows binary.
+function os::build::clean_windows_versioninfo() {
+  rm ${OS_ROOT}/vendor/github.com/openshift/oc/cmd/oc/oc.syso
+}
+readonly -f os::build::clean_windows_versioninfo
+
+# OS_ALL_IMAGES is the list of images built by os::build::images.
+readonly OS_ALL_IMAGES=(
+  origin-hyperkube
+  origin-tests
+)
+
+# os::build::check_binaries ensures that binary sizes do not grow without approval.
+function os::build::check_binaries() {
+  platform=$(os::build::host_platform)
+  if [[ "${platform}" != "linux/amd64" && "${platform}" != "darwin/amd64" ]]; then
+    return 0
+  fi
+  duexe="du"
+
+  # In OSX, the 'du' binary does not provide the --apparent-size flag. However, the homebrew
+  # provide GNU coreutils which provide 'gdu' binary which is equivalent to Linux du.
+  # For now, if the 'gdu' binary is not installed, print annoying warning and don't check the
+  # binary size (the CI will capture possible violation anyway).
+  if [[ "${platform}" == "darwin/amd64" ]]; then
+    duexe=$(which gdu || true)
+    if [[ -z "${duexe}" ]]; then
+        os::log::warning "Unable to locate 'gdu' binary to determine size of the binary. Please install it using: 'brew install coreutils'"
+        return 0
+    fi
+  fi
+
+  if [[ -f "${OS_OUTPUT_BINPATH}/${platform}/pod" ]]; then
+    size=$($duexe --apparent-size -m "${OS_OUTPUT_BINPATH}/${platform}/pod" | cut -f 1)
+    if [[ "${size}" -gt "2" ]]; then
+      os::log::fatal "pod binary has grown substantially to ${size}. You must have approval before bumping this limit."
+    fi
+  fi
+}
+
+# os::build::images builds all images in this repo.
+function os::build::images() {
+  # Create link to file if the FS supports hardlinks, otherwise copy the file
+  function ln_or_cp {
+    local src_file=$1
+    local dst_dir=$2
+    if os::build::archive::internal::is_hardlink_supported "${dst_dir}" ; then
+      ln -f "${src_file}" "${dst_dir}"
+    else
+      cp -pf "${src_file}" "${dst_dir}"
+    fi
+  }
+
+  # determine the correct tag prefix
+  tag_prefix="${OS_IMAGE_PREFIX:-"openshift/origin"}"
+
+  # images that depend on "${tag_prefix}-source" or "${tag_prefix}-base"
+  ( os::build::image "${tag_prefix}-hyperkube"               images/hyperkube ) &
+
+  for i in $(jobs -p); do wait "$i"; done
+
+  # images that depend on "${tag_prefix}-cli" or hyperkube
+  ( os::build::image "${tag_prefix}-tests"          images/tests ) &
+
+  for i in $(jobs -p); do wait "$i"; done
+}
+readonly -f os::build::images
diff --git a/openshift-hack/lib/deps.sh b/openshift-hack/lib/deps.sh
new file mode 100644
index 00000000000..6a9009823de
--- /dev/null
+++ b/openshift-hack/lib/deps.sh
@@ -0,0 +1,28 @@
+#!/usr/bin/env bash
+
+# os::deps::path_with_shellcheck returns a path that includes shellcheck.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  The path that includes shellcheck.
+function os::deps::path_with_shellcheck() {
+  local path="${PATH}"
+  if ! which shellcheck &> /dev/null; then
+    local shellcheck_path="${TMPDIR:-/tmp}/shellcheck"
+    mkdir -p "${shellcheck_path}"
+    pushd "${shellcheck_path}" > /dev/null || exit 1
+      # This version needs to match that required by
+      # hack/verify-shellcheck.sh to avoid the use of docker.
+      local version="v0.7.0"
+      local tar_file="shellcheck-${version}.linux.x86_64.tar.xz"
+      curl -LO "https://github.com/koalaman/shellcheck/releases/download/${version}/${tar_file}"
+      tar xf "${tar_file}"
+      path="${PATH}:$(pwd)/shellcheck-${version}"
+    popd > /dev/null || exit 1
+  fi
+  echo "${path}"
+}
+readonly -f os::deps::path_with_shellcheck
diff --git a/openshift-hack/lib/init.sh b/openshift-hack/lib/init.sh
new file mode 100755
index 00000000000..00321b0ff71
--- /dev/null
+++ b/openshift-hack/lib/init.sh
@@ -0,0 +1,68 @@
+#!/usr/bin/env bash
+
+# This script is meant to be the entrypoint for OpenShift Bash scripts to import all of the support
+# libraries at once in order to make Bash script preambles as minimal as possible. This script recur-
+# sively `source`s *.sh files in this directory tree. As such, no files should be `source`ed outside
+# of this script to ensure that we do not attempt to overwrite read-only variables.
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+OS_SCRIPT_START_TIME="$( date +%s )"; export OS_SCRIPT_START_TIME
+
+# os::util::absolute_path returns the absolute path to the directory provided
+function os::util::absolute_path() {
+	local relative_path="$1"
+	local absolute_path
+
+	pushd "${relative_path}" >/dev/null
+	relative_path="$( pwd )"
+	if [[ -h "${relative_path}" ]]; then
+		absolute_path="$( readlink "${relative_path}" )"
+	else
+		absolute_path="${relative_path}"
+	fi
+	popd >/dev/null
+
+	echo "${absolute_path}"
+}
+readonly -f os::util::absolute_path
+
+# find the absolute path to the root of the Origin source tree
+init_source="$( dirname "${BASH_SOURCE[0]}" )/../.."
+OS_ROOT="$( os::util::absolute_path "${init_source}" )"
+export OS_ROOT
+cd "${OS_ROOT}"
+
+for library_file in $( find "${OS_ROOT}/openshift-hack/lib" -type f -name '*.sh' -not -path '*/openshift-hack/lib/init.sh' ); do
+	source "${library_file}"
+done
+
+unset library_files library_file init_source
+
+# all of our Bash scripts need to have the stacktrace
+# handler installed to deal with errors
+os::log::stacktrace::install
+
+# All of our Bash scripts need to have access to the
+# binaries that we build so we don't have to find
+# them before every invocation.
+os::util::environment::update_path_var
+
+if [[ -z "${OS_TMP_ENV_SET-}" ]]; then
+	# if this file is run via 'source', then $0 will be "-bash" and won't work with basename
+	if [[ "${0}" =~ .*\.sh ]]; then
+		os::util::environment::setup_tmpdir_vars "$( basename "${0}" ".sh" )"
+	else
+		os::util::environment::setup_tmpdir_vars "shell"
+	fi
+fi
+
+# Allow setting $JUNIT_REPORT to toggle output behavior
+if [[ -n "${JUNIT_REPORT:-}" ]]; then
+  export JUNIT_REPORT_OUTPUT="${LOG_DIR}/raw_test_output.log"
+fi
+
+# Use the go version from the system
+export FORCE_HOST_GO=1
diff --git a/openshift-hack/lib/log/output.sh b/openshift-hack/lib/log/output.sh
new file mode 100644
index 00000000000..103fa1ff1be
--- /dev/null
+++ b/openshift-hack/lib/log/output.sh
@@ -0,0 +1,104 @@
+#!/usr/bin/env bash
+
+# This file contains functions used for writing log messages
+# to stdout and stderr from scripts while they run.
+
+# os::log::info writes the message to stdout.
+#
+# Arguments:
+#  - all: message to write
+function os::log::info() {
+	local message; message="$( os::log::internal::prefix_lines "[INFO]" "$*" )"
+	os::log::internal::to_logfile "${message}"
+	echo "${message}"
+}
+readonly -f os::log::info
+
+# os::log::warning writes the message to stderr.
+# A warning indicates something went wrong but
+# not so wrong that we cannot recover.
+#
+# Arguments:
+#  - all: message to write
+function os::log::warning() {
+	local message; message="$( os::log::internal::prefix_lines "[WARNING]" "$*" )"
+	os::log::internal::to_logfile "${message}"
+	os::text::print_yellow "${message}" 1>&2
+}
+readonly -f os::log::warning
+
+# os::log::error writes the message to stderr.
+# An error indicates that something went wrong
+# and we will most likely fail after this.
+#
+# Arguments:
+#  - all: message to write
+function os::log::error() {
+	local message; message="$( os::log::internal::prefix_lines "[ERROR]" "$*" )"
+	os::log::internal::to_logfile "${message}"
+	os::text::print_red "${message}" 1>&2
+}
+readonly -f os::log::error
+
+# os::log::fatal writes the message to stderr and
+# returns a non-zero code to force a process exit.
+# A fatal error indicates that there is no chance
+# of recovery.
+#
+# Arguments:
+#  - all: message to write
+function os::log::fatal() {
+	local message; message="$( os::log::internal::prefix_lines "[FATAL]" "$*" )"
+	os::log::internal::to_logfile "${message}"
+	os::text::print_red "${message}" 1>&2
+	exit 1
+}
+readonly -f os::log::fatal
+
+# os::log::debug writes the message to stderr if
+# the ${OS_DEBUG} variable is set.
+#
+# Globals:
+#  - OS_DEBUG
+# Arguments:
+#  - all: message to write
+function os::log::debug() {
+	local message; message="$( os::log::internal::prefix_lines "[DEBUG]" "$*" )"
+	os::log::internal::to_logfile "${message}"
+	if [[ -n "${OS_DEBUG:-}" ]]; then
+		os::text::print_blue "${message}" 1>&2
+	fi
+}
+readonly -f os::log::debug
+
+# os::log::internal::to_logfile makes a best-effort
+# attempt to write the message to the script logfile
+#
+# Globals:
+#  - LOG_DIR
+# Arguments:
+#  - all: message to write
+function os::log::internal::to_logfile() {
+	if [[ -n "${LOG_DIR:-}" && -d "${LOG_DIR-}" ]]; then
+		echo "$*" >>"${LOG_DIR}/scripts.log"
+	fi
+}
+
+# os::log::internal::prefix_lines prints out the
+# original content with the given prefix at the
+# start of every line.
+#
+# Arguments:
+#  - 1: prefix for lines
+#  - 2: content to prefix
+function os::log::internal::prefix_lines() {
+	local prefix="$1"
+	local content="$2"
+
+	local old_ifs="${IFS}"
+	IFS=$'\n'
+	for line in ${content}; do
+		echo "${prefix} ${line}"
+	done
+	IFS="${old_ifs}"
+}
\ No newline at end of file
diff --git a/openshift-hack/lib/log/stacktrace.sh b/openshift-hack/lib/log/stacktrace.sh
new file mode 100644
index 00000000000..e9915efb634
--- /dev/null
+++ b/openshift-hack/lib/log/stacktrace.sh
@@ -0,0 +1,91 @@
+#!/usr/bin/env bash
+#
+# This library contains an implementation of a stack trace for Bash scripts.
+
+# os::log::stacktrace::install installs the stacktrace as a handler for the ERR signal if one
+# has not already been installed and sets `set -o errtrace` in order to propagate the handler
+# If the ERR trap is not initialized, installing this plugin will initialize it.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  - export OS_USE_STACKTRACE
+function os::log::stacktrace::install() {
+    # setting 'errtrace' propagates our ERR handler to functions, expansions and subshells
+    set -o errtrace
+
+    # OS_USE_STACKTRACE is read by os::util::trap at runtime to request a stacktrace
+    export OS_USE_STACKTRACE=true
+
+    os::util::trap::init_err
+}
+readonly -f os::log::stacktrace::install
+
+# os::log::stacktrace::print prints the stacktrace and exits with the return code from the script that
+# called for a stack trace. This function will always return 0 if it is not handling the signal, and if it
+# is handling the signal, this function will always `exit`, not return, the return code it receives as
+# its first argument.
+#
+# Globals:
+#  - BASH_SOURCE
+#  - BASH_LINENO
+#  - FUNCNAME
+# Arguments:
+#  - 1: the return code of the command in the script that generated the ERR signal
+#  - 2: the last command that ran before handlers were invoked
+#  - 3: whether or not `set -o errexit` was set in the script that generated the ERR signal
+# Returns:
+#  None
+function os::log::stacktrace::print() {
+    local return_code=$1
+    local last_command=$2
+    local errexit_set=${3:-}
+
+    if [[ "${return_code}" = "0" ]]; then
+        # we're not supposed to respond when no error has occurred
+        return 0
+    fi
+
+    if [[ -z "${errexit_set}" ]]; then
+        # if errexit wasn't set in the shell when the ERR signal was issued, then we can ignore the signal
+        # as this is not cause for failure
+        return 0
+    fi
+
+    # dump the entire stack for debugging purposes
+    os::log::debug "$( os::util::repository_relative_path "${BASH_SOURCE[0]}:${LINENO}: ${BASH_COMMAND}" )"
+    for (( i = 0; i < ${#BASH_LINENO[@]}; i++ )); do
+        os::log::debug "$( os::util::repository_relative_path "${BASH_SOURCE[$i+1]:-"$( os::util::repository_relative_path "$0" )"}" ):${BASH_LINENO[$i]}: ${FUNCNAME[$i]}"
+    done
+
+    # iterate backwards through the stack until we leave library files, so we can be sure we start logging
+    # actual script code and not this handler's call
+    local stack_begin_index
+    for (( stack_begin_index = 0; stack_begin_index < ${#BASH_SOURCE[@]}; stack_begin_index++ )); do
+        if [[ ! "${BASH_SOURCE[${stack_begin_index}]}" =~ hack/lib/(log/stacktrace|util/trap)\.sh ]]; then
+            break
+        fi
+    done
+
+    local preamble_finished
+    local stack_index=1
+    local i
+    for (( i = stack_begin_index; i < ${#BASH_SOURCE[@]}; i++ )); do
+        local bash_source
+        bash_source="$( os::util::repository_relative_path "${BASH_SOURCE[$i]}" )"
+        if [[ -z "${preamble_finished:-}" ]]; then
+            preamble_finished=true
+            os::log::error "${bash_source}:${BASH_LINENO[$i-1]}: \`${last_command}\` exited with status ${return_code}." >&2
+            exit "${return_code}"
+        fi
+        stack_index=$(( stack_index + 1 ))
+    done
+
+    # we know we're the privileged handler in this chain, so we can safely exit the shell without
+    # starving another handler of the privilege of reacting to this signal
+    os::log::info "  Exiting with code ${return_code}." >&2
+    exit "${return_code}"
+}
+readonly -f os::log::stacktrace::print
diff --git a/openshift-hack/lib/test/junit.sh b/openshift-hack/lib/test/junit.sh
new file mode 100644
index 00000000000..18bb3ee857d
--- /dev/null
+++ b/openshift-hack/lib/test/junit.sh
@@ -0,0 +1,202 @@
+#!/usr/bin/env bash
+# This utility file contains functions that format test output to be parsed into jUnit XML
+
+# os::test::junit::declare_suite_start prints a message declaring the start of a test suite
+# Any number of suites can be in flight at any time, so there is no failure condition for this
+# script based on the number of suites in flight.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - NUM_OS_JUNIT_SUITES_IN_FLIGHT
+# Arguments:
+#  - 1: the suite name that is starting
+# Returns:
+#  - increment NUM_OS_JUNIT_SUITES_IN_FLIGHT
+function os::test::junit::declare_suite_start() {
+    local suite_name=$1
+    local num_suites=${NUM_OS_JUNIT_SUITES_IN_FLIGHT:-0}
+
+    echo "=== BEGIN TEST SUITE github.com/openshift/origin/test/${suite_name} ===" >> "${JUNIT_REPORT_OUTPUT:-/dev/null}"
+    NUM_OS_JUNIT_SUITES_IN_FLIGHT=$(( num_suites + 1 ))
+    export NUM_OS_JUNIT_SUITES_IN_FLIGHT
+}
+readonly -f os::test::junit::declare_suite_start
+
+# os::test::junit::declare_suite_end prints a message declaring the end of a test suite
+# If there aren't any suites in flight, this function will fail.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - NUM_OS_JUNIT_SUITES_IN_FLIGHT
+# Arguments:
+#  - 1: the suite name that is starting
+# Returns:
+#  - export/decrement NUM_OS_JUNIT_SUITES_IN_FLIGHT
+function os::test::junit::declare_suite_end() {
+    local num_suites=${NUM_OS_JUNIT_SUITES_IN_FLIGHT:-0}
+    if [[ "${num_suites}" -lt "1" ]]; then
+        # we can't end a suite if none have been started yet
+        echo "[ERROR] jUnit suite marker could not be placed, expected suites in flight, got ${num_suites}"
+        return 1
+    fi
+
+    echo "=== END TEST SUITE ===" >> "${JUNIT_REPORT_OUTPUT:-/dev/null}"
+    NUM_OS_JUNIT_SUITES_IN_FLIGHT=$(( num_suites - 1 ))
+    export NUM_OS_JUNIT_SUITES_IN_FLIGHT
+}
+readonly -f os::test::junit::declare_suite_end
+
+# os::test::junit::declare_test_start prints a message declaring the start of a test case
+# If there is already a test marked as being in flight, this function will fail.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - NUM_OS_JUNIT_TESTS_IN_FLIGHT
+# Arguments:
+#  None
+# Returns:
+#  - increment NUM_OS_JUNIT_TESTS_IN_FLIGHT
+function os::test::junit::declare_test_start() {
+    local num_tests=${NUM_OS_JUNIT_TESTS_IN_FLIGHT:-0}
+    if [[ "${num_tests}" -ne "0" ]]; then
+        # someone's declaring the starting of a test when a test is already in flight
+        echo "[ERROR] jUnit test marker could not be placed, expected no tests in flight, got ${num_tests}"
+        return 1
+    fi
+
+    local num_suites=${NUM_OS_JUNIT_SUITES_IN_FLIGHT:-0}
+    if [[ "${num_suites}" -lt "1" ]]; then
+        # we can't end a test if no suites are in flight
+        echo "[ERROR] jUnit test marker could not be placed, expected suites in flight, got ${num_suites}"
+        return 1
+    fi
+
+    echo "=== BEGIN TEST CASE ===" >> "${JUNIT_REPORT_OUTPUT:-/dev/null}"
+    NUM_OS_JUNIT_TESTS_IN_FLIGHT=$(( num_tests + 1 ))
+    export NUM_OS_JUNIT_TESTS_IN_FLIGHT
+}
+readonly -f os::test::junit::declare_test_start
+
+# os::test::junit::declare_test_end prints a message declaring the end of a test case
+# If there is no test marked as being in flight, this function will fail.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - NUM_OS_JUNIT_TESTS_IN_FLIGHT
+# Arguments:
+#  None
+# Returns:
+#  - decrement NUM_OS_JUNIT_TESTS_IN_FLIGHT
+function os::test::junit::declare_test_end() {
+    local num_tests=${NUM_OS_JUNIT_TESTS_IN_FLIGHT:-0}
+    if [[ "${num_tests}" -ne "1" ]]; then
+        # someone's declaring the end of a test when a test is not in flight
+        echo "[ERROR] jUnit test marker could not be placed, expected one test in flight, got ${num_tests}"
+        return 1
+    fi
+
+    echo "=== END TEST CASE ===" >> "${JUNIT_REPORT_OUTPUT:-/dev/null}"
+    NUM_OS_JUNIT_TESTS_IN_FLIGHT=$(( num_tests - 1 ))
+    export NUM_OS_JUNIT_TESTS_IN_FLIGHT
+}
+readonly -f os::test::junit::declare_test_end
+
+# os::test::junit::check_test_counters checks that we do not have any test suites or test cases in flight
+# This function should be called at the very end of any test script using jUnit markers to make sure no error in
+# marking has occurred.
+#
+# Globals:
+#  - NUM_OS_JUNIT_SUITES_IN_FLIGHT
+#  - NUM_OS_JUNIT_TESTS_IN_FLIGHT
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::test::junit::check_test_counters() {
+    if [[ "${NUM_OS_JUNIT_SUITES_IN_FLIGHT-}" -ne "0" ]]; then
+        echo "[ERROR] Expected no test suites to be marked as in-flight at the end of testing, got ${NUM_OS_JUNIT_SUITES_IN_FLIGHT-}"
+        return 1
+    elif [[ "${NUM_OS_JUNIT_TESTS_IN_FLIGHT-}" -ne "0" ]]; then
+        echo "[ERROR] Expected no test cases to be marked as in-flight at the end of testing, got ${NUM_OS_JUNIT_TESTS_IN_FLIGHT-}"
+        return 1
+    fi
+}
+readonly -f os::test::junit::check_test_counters
+
+# os::test::junit::reconcile_output appends the necessary suite and test end statements to the jUnit output file
+# in order to ensure that the file is in a consistent state to allow for parsing
+#
+# Globals:
+#  - NUM_OS_JUNIT_SUITES_IN_FLIGHT
+#  - NUM_OS_JUNIT_TESTS_IN_FLIGHT
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::test::junit::reconcile_output() {
+    if [[ "${NUM_OS_JUNIT_TESTS_IN_FLIGHT:-0}" = "1" ]]; then
+        os::test::junit::declare_test_end
+    fi
+
+    for (( i = 0; i < ${NUM_OS_JUNIT_SUITES_IN_FLIGHT:-0}; i++ )); do
+        os::test::junit::declare_suite_end
+    done
+}
+readonly -f os::test::junit::reconcile_output
+
+# os::test::junit::generate_report determines which type of report is to
+# be generated and does so from the raw output of the tests.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - ARTIFACT_DIR
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::test::junit::generate_report() {
+    if [[ -z "${JUNIT_REPORT_OUTPUT:-}" ||
+          -n "${JUNIT_REPORT_OUTPUT:-}" && ! -s "${JUNIT_REPORT_OUTPUT:-}" ]]; then
+        # we can't generate a report
+        return 0
+    fi
+
+    if grep -q "=== END TEST CASE ===" "${JUNIT_REPORT_OUTPUT}"; then
+        os::test::junit::reconcile_output
+        os::test::junit::check_test_counters
+        os::test::junit::internal::generate_report "oscmd"
+    fi
+}
+
+# os::test::junit::internal::generate_report generates an XML jUnit
+# report for either `os::cmd` or `go test`, based on the passed
+# argument. If the `junitreport` binary is not present, it will be built.
+#
+# Globals:
+#  - JUNIT_REPORT_OUTPUT
+#  - ARTIFACT_DIR
+# Arguments:
+#  - 1: specify which type of tests command output should junitreport read
+# Returns:
+#  export JUNIT_REPORT_NUM_FAILED
+function os::test::junit::internal::generate_report() {
+    local report_type="$1"
+    os::util::ensure::built_binary_exists 'junitreport'
+
+    local report_file
+    report_file="$( mktemp "${ARTIFACT_DIR}/${report_type}_report_XXXXX" ).xml"
+    os::log::info "jUnit XML report placed at $( os::util::repository_relative_path "${report_file}" )"
+    junitreport --type "${report_type}"             \
+                --suites nested                     \
+                --roots github.com/openshift/origin \
+                --output "${report_file}"           \
+                <"${JUNIT_REPORT_OUTPUT}"
+
+    local summary
+    summary=$( junitreport summarize <"${report_file}" )
+
+    JUNIT_REPORT_NUM_FAILED="$( grep -oE "[0-9]+ failed" <<<"${summary}" )"
+    export JUNIT_REPORT_NUM_FAILED
+
+    echo "${summary}"
+}
diff --git a/openshift-hack/lib/util/ensure.sh b/openshift-hack/lib/util/ensure.sh
new file mode 100644
index 00000000000..158d94f984f
--- /dev/null
+++ b/openshift-hack/lib/util/ensure.sh
@@ -0,0 +1,116 @@
+#!/usr/bin/env bash
+
+# This script contains helper functions for ensuring that dependencies
+# exist on a host system that are required to run Origin scripts.
+
+# os::util::ensure::system_binary_exists ensures that the
+# given binary exists on the system in the $PATH.
+#
+# Globals:
+#  None
+# Arguments:
+#  - 1: binary to search for
+# Returns:
+#  None
+function os::util::ensure::system_binary_exists() {
+	local binary="$1"
+
+if ! os::util::find::system_binary "${binary}" >/dev/null 2>&1; then
+		os::log::fatal "Required \`${binary}\` binary was not found in \$PATH."
+	fi
+}
+readonly -f os::util::ensure::system_binary_exists
+
+# os::util::ensure::built_binary_exists ensures that the
+# given binary exists on the system in the local output
+# directory for the current platform. If it doesn't, we
+# will attempt to build it if we can determine the correct
+# hack/build-go.sh target for the binary.
+#
+# This function will attempt to determine the correct
+# hack/build-go.sh target for the binary, but may not
+# be able to do so if the target doesn't live under
+# cmd/ or tools/. In that case, one should be given.
+#
+# Globals:
+#  - OS_ROOT
+# Arguments:
+#  - 1: binary to search for
+#  - 2: optional build target for this binary
+# Returns:
+#  None
+function os::util::ensure::built_binary_exists() {
+	local binary="$1"
+	local target="${2:-}"
+
+	if ! os::util::find::built_binary "${binary}" >/dev/null 2>&1; then
+		if [[ -z "${target}" ]]; then
+			if [[ -d "${OS_ROOT}/cmd/${binary}" ]]; then
+				target="cmd/${binary}"
+			elif [[ -d "${OS_ROOT}/tools/${binary}" ]]; then
+				target="tools/${binary}"
+			elif [[ -d "${OS_ROOT}/openshift-hack/${binary}" ]]; then
+				target="openshift-hack/${binary}"
+			fi
+		fi
+
+		if [[ -n "${target}" ]]; then
+			os::log::info "No compiled \`${binary}\` binary was found. Attempting to build one using:
+  $ hack/build-go.sh ${target}"
+			"${OS_ROOT}/hack/build-go.sh" "${target}"
+		else
+			os::log::fatal "No compiled \`${binary}\` binary was found and no build target could be determined.
+Provide the binary and try running $0 again."
+		fi
+	fi
+}
+readonly -f os::util::ensure::built_binary_exists
+
+# os::util::ensure::gopath_binary_exists ensures that the
+# given binary exists on the system in $GOPATH.  If it
+# doesn't, we will attempt to build it if we can determine
+# the correct install path for the binary.
+#
+# Globals:
+#  - GOPATH
+# Arguments:
+#  - 1: binary to search for
+#  - 2: [optional] path to install from
+# Returns:
+#  None
+function os::util::ensure::gopath_binary_exists() {
+	local binary="$1"
+	local install_path="${2:-}"
+
+	if ! os::util::find::gopath_binary "${binary}" >/dev/null 2>&1; then
+		if [[ -n "${install_path:-}" ]]; then
+			os::log::info "No installed \`${binary}\` was found in \$GOPATH. Attempting to install using:
+  $ go get ${install_path}"
+  			go get "${install_path}"
+		else
+			os::log::fatal "Required \`${binary}\` binary was not found in \$GOPATH."
+		fi
+	fi
+}
+readonly -f os::util::ensure::gopath_binary_exists
+
+# os::util::ensure::iptables_privileges_exist tests if the
+# testing machine has iptables available and in PATH. Also
+# tests that the user can list iptables rules, trying with
+# `sudo` if it fails without.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::util::ensure::iptables_privileges_exist() {
+	os::util::ensure::system_binary_exists 'iptables'
+
+	if ! iptables --list >/dev/null 2>&1 && ! sudo iptables --list >/dev/null 2>&1; then
+		os::log::fatal "You do not have \`iptables\` or \`sudo\` privileges. Kubernetes services will not work
+without \`iptables\` access. See https://github.com/kubernetes/kubernetes/issues/1859."
+	fi
+}
+readonly -f os::util::ensure::iptables_privileges_exist
diff --git a/openshift-hack/lib/util/environment.sh b/openshift-hack/lib/util/environment.sh
new file mode 100644
index 00000000000..1b0d55c7c47
--- /dev/null
+++ b/openshift-hack/lib/util/environment.sh
@@ -0,0 +1,296 @@
+#!/usr/bin/env bash
+
+# This script holds library functions for setting up the shell environment for OpenShift scripts
+
+# os::util::environment::use_sudo updates $USE_SUDO to be 'true', so that later scripts choosing between
+# execution using 'sudo' and execution without it chose to use 'sudo'
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  - export USE_SUDO
+function os::util::environment::use_sudo() {
+    USE_SUDO=true
+    export USE_SUDO
+}
+readonly -f os::util::environment::use_sudo
+
+# os::util::environment::setup_time_vars sets up environment variables that describe durations of time
+# These variables can be used to specify times for other utility functions
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  - export TIME_MS
+#  - export TIME_SEC
+#  - export TIME_MIN
+function os::util::environment::setup_time_vars() {
+    TIME_MS=1
+    export TIME_MS
+    TIME_SEC="$(( 1000  * TIME_MS ))"
+    export TIME_SEC
+    TIME_MIN="$(( 60 * TIME_SEC ))"
+    export TIME_MIN
+}
+readonly -f os::util::environment::setup_time_vars
+
+# os::util::environment::setup_all_server_vars sets up all environment variables necessary to configure and start an OpenShift server
+#
+# Globals:
+#  - OS_ROOT
+#  - PATH
+#  - TMPDIR
+#  - LOG_DIR
+#  - ARTIFACT_DIR
+#  - KUBELET_SCHEME
+#  - KUBELET_BIND_HOST
+#  - KUBELET_HOST
+#  - KUBELET_PORT
+#  - BASETMPDIR
+#  - ETCD_PORT
+#  - ETCD_PEER_PORT
+#  - API_BIND_HOST
+#  - API_HOST
+#  - API_PORT
+#  - API_SCHEME
+#  - PUBLIC_MASTER_HOST
+#  - USE_IMAGES
+# Arguments:
+#  - 1: the path under the root temporary directory for OpenShift where these subdirectories should be made
+# Returns:
+#  - export PATH
+#  - export BASETMPDIR
+#  - export LOG_DIR
+#  - export VOLUME_DIR
+#  - export ARTIFACT_DIR
+#  - export FAKE_HOME_DIR
+#  - export KUBELET_SCHEME
+#  - export KUBELET_BIND_HOST
+#  - export KUBELET_HOST
+#  - export KUBELET_PORT
+#  - export ETCD_PORT
+#  - export ETCD_PEER_PORT
+#  - export ETCD_DATA_DIR
+#  - export API_BIND_HOST
+#  - export API_HOST
+#  - export API_PORT
+#  - export API_SCHEME
+#  - export SERVER_CONFIG_DIR
+#  - export MASTER_CONFIG_DIR
+#  - export NODE_CONFIG_DIR
+#  - export USE_IMAGES
+#  - export TAG
+function os::util::environment::setup_all_server_vars() {
+    os::util::environment::setup_kubelet_vars
+    os::util::environment::setup_etcd_vars
+    os::util::environment::setup_server_vars
+    os::util::environment::setup_images_vars
+}
+readonly -f os::util::environment::setup_all_server_vars
+
+# os::util::environment::update_path_var updates $PATH so that OpenShift binaries are available
+#
+# Globals:
+#  - OS_ROOT
+#  - PATH
+# Arguments:
+#  None
+# Returns:
+#  - export PATH
+function os::util::environment::update_path_var() {
+    local prefix
+    if os::util::find::system_binary 'go' >/dev/null 2>&1; then
+        prefix+="${OS_OUTPUT_BINPATH}/$(os::build::host_platform):"
+    fi
+    if [[ -n "${GOPATH:-}" ]]; then
+        prefix+="${GOPATH}/bin:"
+    fi
+
+    PATH="${prefix:-}${PATH}"
+    export PATH
+}
+readonly -f os::util::environment::update_path_var
+
+# os::util::environment::setup_tmpdir_vars sets up temporary directory path variables
+#
+# Globals:
+#  - TMPDIR
+# Arguments:
+#  - 1: the path under the root temporary directory for OpenShift where these subdirectories should be made
+# Returns:
+#  - export BASETMPDIR
+#  - export BASEOUTDIR
+#  - export LOG_DIR
+#  - export VOLUME_DIR
+#  - export ARTIFACT_DIR
+#  - export FAKE_HOME_DIR
+#  - export OS_TMP_ENV_SET
+function os::util::environment::setup_tmpdir_vars() {
+    local sub_dir=$1
+
+    BASETMPDIR="${TMPDIR:-/tmp}/openshift/${sub_dir}"
+    export BASETMPDIR
+    VOLUME_DIR="${BASETMPDIR}/volumes"
+    export VOLUME_DIR
+
+    BASEOUTDIR="${OS_OUTPUT_SCRIPTPATH}/${sub_dir}"
+    export BASEOUTDIR
+    LOG_DIR="${ARTIFACT_DIR:-${BASEOUTDIR}}/logs"
+    export LOG_DIR
+    ARTIFACT_DIR="${ARTIFACT_DIR:-${BASEOUTDIR}/artifacts}"
+    export ARTIFACT_DIR
+    FAKE_HOME_DIR="${BASEOUTDIR}/openshift.local.home"
+    export FAKE_HOME_DIR
+
+    mkdir -p "${LOG_DIR}" "${VOLUME_DIR}" "${ARTIFACT_DIR}" "${FAKE_HOME_DIR}"
+
+    export OS_TMP_ENV_SET="${sub_dir}"
+}
+readonly -f os::util::environment::setup_tmpdir_vars
+
+# os::util::environment::setup_kubelet_vars sets up environment variables necessary for interacting with the kubelet
+#
+# Globals:
+#  - KUBELET_SCHEME
+#  - KUBELET_BIND_HOST
+#  - KUBELET_HOST
+#  - KUBELET_PORT
+# Arguments:
+#  None
+# Returns:
+#  - export KUBELET_SCHEME
+#  - export KUBELET_BIND_HOST
+#  - export KUBELET_HOST
+#  - export KUBELET_PORT
+function os::util::environment::setup_kubelet_vars() {
+    KUBELET_SCHEME="${KUBELET_SCHEME:-https}"
+    export KUBELET_SCHEME
+    KUBELET_BIND_HOST="${KUBELET_BIND_HOST:-127.0.0.1}"
+    export KUBELET_BIND_HOST
+    KUBELET_HOST="${KUBELET_HOST:-${KUBELET_BIND_HOST}}"
+    export KUBELET_HOST
+    KUBELET_PORT="${KUBELET_PORT:-10250}"
+    export KUBELET_PORT
+}
+readonly -f os::util::environment::setup_kubelet_vars
+
+# os::util::environment::setup_etcd_vars sets up environment variables necessary for interacting with etcd
+#
+# Globals:
+#  - BASETMPDIR
+#  - ETCD_HOST
+#  - ETCD_PORT
+#  - ETCD_PEER_PORT
+# Arguments:
+#  None
+# Returns:
+#  - export ETCD_HOST
+#  - export ETCD_PORT
+#  - export ETCD_PEER_PORT
+#  - export ETCD_DATA_DIR
+function os::util::environment::setup_etcd_vars() {
+    ETCD_HOST="${ETCD_HOST:-127.0.0.1}"
+    export ETCD_HOST
+    ETCD_PORT="${ETCD_PORT:-4001}"
+    export ETCD_PORT
+    ETCD_PEER_PORT="${ETCD_PEER_PORT:-7001}"
+    export ETCD_PEER_PORT
+
+    ETCD_DATA_DIR="${BASETMPDIR}/etcd"
+    export ETCD_DATA_DIR
+
+    mkdir -p "${ETCD_DATA_DIR}"
+}
+readonly -f os::util::environment::setup_etcd_vars
+
+# os::util::environment::setup_server_vars sets up environment variables necessary for interacting with the server
+#
+# Globals:
+#  - BASETMPDIR
+#  - KUBELET_HOST
+#  - API_BIND_HOST
+#  - API_HOST
+#  - API_PORT
+#  - API_SCHEME
+#  - PUBLIC_MASTER_HOST
+# Arguments:
+#  None
+# Returns:
+#  - export API_BIND_HOST
+#  - export API_HOST
+#  - export API_PORT
+#  - export API_SCHEME
+#  - export SERVER_CONFIG_DIR
+#  - export MASTER_CONFIG_DIR
+#  - export NODE_CONFIG_DIR
+function os::util::environment::setup_server_vars() {
+    # turn on cache mutation detector every time we start a server
+    KUBE_CACHE_MUTATION_DETECTOR="${KUBE_CACHE_MUTATION_DETECTOR:-true}"
+    export KUBE_CACHE_MUTATION_DETECTOR
+
+    API_BIND_HOST="${API_BIND_HOST:-127.0.0.1}"
+    export API_BIND_HOST
+    API_HOST="${API_HOST:-${API_BIND_HOST}}"
+    export API_HOST
+    API_PORT="${API_PORT:-8443}"
+    export API_PORT
+    API_SCHEME="${API_SCHEME:-https}"
+    export API_SCHEME
+
+    MASTER_ADDR="${API_SCHEME}://${API_HOST}:${API_PORT}"
+    export MASTER_ADDR
+    PUBLIC_MASTER_HOST="${PUBLIC_MASTER_HOST:-${API_HOST}}"
+    export PUBLIC_MASTER_HOST
+
+    SERVER_CONFIG_DIR="${BASETMPDIR}/openshift.local.config"
+    export SERVER_CONFIG_DIR
+    MASTER_CONFIG_DIR="${SERVER_CONFIG_DIR}/master"
+    export MASTER_CONFIG_DIR
+    NODE_CONFIG_DIR="${SERVER_CONFIG_DIR}/node-${KUBELET_HOST}"
+    export NODE_CONFIG_DIR
+
+    ETCD_CLIENT_CERT="${MASTER_CONFIG_DIR}/master.etcd-client.crt"
+    export ETCD_CLIENT_CERT
+    ETCD_CLIENT_KEY="${MASTER_CONFIG_DIR}/master.etcd-client.key"
+    export ETCD_CLIENT_KEY
+    ETCD_CA_BUNDLE="${MASTER_CONFIG_DIR}/ca-bundle.crt"
+    export ETCD_CA_BUNDLE
+
+    mkdir -p "${SERVER_CONFIG_DIR}" "${MASTER_CONFIG_DIR}" "${NODE_CONFIG_DIR}"
+}
+readonly -f os::util::environment::setup_server_vars
+
+# os::util::environment::setup_images_vars sets up environment variables necessary for interacting with release images
+#
+# Globals:
+#  - OS_ROOT
+#  - USE_IMAGES
+# Arguments:
+#  None
+# Returns:
+#  - export USE_IMAGES
+#  - export TAG
+#  - export MAX_IMAGES_BULK_IMPORTED_PER_REPOSITORY
+function os::util::environment::setup_images_vars() {
+    # Use either the latest release built images, or latest.
+    IMAGE_PREFIX="${OS_IMAGE_PREFIX:-"openshift/origin"}"
+    if [[ -z "${USE_IMAGES-}" ]]; then
+        TAG='latest'
+        export TAG
+        USE_IMAGES="${IMAGE_PREFIX}-\${component}:latest"
+        export USE_IMAGES
+
+        if [[ -e "${OS_ROOT}/_output/local/releases/.commit" ]]; then
+            TAG="$(cat "${OS_ROOT}/_output/local/releases/.commit")"
+            export TAG
+            USE_IMAGES="${IMAGE_PREFIX}-\${component}:${TAG}"
+            export USE_IMAGES
+        fi
+    fi
+	export MAX_IMAGES_BULK_IMPORTED_PER_REPOSITORY="${MAX_IMAGES_BULK_IMPORTED_PER_REPOSITORY:-3}"
+}
+readonly -f os::util::environment::setup_images_vars
diff --git a/openshift-hack/lib/util/find.sh b/openshift-hack/lib/util/find.sh
new file mode 100644
index 00000000000..4ca12d040f9
--- /dev/null
+++ b/openshift-hack/lib/util/find.sh
@@ -0,0 +1,73 @@
+#!/usr/bin/env bash
+
+# This script contains helper functions for finding components
+# in the Origin repository or on the host machine running scripts.
+
+# os::util::find::system_binary determines the absolute path to a
+# system binary, if it exists.
+#
+# Globals:
+#  None
+# Arguments:
+#  - 1: binary name
+# Returns:
+#  - location of the binary
+function os::util::find::system_binary() {
+	local binary_name="$1"
+
+	command -v "${binary_name}"
+}
+readonly -f os::util::find::system_binary
+
+# os::util::find::built_binary determines the absolute path to a
+# built binary for the current platform, if it exists.
+#
+# Globals:
+#  - OS_OUTPUT_BINPATH
+# Arguments:
+#  - 1: binary name
+# Returns:
+#  - location of the binary
+function os::util::find::built_binary() {
+	local binary_name="$1"
+
+	local binary_path; binary_path="${OS_OUTPUT_BINPATH}/$( os::build::host_platform )/${binary_name}"
+	# we need to check that the path leads to a file
+	# as directories also have the executable bit set
+	if [[ -f "${binary_path}" && -x "${binary_path}" ]]; then
+		echo "${binary_path}"
+		return 0
+	else
+		return 1
+	fi
+}
+readonly -f os::util::find::built_binary
+
+# os::util::find::gopath_binary determines the absolute path to a
+# binary installed through the go toolchain, if it exists.
+#
+# Globals:
+#  - GOPATH
+# Arguments:
+#  - 1: binary name
+# Returns:
+#  - location of the binary
+function os::util::find::gopath_binary() {
+	local binary_name="$1"
+
+	local old_ifs="${IFS}"
+	IFS=":"
+	for part in ${GOPATH}; do
+		local binary_path="${part}/bin/${binary_name}"
+		# we need to check that the path leads to a file
+		# as directories also have the executable bit set
+		if [[ -f "${binary_path}" && -x "${binary_path}" ]]; then
+			echo "${binary_path}"
+			IFS="${old_ifs}"
+			return 0
+		fi
+	done
+	IFS="${old_ifs}"
+	return 1
+}
+readonly -f os::util::find::gopath_binary
\ No newline at end of file
diff --git a/openshift-hack/lib/util/misc.sh b/openshift-hack/lib/util/misc.sh
new file mode 100644
index 00000000000..69ea27dc43e
--- /dev/null
+++ b/openshift-hack/lib/util/misc.sh
@@ -0,0 +1,224 @@
+#!/usr/bin/env bash
+#
+# This library holds miscellaneous utility functions. If there begin to be groups of functions in this
+# file that share intent or are thematically similar, they should be split into their own files.
+
+# os::util::describe_return_code describes an exit code
+#
+# Globals:
+#  - OS_SCRIPT_START_TIME
+# Arguments:
+#  - 1: exit code to describe
+# Returns:
+#  None
+function os::util::describe_return_code() {
+	local return_code=$1
+	local message
+    message="$( os::util::repository_relative_path "$0" ) exited with code ${return_code} "
+
+	if [[ -n "${OS_SCRIPT_START_TIME:-}" ]]; then
+		local end_time
+        end_time="$(date +%s)"
+		local elapsed_time
+        elapsed_time="$(( end_time - OS_SCRIPT_START_TIME ))"
+		local formatted_time
+        formatted_time="$( os::util::format_seconds "${elapsed_time}" )"
+		message+="after ${formatted_time}"
+	fi
+
+	if [[ "${return_code}" = "0" ]]; then
+		os::log::info "${message}"
+	else
+		os::log::error "${message}"
+	fi
+}
+readonly -f os::util::describe_return_code
+
+# os::util::install_describe_return_code installs the return code describer for the EXIT trap
+# If the EXIT trap is not initialized, installing this plugin will initialize it.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  - export OS_DESCRIBE_RETURN_CODE
+#  - export OS_SCRIPT_START_TIME
+function os::util::install_describe_return_code() {
+	export OS_DESCRIBE_RETURN_CODE="true"
+	OS_SCRIPT_START_TIME="$( date +%s )"; export OS_SCRIPT_START_TIME
+	os::util::trap::init_exit
+}
+readonly -f os::util::install_describe_return_code
+
+# OS_ORIGINAL_WD is the original working directory the script sourcing this utility file was called
+# from. This is an important directory as if $0 is a relative path, we cannot use the following path
+# utility without knowing from where $0 is relative.
+if [[ -z "${OS_ORIGINAL_WD:-}" ]]; then
+	# since this could be sourced in a context where the utilities are already loaded,
+	# we want to ensure that this is re-entrant, so we only set $OS_ORIGINAL_WD if it
+	# is not set already
+	OS_ORIGINAL_WD="$( pwd )"
+	readonly OS_ORIGINAL_WD
+	export OS_ORIGINAL_WD
+fi
+
+# os::util::repository_relative_path returns the relative path from the $OS_ROOT directory to the
+# given file, if the file is inside of the $OS_ROOT directory. If the file is outside of $OS_ROOT,
+# this function will return the absolute path to the file
+#
+# Globals:
+#  - OS_ROOT
+# Arguments:
+#  - 1: the path to relativize
+# Returns:
+#  None
+function os::util::repository_relative_path() {
+	local filename=$1
+	local directory; directory="$( dirname "${filename}" )"
+	filename="$( basename "${filename}" )"
+
+	if [[ "${directory}" != "${OS_ROOT}"* ]]; then
+		pushd "${OS_ORIGINAL_WD}" >/dev/null 2>&1 || exit 1
+		directory="$( os::util::absolute_path "${directory}" )"
+		popd >/dev/null 2>&1 || exit 1
+	fi
+
+	directory="${directory##*${OS_ROOT}/}"
+
+	echo "${directory}/${filename}"
+}
+readonly -f os::util::repository_relative_path
+
+# os::util::format_seconds formats a duration of time in seconds to print in HHh MMm SSs
+#
+# Globals:
+#  None
+# Arguments:
+#  - 1: time in seconds to format
+# Return:
+#  None
+function os::util::format_seconds() {
+	local raw_seconds=$1
+
+	local hours minutes seconds
+	(( hours=raw_seconds/3600 ))
+	(( minutes=(raw_seconds%3600)/60 ))
+	(( seconds=raw_seconds%60 ))
+
+	printf '%02dh %02dm %02ds' "${hours}" "${minutes}" "${seconds}"
+}
+readonly -f os::util::format_seconds
+
+# os::util::sed attempts to make our Bash scripts agnostic to the platform
+# on which they run `sed` by glossing over a discrepancy in flag use in GNU.
+#
+# Globals:
+#  None
+# Arguments:
+#  - all: arguments to pass to `sed -i`
+# Return:
+#  None
+function os::util::sed() {
+	local sudo="${USE_SUDO:+sudo}"
+	if LANG=C sed --help 2>&1 | grep -q "GNU sed"; then
+		${sudo} sed -i'' "$@"
+	else
+		${sudo} sed -i '' "$@"
+	fi
+}
+readonly -f os::util::sed
+
+# os::util::base64decode attempts to make our Bash scripts agnostic to the platform
+# on which they run `base64decode` by glossing over a discrepancy in flag use in GNU.
+#
+# Globals:
+#  None
+# Arguments:
+#  - all: arguments to pass to `base64decode`
+# Return:
+#  None
+function os::util::base64decode() {
+	if [[ "$(go env GOHOSTOS)" == "darwin" ]]; then
+		base64 -D "$@"
+	else
+		base64 -d "$@"
+	fi
+}
+readonly -f os::util::base64decode
+
+# os::util::curl_etcd sends a request to the backing etcd store for the master.
+# We use the administrative client cert and key for access and re-encode them
+# as necessary for OSX clients.
+#
+# Globals:
+#  MASTER_CONFIG_DIR
+#  API_SCHEME
+#  API_HOST
+#  ETCD_PORT
+# Arguments:
+#  - 1: etcd-relative URL to curl, with leading slash
+# Returns:
+#  None
+function os::util::curl_etcd() {
+	local url="$1"
+	local full_url="${API_SCHEME}://${API_HOST}:${ETCD_PORT}${url}"
+
+	local etcd_client_cert="${MASTER_CONFIG_DIR}/master.etcd-client.crt"
+	local etcd_client_key="${MASTER_CONFIG_DIR}/master.etcd-client.key"
+	local ca_bundle="${MASTER_CONFIG_DIR}/ca-bundle.crt"
+
+	if curl -V | grep -q 'SecureTransport'; then
+		# on newer OSX `curl` implementations, SSL is not used and client certs
+		# and keys are expected to be encoded in P12 format instead of PEM format,
+		# so we need to convert the secrets that the server wrote if we haven't
+		# already done so
+		local etcd_client_cert_p12="${MASTER_CONFIG_DIR}/master.etcd-client.crt.p12"
+		local etcd_client_cert_p12_password="${CURL_CERT_P12_PASSWORD:-'password'}"
+		if [[ ! -f "${etcd_client_cert_p12}" ]]; then
+			openssl pkcs12 -export                        \
+			               -in "${etcd_client_cert}"      \
+			               -inkey "${etcd_client_key}"    \
+			               -out "${etcd_client_cert_p12}" \
+			               -password "pass:${etcd_client_cert_p12_password}"
+		fi
+
+		curl --fail --silent --cacert "${ca_bundle}" \
+		     --cert "${etcd_client_cert_p12}:${etcd_client_cert_p12_password}" "${full_url}"
+	else
+		curl --fail --silent --cacert "${ca_bundle}" \
+		     --cert "${etcd_client_cert}" --key "${etcd_client_key}" "${full_url}"
+	fi
+}
+
+# os::util::ensure_tmpfs ensures that the target dir is mounted on tmpfs
+#
+# Globals:
+#  OS_TMPFS_REQUIRED
+# Arguments:
+#  - 1: target to check
+# Returns:
+#  None
+function os::util::ensure_tmpfs() {
+	if [[ -z "${OS_TMPFS_REQUIRED:-}" ]]; then
+		return 0
+	fi
+
+	local target="$1"
+	if [[ ! -d "${target}" ]]; then
+		os::log::fatal "Target dir ${target} does not exist, cannot perform fstype check."
+	fi
+
+	os::log::debug "Filesystem information:
+$( df -h -T )"
+
+	os::log::debug "Mount information:
+$( findmnt --all )"
+
+	local fstype
+	fstype="$( df --output=fstype "${target}" | tail -n 1 )"
+	if [[ "${fstype}" != "tmpfs" ]]; then
+		local message="Expected \`${target}\` to be mounted on \`tmpfs\` but found \`${fstype}\` instead."
+		os::log::fatal "${message}"
+	fi
+}
diff --git a/openshift-hack/lib/util/text.sh b/openshift-hack/lib/util/text.sh
new file mode 100644
index 00000000000..708a47251cb
--- /dev/null
+++ b/openshift-hack/lib/util/text.sh
@@ -0,0 +1,164 @@
+#!/usr/bin/env bash
+
+# This file contains helpful aliases for manipulating the output text to the terminal as
+# well as functions for one-command augmented printing.
+
+# os::text::reset resets the terminal output to default if it is called in a TTY
+function os::text::reset() {
+	if os::text::internal::is_tty; then
+		tput sgr0
+	fi
+}
+readonly -f os::text::reset
+
+# os::text::bold sets the terminal output to bold text if it is called in a TTY
+function os::text::bold() {
+	if os::text::internal::is_tty; then
+		tput bold
+	fi
+}
+readonly -f os::text::bold
+
+# os::text::red sets the terminal output to red text if it is called in a TTY
+function os::text::red() {
+	if os::text::internal::is_tty; then
+		tput setaf 1
+	fi
+}
+readonly -f os::text::red
+
+# os::text::green sets the terminal output to green text if it is called in a TTY
+function os::text::green() {
+	if os::text::internal::is_tty; then
+		tput setaf 2
+	fi
+}
+readonly -f os::text::green
+
+# os::text::blue sets the terminal output to blue text if it is called in a TTY
+function os::text::blue() {
+	if os::text::internal::is_tty; then
+		tput setaf 4
+	fi
+}
+readonly -f os::text::blue
+
+# os::text::yellow sets the terminal output to yellow text if it is called in a TTY
+function os::text::yellow() {
+	if os::text::internal::is_tty; then
+		tput setaf 11
+	fi
+}
+readonly -f os::text::yellow
+
+# os::text::clear_last_line clears the text from the last line of output to the
+# terminal and leaves the cursor on that line to allow for overwriting that text
+# if it is called in a TTY
+function os::text::clear_last_line() {
+	if os::text::internal::is_tty; then
+		tput cuu 1
+		tput el
+	fi
+}
+readonly -f os::text::clear_last_line
+
+# os::text::clear_string attempts to clear the entirety of a string from the terminal.
+# If the string contains literal tabs or other characters that take up more than one
+# character space in output, or if the window size is changed before this function
+# is called, it will not function correctly.
+# No action is taken if this is called outside of a TTY
+function os::text::clear_string() {
+    local -r string="$1"
+    if os::text::internal::is_tty; then
+        echo "${string}" | while read -r line; do
+            # num_lines is the number of terminal lines this one line of output
+            # would have taken up with the current terminal width in columns
+            local num_lines=$(( ${#line} / $( tput cols ) ))
+            for (( i = 0; i <= num_lines; i++ )); do
+                os::text::clear_last_line
+            done
+        done
+    fi
+}
+
+# os::text::internal::is_tty determines if we are outputting to a TTY
+function os::text::internal::is_tty() {
+	[[ -t 1 && -n "${TERM:-}" ]]
+}
+readonly -f os::text::internal::is_tty
+
+# os::text::print_bold prints all input in bold text
+function os::text::print_bold() {
+	os::text::bold
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_bold
+
+# os::text::print_red prints all input in red text
+function os::text::print_red() {
+	os::text::red
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_red
+
+# os::text::print_red_bold prints all input in bold red text
+function os::text::print_red_bold() {
+	os::text::red
+	os::text::bold
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_red_bold
+
+# os::text::print_green prints all input in green text
+function os::text::print_green() {
+	os::text::green
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_green
+
+# os::text::print_green_bold prints all input in bold green text
+function os::text::print_green_bold() {
+	os::text::green
+	os::text::bold
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_green_bold
+
+# os::text::print_blue prints all input in blue text
+function os::text::print_blue() {
+	os::text::blue
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_blue
+
+# os::text::print_blue_bold prints all input in bold blue text
+function os::text::print_blue_bold() {
+	os::text::blue
+	os::text::bold
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_blue_bold
+
+# os::text::print_yellow prints all input in yellow text
+function os::text::print_yellow() {
+	os::text::yellow
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_yellow
+
+# os::text::print_yellow_bold prints all input in bold yellow text
+function os::text::print_yellow_bold() {
+	os::text::yellow
+	os::text::bold
+	echo "${*}"
+	os::text::reset
+}
+readonly -f os::text::print_yellow_bold
diff --git a/openshift-hack/lib/util/trap.sh b/openshift-hack/lib/util/trap.sh
new file mode 100644
index 00000000000..f76d6bfe404
--- /dev/null
+++ b/openshift-hack/lib/util/trap.sh
@@ -0,0 +1,99 @@
+#!/usr/bin/env bash
+#
+# This library defines the trap handlers for the ERR and EXIT signals. Any new handler for these signals
+# must be added to these handlers and activated by the environment variable mechanism that the rest use.
+# These functions ensure that no handler can ever alter the exit code that was emitted by a command
+# in a test script.
+
+# os::util::trap::init_err initializes the privileged handler for the ERR signal if it hasn't
+# been registered already. This will overwrite any other handlers registered on the signal.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::util::trap::init_err() {
+    if ! trap -p ERR | grep -q 'os::util::trap::err_handler'; then
+        trap 'os::util::trap::err_handler;' ERR
+    fi
+}
+readonly -f os::util::trap::init_err
+
+# os::util::trap::init_exit initializes the privileged handler for the EXIT signal if it hasn't
+# been registered already. This will overwrite any other handlers registered on the signal.
+#
+# Globals:
+#  None
+# Arguments:
+#  None
+# Returns:
+#  None
+function os::util::trap::init_exit() {
+    if ! trap -p EXIT | grep -q 'os::util::trap::exit_handler'; then
+        trap 'os::util::trap::exit_handler;' EXIT
+    fi
+}
+readonly -f os::util::trap::init_exit
+
+# os::util::trap::err_handler is the handler for the ERR signal.
+#
+# Globals:
+#  - OS_TRAP_DEBUG
+#  - OS_USE_STACKTRACE
+# Arguments:
+#  None
+# Returns:
+#  - returns original return code, allows privileged handler to exit if necessary
+function os::util::trap::err_handler() {
+    local -r return_code=$?
+    local -r last_command="${BASH_COMMAND}"
+
+    if set +o | grep -q '\-o errexit'; then
+        local -r errexit_set=true
+    fi
+
+    if [[ "${OS_TRAP_DEBUG:-}" = "true" ]]; then
+        echo "[DEBUG] Error handler executing with return code \`${return_code}\`, last command \`${last_command}\`, and errexit set \`${errexit_set:-}\`"
+    fi
+
+    if [[ "${OS_USE_STACKTRACE:-}" = "true" ]]; then
+        # the OpenShift stacktrace function is treated as a privileged handler for this signal
+        # and is therefore allowed to run outside of a subshell in order to allow it to `exit`
+        # if necessary
+        os::log::stacktrace::print "${return_code}" "${last_command}" "${errexit_set:-}"
+    fi
+
+    return "${return_code}"
+}
+readonly -f os::util::trap::err_handler
+
+# os::util::trap::exit_handler is the handler for the EXIT signal.
+#
+# Globals:
+#  - OS_TRAP_DEBUG
+#  - OS_DESCRIBE_RETURN_CODE
+# Arguments:
+#  None
+# Returns:
+#  - original exit code of the script that exited
+function os::util::trap::exit_handler() {
+    local -r return_code=$?
+
+    # we do not want these traps to be able to trigger more errors, we can let them fail silently
+    set +o errexit
+
+    if [[ "${OS_TRAP_DEBUG:-}" = "true" ]]; then
+        echo "[DEBUG] Exit handler executing with return code \`${return_code}\`"
+    fi
+
+    # the following envars selectively enable optional exit traps, all of which are run inside of
+    # a subshell in order to sandbox them and not allow them to influence how this script will exit
+    if [[ "${OS_DESCRIBE_RETURN_CODE:-}" = "true" ]]; then
+        ( os::util::describe_return_code "${return_code}" )
+    fi
+
+    exit "${return_code}"
+}
+readonly -f os::util::trap::exit_handler
diff --git a/openshift-hack/rebase.sh b/openshift-hack/rebase.sh
new file mode 100755
index 00000000000..70ea50b38ba
--- /dev/null
+++ b/openshift-hack/rebase.sh
@@ -0,0 +1,175 @@
+#!/bin/bash
+
+# READ FIRST BEFORE USING THIS SCRIPT
+#
+# This script requires jq, git, podman and bash to work properly (dependencies are checked for you).
+# The Github CLI "gh" is optional, but convenient to create a pull request automatically at the end.
+#
+# This script generates a git remote structure described in:
+# https://github.com/openshift/kubernetes/blob/master/REBASE.openshift.md#preparing-the-local-repo-clone
+# Please check if you have configured the correct remotes, otherwise the script will fail.
+#
+# The usage is described in /Rebase.openshift.md.
+
+# validate input args --k8s-tag=v1.21.2 --openshift-release=release-4.8 --bugzilla-id=2003027
+k8s_tag=""
+openshift_release=""
+bugzilla_id=""
+
+usage() {
+  echo "Available arguments:"
+  echo "  --k8s-tag            (required) Example: --k8s-tag=v1.21.2"
+  echo "  --openshift-release  (required) Example: --openshift-release=release-4.8"
+  echo "  --bugzilla-id        (optional) creates new PR against openshift/kubernetes:${openshift-release}: Example: --bugzilla-id=2003027"
+}
+
+for i in "$@"; do
+  case $i in
+  --k8s-tag=*)
+    k8s_tag="${i#*=}"
+    shift
+    ;;
+  --openshift-release=*)
+    openshift_release="${i#*=}"
+    shift
+    ;;
+  --bugzilla-id=*)
+    bugzilla_id="${i#*=}"
+    shift
+    ;;
+  *)
+    usage
+    exit 1
+    ;;
+  esac
+done
+
+if [ -z "${k8s_tag}" ]; then
+  echo "Required argument missing: --k8s-tag"
+  echo ""
+  usage
+  exit 1
+fi
+
+if [ -z "${openshift_release}" ]; then
+  echo "Required argument missing: --openshift-release"
+  echo ""
+  usage
+  exit 1
+fi
+
+echo "Processed arguments are:"
+echo "--k8s_tag=${k8s_tag}"
+echo "--openshift_release=${openshift_release}"
+echo "--bugzilla_id=${bugzilla_id}"
+
+# prerequisites (check git, podman, ... is present)
+if ! command -v git &>/dev/null; then
+  echo "git not installed, exiting"
+  exit 1
+fi
+
+if ! command -v jq &>/dev/null; then
+  echo "jq not installed, exiting"
+  exit 1
+fi
+
+if ! command -v podman &>/dev/null; then
+  echo "podman not installed, exiting"
+  exit 1
+fi
+
+# make sure we're in "kubernetes" dir
+if [[ $(basename "$PWD") != "kubernetes" ]]; then
+  echo "Not in kubernetes dir, exiting"
+  exit 1
+fi
+
+origin=$(git remote get-url origin)
+if [[ "$origin" =~ .*kubernetes/kubernetes.* || "$origin" =~ .*openshift/kubernetes.* ]]; then
+  echo "cannot rebase against k/k or o/k! found: ${origin}, exiting"
+  exit 1
+fi
+
+# fetch remote https://github.com/kubernetes/kubernetes
+git remote add upstream git@github.com:kubernetes/kubernetes.git
+git fetch upstream --tags -f
+# fetch remote https://github.com/openshift/kubernetes
+git remote add openshift git@github.com:openshift/kubernetes.git
+git fetch openshift
+
+#git checkout --track "openshift/$openshift_release"
+git pull openshift "$openshift_release"
+
+git merge "$k8s_tag"
+# shellcheck disable=SC2181
+if [ $? -eq 0 ]; then
+  echo "No conflicts detected. Automatic merge looks to have succeeded"
+else
+  # commit conflicts
+  git commit -a
+  # resolve conflicts
+  git status
+  # TODO(tjungblu): we follow-up with a more automated approach:
+  # - 2/3s of conflicts stem from go.mod/sum, which can be resolved deterministically
+  # - the large majority of the remainder are vendor/generation conflicts
+  # - only very few cases require manual intervention due to conflicting business logic
+  echo "Resolve conflicts manually in another terminal, only then continue"
+
+  # wait for user interaction
+  read -n 1 -s -r -p "PRESS ANY KEY TO CONTINUE"
+
+  # TODO(tjungblu): verify that the conflicts have been resolved
+  git commit -am "UPSTREAM: <drop>: manually resolve conflicts"
+fi
+
+# openshift-hack/images/hyperkube/Dockerfile.rhel still has FROM pointing to old tag
+# we need to remove the prefix "v" from the $k8s_tag to stay compatible
+sed -i -E "s/(io.openshift.build.versions=\"kubernetes=)(1.[1-9]+.[1-9]+)/\1${k8s_tag:1}/" openshift-hack/images/hyperkube/Dockerfile.rhel
+go_mod_go_ver=$(grep -E 'go 1\.[1-9][0-9]?' go.mod | sed -E 's/go (1\.[1-9][0-9]?)/\1/')
+tag="rhel-8-release-golang-${go_mod_go_ver}-openshift-${openshift_release#release-}"
+
+# update openshift go.mod dependencies
+sed -i -E "/=>/! s/(\tgithub.com\/openshift\/[a-z|-]+) (.*)$/\1 $openshift_release/" go.mod
+
+echo "> go mod tidy && hack/update-vendor.sh"
+podman run -it --rm -v "$(pwd):/go/k8s.io/kubernetes:Z" \
+  --workdir=/go/k8s.io/kubernetes \
+  "registry.ci.openshift.org/openshift/release:$tag" \
+  go mod tidy && hack/update-vendor.sh
+
+# shellcheck disable=SC2181
+if [ $? -ne 0 ]; then
+  echo "updating the vendor folder failed, is any dependency missing?"
+  exit 1
+fi
+
+podman run -it --rm -v "$(pwd):/go/k8s.io/kubernetes:Z" \
+  --workdir=/go/k8s.io/kubernetes \
+  "registry.ci.openshift.org/openshift/release:$tag" \
+  make update OS_RUN_WITHOUT_DOCKER=yes
+
+git add -A
+git commit -m "UPSTREAM: <drop>: hack/update-vendor.sh, make update and update image"
+
+remote_branch="rebase-$k8s_tag"
+git push origin "$openshift_release:$remote_branch"
+
+XY=$(echo "$k8s_tag" | sed -E "s/v(1\.[0-9]+)\.[0-9]+/\1/")
+ver=$(echo "$k8s_tag" | sed "s/\.//g")
+link="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-$XY.md#$ver"
+if [ -n "${bugzilla_id}" ]; then
+  if command -v gh &>/dev/null; then
+    XY=$(echo "$k8s_tag" | sed -E "s/v(1\.[0-9]+)\.[0-9]+/\1/")
+    ver=$(echo "$k8s_tag" | sed "s/\.//g")
+    link="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-$XY.md#$ver"
+
+    # opens a web browser, because we can't properly create PRs against remote repositories with the GH CLI (yet):
+    # https://github.com/cli/cli/issues/2691
+    gh pr create \
+      --title "Bug $bugzilla_id: Rebase $k8s_tag" \
+      --body "CHANGELOG $link" \
+      --web
+
+  fi
+fi
diff --git a/openshift-hack/sysctls/50-kubelet.conf b/openshift-hack/sysctls/50-kubelet.conf
new file mode 100644
index 00000000000..3a4d5a7b1af
--- /dev/null
+++ b/openshift-hack/sysctls/50-kubelet.conf
@@ -0,0 +1,6 @@
+kernel.keys.root_maxbytes=25000000
+kernel.keys.root_maxkeys=1000000
+kernel.panic=10
+kernel.panic_on_oops=1
+vm.overcommit_memory=1
+vm.panic_on_oom=0
diff --git a/openshift-hack/test-go.sh b/openshift-hack/test-go.sh
new file mode 100755
index 00000000000..30793e2b082
--- /dev/null
+++ b/openshift-hack/test-go.sh
@@ -0,0 +1,16 @@
+#!/usr/bin/env bash
+
+# shellcheck source=openshift-hack/lib/init.sh
+source "$(dirname "${BASH_SOURCE[0]}")/lib/init.sh"
+
+ARTIFACTS="${ARTIFACTS:-/tmp/artifacts}"
+mkdir -p "${ARTIFACTS}"
+
+export KUBERNETES_SERVICE_HOST=
+export KUBE_JUNIT_REPORT_DIR="${ARTIFACTS}"
+export KUBE_KEEP_VERBOSE_TEST_OUTPUT=y
+export KUBE_RACE=-race
+export KUBE_TEST_ARGS='-p 8'
+export KUBE_TIMEOUT='--timeout=360s'
+
+make test
diff --git a/openshift-hack/test-integration.sh b/openshift-hack/test-integration.sh
new file mode 100755
index 00000000000..93c3ea902b0
--- /dev/null
+++ b/openshift-hack/test-integration.sh
@@ -0,0 +1,20 @@
+#!/usr/bin/env bash
+
+# shellcheck source=openshift-hack/lib/init.sh
+source "$(dirname "${BASH_SOURCE[0]}")/lib/init.sh"
+
+./hack/install-etcd.sh
+PATH="${OS_ROOT}/third_party/etcd:${PATH}"
+
+ARTIFACTS="${ARTIFACTS:-/tmp/artifacts}"
+mkdir -p "${ARTIFACTS}"
+
+export KUBERNETES_SERVICE_HOST=
+export KUBE_JUNIT_REPORT_DIR="${ARTIFACTS}"
+export KUBE_KEEP_VERBOSE_TEST_OUTPUT=y
+export KUBE_RACE=-race
+export KUBE_TEST_ARGS='-p 8'
+export LOG_LEVEL=4
+export PATH
+
+make test-integration
diff --git a/openshift-hack/test-kubernetes-e2e.sh b/openshift-hack/test-kubernetes-e2e.sh
new file mode 100755
index 00000000000..9292de265a0
--- /dev/null
+++ b/openshift-hack/test-kubernetes-e2e.sh
@@ -0,0 +1,88 @@
+#!/bin/bash
+
+set -o nounset
+set -o errexit
+set -o pipefail
+
+# This script is executes kubernetes e2e tests against an openshift
+# cluster. It is intended to be copied to the kubernetes-tests image
+# for use in CI and should have no dependencies beyond oc, kubectl and
+# k8s-e2e.test.
+
+# Identify the platform under test to allow skipping tests that are
+# not compatible.
+CLUSTER_TYPE="${CLUSTER_TYPE:-gcp}"
+case "${CLUSTER_TYPE}" in
+  gcp)
+    # gce is used as a platform label instead of gcp
+    PLATFORM=gce
+    ;;
+  *)
+    PLATFORM="${CLUSTER_TYPE}"
+    ;;
+esac
+
+# openshift-tests will check the cluster's network configuration and
+# automatically skip any incompatible tests. We have to do that manually
+# here.
+NETWORK_SKIPS="\[Skipped:Network/OpenShiftSDN\]|\[Feature:Networking-IPv6\]|\[Feature:IPv6DualStack.*\]|\[Feature:SCTPConnectivity\]"
+
+# Support serial and parallel test suites
+TEST_SUITE="${TEST_SUITE:-parallel}"
+COMMON_SKIPS="\[Slow\]|\[Disruptive\]|\[Flaky\]|\[Disabled:.+\]|\[Skipped:${PLATFORM}\]|${NETWORK_SKIPS}"
+case "${TEST_SUITE}" in
+serial)
+  DEFAULT_TEST_ARGS="-focus=\[Serial\] -skip=${COMMON_SKIPS}"
+  NODES=1
+  ;;
+parallel)
+  DEFAULT_TEST_ARGS="-skip=\[Serial\]|${COMMON_SKIPS}"
+  # Use the same number of nodes - 30 - as specified for the parallel
+  # suite defined in origin.
+  NODES=${NODES:-30}
+  ;;
+*)
+  echo >&2 "Unsupported test suite '${TEST_SUITE}'"
+  exit 1
+  ;;
+esac
+
+# Set KUBE_E2E_TEST_ARGS to configure test arguments like
+# -skip and -focus.
+KUBE_E2E_TEST_ARGS="${KUBE_E2E_TEST_ARGS:-${DEFAULT_TEST_ARGS}}"
+
+# k8s-e2e.test and ginkgo are expected to be in the path in
+# CI. Outside of CI, ensure k8s-e2e.test and ginkgo are built and
+# available in PATH.
+if ! which k8s-e2e.test &> /dev/null; then
+  make WHAT=vendor/github.com/onsi/ginkgo/v2/ginkgo
+  make WHAT=openshift-hack/e2e/k8s-e2e.test
+  ROOT_PATH="$(cd "$(dirname "${BASH_SOURCE[0]}")/.."; pwd -P)"
+  PATH="${ROOT_PATH}/_output/local/bin/$(go env GOHOSTOS)/$(go env GOARCH):${PATH}"
+  export PATH
+fi
+
+# Execute OpenShift prerequisites
+# Disable container security
+oc adm policy add-scc-to-group privileged system:authenticated system:serviceaccounts
+oc adm policy add-scc-to-group anyuid system:authenticated system:serviceaccounts
+unschedulable="$( ( oc get nodes -o name -l 'node-role.kubernetes.io/master'; ) | wc -l )"
+
+test_report_dir="${ARTIFACTS:-/tmp/artifacts}"
+mkdir -p "${test_report_dir}"
+
+# Retrieve the hostname of the server to enable kubectl testing
+SERVER=
+SERVER="$( kubectl config view | grep server | head -n 1 | awk '{print $2}' )"
+
+# shellcheck disable=SC2086
+ginkgo \
+  --flake-attempts=3 \
+  --timeout="24h" \
+  --output-interceptor-mode=none \
+  -nodes "${NODES}" -no-color ${KUBE_E2E_TEST_ARGS} \
+  "$( which k8s-e2e.test )" -- \
+  -report-dir "${test_report_dir}" \
+  -host "${SERVER}" \
+  -allowed-not-ready-nodes ${unschedulable} \
+  2>&1 | tee -a "${test_report_dir}/k8s-e2e.log"
diff --git a/openshift-hack/update-kubensenter.sh b/openshift-hack/update-kubensenter.sh
new file mode 100755
index 00000000000..a7ca2693964
--- /dev/null
+++ b/openshift-hack/update-kubensenter.sh
@@ -0,0 +1,139 @@
+#!/usr/bin/env bash
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+KUBE_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
+source "$KUBE_ROOT/hack/lib/init.sh"
+
+# Convert a path relative to $KUBE_ROOT to a real path
+localpath() {
+    realpath "$KUBE_ROOT/$1"
+}
+
+# Configuration for fetching this file, relative to this repository root
+ENVFILE=openshift-hack/kubensenter.env
+
+# The source of the file, relative to the remote repository root
+SOURCE=utils/kubensenter/kubensenter
+
+# The destination of the file, relative to this repository root
+DESTINATION=openshift-hack/images/hyperkube/kubensenter
+
+usage() {
+    source_env
+    echo "Usage:"
+    echo "  $0 [--to-latest]"
+    echo
+    echo "Updates the local copy of $DESTINATION as configured in $ENVFILE:"
+    echo "  REPO: $REPO"
+    echo "  COMMIT: $COMMIT"
+    echo
+    echo "Options:"
+    echo "  --to-latest (or env UPDATE_TO_LATEST=1)"
+    echo "    Update $ENVFILE to the latest commit or tag in $REPO configured by the TARGET entry"
+    echo "    (currently \"$TARGET\"), and synchronize to the updated commit."
+    echo "    - If TARGET resolves to a branch, pin to the latest commit hash from that branch"
+    echo "    - If TARGET resolves to a tag, pin to the latest tag that matches that pattern"
+    echo "    - TARGET may be a glob-like expression such as \"v1.1.*\" that would match any of the following:"
+    echo "        v1.1.0 v1.1.3 v1.1.22-rc1"
+    exit 1
+}
+
+source_env() {
+    source "$(localpath "$ENVFILE")"
+    # Intentionally global scope:
+    REPO=${REPO:-"github.com/containers/kubensmnt"}
+    COMMIT=${COMMIT:-"main"}
+    TARGET=${TARGET:-"main"}
+}
+
+edit_envfile() {
+    local envfile=$1
+    local refname=$2
+
+    # Shell-quote refname in case it contains any shell-special characters
+    local newcommit=$(printf 'COMMIT=%q' "$refname")
+    if [[ $# -gt 2 ]]; then
+        shift 2
+        # Add the comment suffix
+        newcommit="$newcommit # $*"
+    fi
+
+    local patch
+    patch=$(printf "%q" "$newcommit")
+    # Note: Using ':' since it is not a valid tag character according to git-check-ref-format(1)
+    sed -i "s:^COMMIT=.*:$patch:" "$envfile"
+}
+
+update_env() {
+    local repouri latest refhash reftype refname
+    source_env
+    repouri=https://$REPO.git
+    echo "Updating to latest $TARGET from $repouri"
+
+    latest=$(git \
+                   -c "versionsort.suffix=-alpha" \
+                   -c "versionsort.suffix=-beta" \
+                   -c "versionsort.suffix=-rc" \
+                 ls-remote \
+                   --heads --tags \
+                   --sort='-version:refname' \
+                   "$repouri" "$TARGET" \
+             | head -n 1)
+    if [[ -z $latest ]]; then
+        echo "ERROR: No matching ref found for $TARGET"
+        return 1
+    fi
+    refhash=$(cut -f1 <<<"$latest")
+    reftype=$(cut -d/ -f2 <<<"$latest")
+    refname=$(cut -d/ -f3 <<<"$latest")
+
+    if [[ $reftype == "tags" ]]; then
+        echo "  Latest tag is $refname ($refhash)"
+        edit_envfile "$ENVFILE" "$refname" "($refhash)"
+    else
+        echo "  Latest on branch $refname is $refhash"
+        edit_envfile "$ENVFILE" "$refhash"
+    fi
+}
+
+do_fetch() {
+    source_env
+    local repohost reponame uri
+    repohost=$(cut -d/ -f1 <<<"$REPO")
+    reponame=${REPO#$repohost/}
+    case $repohost in
+        github.com)
+            uri=https://raw.githubusercontent.com/$reponame/$COMMIT/$SOURCE
+            ;;
+        *)
+            echo "No support for repositories hosted on $repohost"
+            return 2
+            ;;
+    esac
+
+    echo "Fetching $DESTINATION from $uri"
+    curl -fsLo "$(localpath "$DESTINATION")" "$uri"
+}
+
+main() {
+    local to_latest=${UPDATE_TO_LATEST:-}
+    if [[ $# -gt 0 ]]; then
+        if [[ $1 == "--help" || $1 == "-h" ]]; then
+            usage
+        elif [[ $1 == "--to-latest" ]]; then
+            to_latest=1
+        fi
+    fi
+
+    if [[ $to_latest ]]; then
+        update_env
+    fi
+
+    do_fetch
+}
+
+# bash modulino
+[[ "${BASH_SOURCE[0]}" == "$0" ]] && main "$@"
diff --git a/openshift-hack/update-test-annotations.sh b/openshift-hack/update-test-annotations.sh
new file mode 100755
index 00000000000..244fdc67e54
--- /dev/null
+++ b/openshift-hack/update-test-annotations.sh
@@ -0,0 +1,13 @@
+#!/usr/bin/env bash
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+KUBE_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
+source "${KUBE_ROOT}/hack/lib/init.sh"
+
+kube::golang::setup_env
+
+# Update e2e test annotations that indicate openshift compatibility
+GO111MODULE=on go generate -mod vendor ./openshift-hack/e2e
diff --git a/openshift-hack/verify-kubensenter.sh b/openshift-hack/verify-kubensenter.sh
new file mode 100755
index 00000000000..07093f09809
--- /dev/null
+++ b/openshift-hack/verify-kubensenter.sh
@@ -0,0 +1,12 @@
+#!/usr/bin/env bash
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+KUBE_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
+source "${KUBE_ROOT}/hack/lib/init.sh"
+
+# Update kubensenter and error if a change is detected
+"${KUBE_ROOT}"/hack/update-kubensenter.sh
+git diff --quiet "${KUBE_ROOT}/openshift-hack/images/hyperkube/kubensenter"
diff --git a/openshift-hack/verify-test-annotations.sh b/openshift-hack/verify-test-annotations.sh
new file mode 100755
index 00000000000..a60e30ba7bc
--- /dev/null
+++ b/openshift-hack/verify-test-annotations.sh
@@ -0,0 +1,12 @@
+#!/usr/bin/env bash
+
+set -o errexit
+set -o nounset
+set -o pipefail
+
+KUBE_ROOT=$(dirname "${BASH_SOURCE[0]}")/..
+source "${KUBE_ROOT}/hack/lib/init.sh"
+
+# Verify e2e test annotations that indicate openshift compatibility
+"${KUBE_ROOT}"/hack/update-test-annotations.sh
+git diff --quiet "${KUBE_ROOT}/openshift-hack/e2e/annotate/generated/"
diff --git a/openshift-hack/verify.sh b/openshift-hack/verify.sh
new file mode 100755
index 00000000000..9361e8f4fae
--- /dev/null
+++ b/openshift-hack/verify.sh
@@ -0,0 +1,26 @@
+#!/usr/bin/env bash
+
+# shellcheck source=openshift-hack/lib/init.sh
+source "$(dirname "${BASH_SOURCE[0]}")/lib/init.sh"
+
+# Required for openapi verification
+PATH="$(pwd)/third_party/etcd:${PATH}"
+
+# Attempt to verify without docker if it is not available.
+OS_RUN_WITHOUT_DOCKER=
+if ! which docker &> /dev/null; then
+  os::log::warning "docker not available, attempting to run verify without it"
+  OS_RUN_WITHOUT_DOCKER=y
+
+  # Without docker, shellcheck may need to be installed.
+  PATH="$( os::deps::path_with_shellcheck )"
+fi
+export OS_RUN_WITHOUT_DOCKER
+
+export PATH
+
+ARTIFACTS="${ARTIFACTS:-/tmp/artifacts}"
+mkdir -p "${ARTIFACTS}"
+export KUBE_JUNIT_REPORT_DIR="${ARTIFACTS}"
+
+make verify
diff --git a/openshift.spec b/openshift.spec
new file mode 100644
index 00000000000..35d1ecb6320
--- /dev/null
+++ b/openshift.spec
@@ -0,0 +1,137 @@
+#debuginfo not supported with Go
+%global debug_package %{nil}
+# modifying the Go binaries breaks the DWARF debugging
+%global __os_install_post %{_rpmconfigdir}/brp-compress
+
+%global gopath      %{_datadir}/gocode
+%global import_path k8s.io/kubernetes
+
+%global golang_version 1.15
+
+%{!?commit:
+# DO NOT MODIFY: the value on the line below is sed-like replaced by openshift/doozer
+%global commit 86b5e46426ba828f49195af21c56f7c6674b48f7
+}
+%global shortcommit %(c=%{commit}; echo ${c:0:7})
+# DO NOT MODIFY: the value on the line below is sed-like replaced by openshift/doozer
+%{!?os_git_vars:
+%global os_git_vars OS_GIT_VERSION='' OS_GIT_COMMIT='' OS_GIT_MAJOR='' OS_GIT_MINOR='' OS_GIT_TREE_STATE=''
+}
+
+%if 0%{?skip_build}
+%global do_build 0
+%else
+%global do_build 1
+%endif
+%if 0%{?skip_prep}
+%global do_prep 0
+%else
+%global do_prep 1
+%endif
+%if 0%{?skip_dist}
+%global package_dist %{nil}
+%else
+%global package_dist %{dist}
+%endif
+
+%{!?version: %global version 4.0.0}
+%{!?release: %global release 1}
+
+Name:           openshift
+Version:        %{version}
+Release:        %{release}%{package_dist}
+Summary:        Open Source Container Management by Red Hat
+License:        ASL 2.0
+URL:            https://%{import_path}
+
+# If go_arches not defined fall through to implicit golang archs
+%if 0%{?go_arches:1}
+ExclusiveArch:  %{go_arches}
+%else
+ExclusiveArch:  x86_64 aarch64 ppc64le s390x
+%endif
+
+# TODO(marun) tar archives are no longer published for 4.x. Should this value be removed?
+Source0:        https://%{import_path}/archive/%{commit}/%{name}-%{version}.tar.gz
+BuildRequires:  systemd
+BuildRequires:  bsdtar
+BuildRequires:  golang >= %{golang_version}
+BuildRequires:  krb5-devel
+BuildRequires:  rsync
+
+%description
+OpenShift is a distribution of Kubernetes optimized for enterprise application
+development and deployment. OpenShift adds developer and operational centric
+tools on top of Kubernetes to enable rapid application development, easy
+deployment and scaling, and long-term lifecycle maintenance for small and large
+teams and applications. It provides a secure and multi-tenant configuration for
+Kubernetes allowing you to safely host many different applications and workloads
+on a unified cluster.
+
+%package hyperkube
+Summary:        OpenShift Kubernetes server commands
+Requires:       util-linux
+Requires:       socat
+Requires:       iptables
+Provides:       hyperkube = %{version}
+Obsoletes:      atomic-openshift-hyperkube <= %{version}
+Obsoletes:      atomic-openshift-node <= %{version}
+
+%description hyperkube
+%{summary}
+
+%prep
+%if 0%{do_prep}
+%setup -q
+%endif
+
+%build
+%if 0%{do_build}
+# Create Binaries only for building arch
+%ifarch x86_64
+  BUILD_PLATFORM="linux/amd64"
+%endif
+%ifarch ppc64le
+  BUILD_PLATFORM="linux/ppc64le"
+%endif
+%ifarch %{arm} aarch64
+  BUILD_PLATFORM="linux/arm64"
+%endif
+%ifarch s390x
+  BUILD_PLATFORM="linux/s390x"
+%endif
+KUBE_BUILD_PLATFORMS="${BUILD_PLATFORM}" %{os_git_vars} make all WHAT='cmd/kube-apiserver cmd/kube-controller-manager cmd/kube-scheduler cmd/kubelet'
+%endif
+
+%install
+
+PLATFORM="$(go env GOHOSTOS)/$(go env GOHOSTARCH)"
+install -d %{buildroot}%{_bindir}
+install -d %{buildroot}%{_sysctldir}
+
+# Install linux components
+for bin in kube-apiserver kube-controller-manager kube-scheduler kubelet
+do
+  echo "+++ INSTALLING ${bin}"
+  install -p -m 755 _output/local/bin/${PLATFORM}/${bin} %{buildroot}%{_bindir}/${bin}
+done
+
+install -p -m 755 openshift-hack/images/hyperkube/hyperkube %{buildroot}%{_bindir}/hyperkube
+install -p -m 755 openshift-hack/images/hyperkube/kubensenter %{buildroot}%{_bindir}/kubensenter
+install -p -m 755 openshift-hack/sysctls/50-kubelet.conf %{buildroot}%{_sysctldir}/50-kubelet.conf
+
+%post
+%sysctl_apply 50-kubelet.conf
+
+%files hyperkube
+%license LICENSE
+%{_bindir}/hyperkube
+%{_bindir}/kube-apiserver
+%{_bindir}/kube-controller-manager
+%{_bindir}/kube-scheduler
+%{_bindir}/kubelet
+%{_bindir}/kubensenter
+%{_sysctldir}/50-kubelet.conf
+%defattr(-,root,root,0700)
+
+%changelog
diff --git a/pkg/kubelet/DOWNSTREAM_OWNERS b/pkg/kubelet/DOWNSTREAM_OWNERS
new file mode 100644
index 00000000000..d484fa4fc24
--- /dev/null
+++ b/pkg/kubelet/DOWNSTREAM_OWNERS
@@ -0,0 +1,17 @@
+# See the OWNERS docs at https://go.k8s.io/owners
+
+# Downstream reviewers, don't have to match those in OWNERS
+reviewers:
+  - rphillips
+  - sjenning
+  - mrunalp
+
+# Sub-package approvers from upstream with permission to approve downstream backports following these rules:
+# - they MUST be approvers upstream (here compare https://github.com/kubernetes/kubernetes/blob/17bb2fc050ec786b60db7d8d6d4d3ac8eeac205b/pkg/kubelet/OWNERS#L10-L11)
+# - they may approve "UPSTREAM: <PR>: ..." changes that merged upstream.
+# - carry patches for "UPSTREAM: <carry>: ..." and any unmerged PRs of the previous kind will have to be approved by the top-level approvers.
+approvers:
+  - sjenning
+  - mrunalp
+
+component: node
diff --git a/test/typecheck/main.go b/test/typecheck/main.go
index eb213293cfd..1f6410b020d 100644
--- a/test/typecheck/main.go
+++ b/test/typecheck/main.go
@@ -74,6 +74,9 @@ var (
 		// Tools we use for maintaining the code base but not necessarily
 		// ship as part of the release
 		"hack/tools",
+		// Tooling specific to openshift that is not shipped as part
+		// of a release.
+		"openshift-hack",
 	}
 )
 
-- 
2.41.0

